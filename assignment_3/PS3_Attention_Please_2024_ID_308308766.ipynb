{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOpGoE2T-YXS"
   },
   "source": [
    "# Neural Machine Translation with Attention\n",
    "\n",
    "Advanced Learning Fall 2024.   \n",
    "Last updated: 2025-01-12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpJdYve9cZa6"
   },
   "source": [
    "For SUBMISSION:   \n",
    "\n",
    "Please upload the complete and executed `ipynb` to your git repository. Verify that all of your output can be viewed directly from github, and provide a link to that git file below.\n",
    "\n",
    "~~~\n",
    "STUDENT ID: 308308766\n",
    "~~~\n",
    "\n",
    "~~~\n",
    "STUDENT GIT LINK: https://github.com/Nitzan93/52025-Course/tree/main/assignment_3\n",
    "~~~\n",
    "In Addition, don't forget to add your ID to the files, and upload to moodle the html version:    \n",
    "  \n",
    "`PS3_Attention_2024_ID_[000000000].html`   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eecp2PAf7qJq"
   },
   "source": [
    "In this problem set we are going to jump into the depths of `seq2seq` and `attention` and build a couple of PyTorch translation mechanisms with some  twists.     \n",
    "\n",
    "\n",
    "*   Part 1 consists of a somewhat unorthodox `seq2seq` model for simple arithmetics\n",
    "*   Part 2 consists of an `seq2seq - attention` language translation model. We will use it for Hebrew and English.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VpUCez9gOZn"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajNDsL5HlZN6"
   },
   "source": [
    "A **seq2seq** model (sequence-to-sequence model) is a type of neural network designed specifically to handle sequences of data. The model converts input sequences into other sequences of data. This makes them particularly useful for tasks involving language, where the input and output are naturally sequences of words.\n",
    "\n",
    "Here's a breakdown of how `seq2seq` models work:\n",
    "\n",
    "* The encoder takes the input sequence, like a sentence in English, and processes it to capture its meaning and context.\n",
    "\n",
    "* information is then passed to the decoder, which uses it to generate the output sequence, like a translation in French.\n",
    "\n",
    "* Attention mechanism (optional): Some `seq2seq` models also incorporate an attention mechanism. This allows the decoder to focus on specific parts of the input sequence that are most relevant to generating the next element in the output sequence.\n",
    "\n",
    "`seq2seq` models are used in many natural language processing (NLP) tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbUDn4FObol7"
   },
   "source": [
    "imports: (feel free to add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "crTe33wcD_Eg"
   },
   "outputs": [],
   "source": [
    "# from __future__ import unicode_literals, print_function, division\n",
    "# from io import open\n",
    "# import unicodedata\n",
    "import re\n",
    "import random\n",
    "import unicodedata\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiwtNgENbx2g"
   },
   "source": [
    "## Part 1: Seq2Seq Arithmetic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1gWov3Gx67I"
   },
   "source": [
    "**Using RNN `seq2seq` model to \"learn\" simple arithmetics!**\n",
    "\n",
    "> Given the string \"54-7\", the model should return a prediction: \"47\".  \n",
    "> Given the string \"10+20\", the model should return a prediction: \"30\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dxo92ZgTy6ED"
   },
   "source": [
    "- Watch Lukas Biewald's short [video](https://youtu.be/MqugtGD605k?si=rAH34ZTJyYDj-XJ1) explaining `seq2seq` models and his toy application (somewhat outdated).\n",
    "- You can find the code for his example [here](https://github.com/lukas/ml-class/blob/master/videos/seq2seq/train.py).    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEu_5YvqFPai"
   },
   "source": [
    "1.1) Using Lukas' code, implement a `seq2seq` network that can learn how to solve **addition AND substraction** of two numbers of maximum length of 4, using the following steps (similar to the example):      \n",
    "\n",
    "* Generate data; X: queries (two numbers), and Y: answers   \n",
    "* One-hot encode X and Y,\n",
    "* Build a `seq2seq` network (with LSTM, RepeatVector, and TimeDistributed layers)\n",
    "* Train the model.\n",
    "* While training, sample from the validation set at random so we can visualize the generated solutions against the true solutions.    \n",
    "\n",
    "Notes:  \n",
    "* The code in the example is quite old and based on Keras. You might have to adapt some of the code to overcome methods/code that is not supported anymore. Hint: for the evaluation part, review the type and format of the \"correct\" output - this will help you fix the unsupported \"model.predict_classes\".\n",
    "* Please use the parameters in the code cell below to train the model.     \n",
    "* Instead of using a `wandb.config` object, please use a simple dictionary instead.   \n",
    "* You don't need to run the model for more than 50 iterations (epochs) to get a gist of what is happening and what the algorithm is doing.\n",
    "* Extra credit if you can implement the network in PyTorch (this is not difficult).    \n",
    "* Extra credit if you are able to significantly improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdq4PyXNXe-j"
   },
   "source": [
    "# **Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAq7vFDXXj0t",
    "outputId": "3821ac9a-9895-474d-cff6-feab8a0f2665"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - accuracy: 0.2173 - loss: 2.2427 - val_accuracy: 0.2785 - val_loss: 1.9883\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 534ms/step\n",
      "Q: 876-2779 | True: -1903 | Pred: -222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 6521+9269 | True: 15790 | Pred: 1002\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 863-6340 | True: -5477 | Pred: -222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Q: 621-6562 | True: -5941 | Pred: -222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 8333-4169 | True: 4164 | Pred: -22\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 9013-3191 | True: 5822 | Pred: 222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 6323+9541 | True: 15864 | Pred: 1002\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 5475-7740 | True: -2265 | Pred: -222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 5545+2995 | True: 8540 | Pred: 1002\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 7625+4155 | True: 11780 | Pred: 1002\n",
      "Epoch 2/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 59ms/step - accuracy: 0.2856 - loss: 1.9536 - val_accuracy: 0.3123 - val_loss: 1.8640\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 6330+7595 | True: 13925 | Pred: 14177\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 1305-4237 | True: -2932 | Pred: -213\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 298+9139 | True: 9437 | Pred: 7911\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 2503+372 | True: 2875 | Pred: 5911\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 6559+2558 | True: 9117 | Pred: 1011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 669+9344 | True: 10013 | Pred: 7011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5650+7133 | True: 12783 | Pred: 12111\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 5476-6752 | True: -1276 | Pred: -133\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 3614+2949 | True: 6563 | Pred: 1011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 3051-6949 | True: -3898 | Pred: -133\n",
      "Epoch 3/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.3170 - loss: 1.8402 - val_accuracy: 0.3291 - val_loss: 1.7869\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 7298-7504 | True: -206 | Pred: -10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 6019+4770 | True: 10789 | Pred: 11998\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8419-6143 | True: 2276 | Pred: 5087\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 5087-3171 | True: 1916 | Pred: 3087\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 5820+9173 | True: 14993 | Pred: 14099\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 470+4627 | True: 5097 | Pred: 5911\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 184-1043 | True: -859 | Pred: -1077\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8710+8075 | True: 16785 | Pred: 17999\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 8683-9684 | True: -1001 | Pred: -10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 6319+958 | True: 7277 | Pred: 1007\n",
      "Epoch 4/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 60ms/step - accuracy: 0.3416 - loss: 1.7642 - val_accuracy: 0.3469 - val_loss: 1.7417\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 9108-2480 | True: 6628 | Pred: 4611\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 5900-4109 | True: 1791 | Pred: 116\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 4761-2577 | True: 2184 | Pred: 116\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8539+1677 | True: 10216 | Pred: 1002\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 5648-1921 | True: 3727 | Pred: 2166\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 8872+2619 | True: 11491 | Pred: 11066\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 1014+7573 | True: 8587 | Pred: 8046\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 5611-1032 | True: 4579 | Pred: 3881\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 622-8353 | True: -7731 | Pred: -6383\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 6317-9097 | True: -2780 | Pred: -3333\n",
      "Epoch 5/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 60ms/step - accuracy: 0.3566 - loss: 1.7112 - val_accuracy: 0.3558 - val_loss: 1.7035\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 4259+6238 | True: 10497 | Pred: 9001\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 976-7952 | True: -6976 | Pred: -7477\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 7745-173 | True: 7572 | Pred: 7711\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 2553-9526 | True: -6973 | Pred: -7714\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 7926-9478 | True: -1552 | Pred: -1088\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 8201-1986 | True: 6215 | Pred: 4411\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 4730+7420 | True: 12150 | Pred: 11912\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 6687+158 | True: 6845 | Pred: 4111\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 9330+3316 | True: 12646 | Pred: 11912\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 795-5311 | True: -4516 | Pred: -5447\n",
      "Epoch 6/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.3729 - loss: 1.6615 - val_accuracy: 0.3837 - val_loss: 1.6407\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 7591+8801 | True: 16392 | Pred: 16422\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Q: 3592+3406 | True: 6998 | Pred: 6853\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Q: 3040+6286 | True: 9326 | Pred: 9001\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 9707-922 | True: 8785 | Pred: 7888\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 603-9698 | True: -9095 | Pred: -8910\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 9588+9411 | True: 18999 | Pred: 18762\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 6903-371 | True: 6532 | Pred: 6811\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 9623-6291 | True: 3332 | Pred: 3885\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Q: 6740-6719 | True: 21 | Pred: 112\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 447-1733 | True: -1286 | Pred: -1044\n",
      "Epoch 7/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 60ms/step - accuracy: 0.3851 - loss: 1.6330 - val_accuracy: 0.3980 - val_loss: 1.5985\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 3144-948 | True: 2196 | Pred: -14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 7080-2055 | True: 5025 | Pred: 4811\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 191-4560 | True: -4369 | Pred: -4028\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 2967-7544 | True: -4577 | Pred: -4028\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 1372+6934 | True: 8306 | Pred: 8944\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 3923+6522 | True: 10445 | Pred: 10622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 8789-5971 | True: 2818 | Pred: 2282\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 9569-7080 | True: 2489 | Pred: 2282\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 9791-311 | True: 9480 | Pred: 7882\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 8967+9301 | True: 18268 | Pred: 18598\n",
      "Epoch 8/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.3968 - loss: 1.6024 - val_accuracy: 0.3809 - val_loss: 1.6119\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 7304-490 | True: 6814 | Pred: 6225\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 9791-311 | True: 9480 | Pred: 7786\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 513+1592 | True: 2105 | Pred: 3211\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 9117-7804 | True: 1313 | Pred: 125\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 6595-8442 | True: -1847 | Pred: -2333\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 4276+2080 | True: 6356 | Pred: 6333\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 4312+7871 | True: 12183 | Pred: 12888\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 9750-5003 | True: 4747 | Pred: 4325\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 4843-6512 | True: -1669 | Pred: -1753\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 8521-1296 | True: 7225 | Pred: 6226\n",
      "Epoch 9/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 61ms/step - accuracy: 0.4018 - loss: 1.5837 - val_accuracy: 0.4108 - val_loss: 1.5656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 6976-6461 | True: 515 | Pred: 112\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 6218+3151 | True: 9369 | Pred: 9038\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 7818-4747 | True: 3071 | Pred: 3349\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8189-816 | True: 7373 | Pred: 7789\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4937-4112 | True: 825 | Pred: 122\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 6106+343 | True: 6449 | Pred: 5734\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 6330+5690 | True: 12020 | Pred: 11273\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 7995-6218 | True: 1777 | Pred: 2249\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 5353-1537 | True: 3816 | Pred: 3341\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 7382-7676 | True: -294 | Pred: -10\n",
      "Epoch 10/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.4098 - loss: 1.5642 - val_accuracy: 0.4066 - val_loss: 1.5629\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 4450+9440 | True: 13890 | Pred: 13383\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5017-4162 | True: 855 | Pred: 11\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 6773-599 | True: 6174 | Pred: 5179\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 8099-8354 | True: -255 | Pred: -100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 3496-1068 | True: 2428 | Pred: 2239\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 5642-6375 | True: -733 | Pred: -103\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 4423-9779 | True: -5356 | Pred: -5444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 3569+6310 | True: 9879 | Pred: 9011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 6758+4268 | True: 11026 | Pred: 11813\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Q: 9231+9215 | True: 18446 | Pred: 17188\n",
      "Epoch 11/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 62ms/step - accuracy: 0.4149 - loss: 1.5472 - val_accuracy: 0.4251 - val_loss: 1.5284\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 8226+9240 | True: 17466 | Pred: 17988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 3173-9432 | True: -6259 | Pred: -6655\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 5031-1353 | True: 3678 | Pred: 3381\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Q: 2513+6993 | True: 9506 | Pred: 9011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Q: 5004-6655 | True: -1651 | Pred: -1513\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 1142-3340 | True: -2198 | Pred: -2383\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 4530+8475 | True: 13005 | Pred: 12888\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Q: 860+949 | True: 1809 | Pred: 8532\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 323+8225 | True: 8548 | Pred: 8833\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 2419+3862 | True: 6281 | Pred: 6111\n",
      "Epoch 12/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.4224 - loss: 1.5281 - val_accuracy: 0.4220 - val_loss: 1.5284\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 1545+378 | True: 1923 | Pred: 2223\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 8666-3470 | True: 5196 | Pred: 5355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 4756+4123 | True: 8879 | Pred: 8931\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 6106+343 | True: 6449 | Pred: 5233\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 2319-3080 | True: -761 | Pred: -105\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5545+2995 | True: 8540 | Pred: 8922\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 1944+3732 | True: 5676 | Pred: 5553\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 2516-245 | True: 2271 | Pred: 2335\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 9425+8820 | True: 18245 | Pred: 18555\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 3420-145 | True: 3275 | Pred: 3235\n",
      "Epoch 13/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 63ms/step - accuracy: 0.4251 - loss: 1.5181 - val_accuracy: 0.4283 - val_loss: 1.5088\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 3244-6841 | True: -3597 | Pred: -3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 5332-9174 | True: -3842 | Pred: -3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 1295-1213 | True: 82 | Pred: -0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 1714-3419 | True: -1705 | Pred: -1033\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 5731+8940 | True: 14671 | Pred: 14500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 1862-9644 | True: -7782 | Pred: -7100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 3978+531 | True: 4509 | Pred: 6286\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 3563-7728 | True: -4165 | Pred: -4000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4845+4549 | True: 9394 | Pred: 9400\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 8192+2053 | True: 10245 | Pred: 10008\n",
      "Epoch 14/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 61ms/step - accuracy: 0.4267 - loss: 1.5113 - val_accuracy: 0.4308 - val_loss: 1.4947\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 1096+1832 | True: 2928 | Pred: 3889\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 7594-443 | True: 7151 | Pred: 6881\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 7151-3506 | True: 3645 | Pred: 3355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 7613-4863 | True: 2750 | Pred: 2655\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 1888-5896 | True: -4008 | Pred: -3588\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 5689-6802 | True: -1113 | Pred: -1055\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 1615-1003 | True: 612 | Pred: 100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 6189+271 | True: 6460 | Pred: 4182\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 7783+5442 | True: 13225 | Pred: 13819\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 683-5356 | True: -4673 | Pred: -4148\n",
      "Epoch 15/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.4339 - loss: 1.4943 - val_accuracy: 0.4296 - val_loss: 1.4954\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 409+4349 | True: 4758 | Pred: 4001\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 9796-5489 | True: 4307 | Pred: 4000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 3900+3214 | True: 7114 | Pred: 7200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4151+6745 | True: 10896 | Pred: 10922\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 809-4976 | True: -4167 | Pred: -4778\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 5471-5163 | True: 308 | Pred: 120\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Q: 322-5949 | True: -5627 | Pred: -5800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 2325+1093 | True: 3418 | Pred: 3600\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Q: 4935-6927 | True: -1992 | Pred: -1253\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Q: 8260+7237 | True: 15497 | Pred: 15020\n",
      "Epoch 16/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 61ms/step - accuracy: 0.4384 - loss: 1.4842 - val_accuracy: 0.4294 - val_loss: 1.4985\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 3347+4236 | True: 7583 | Pred: 7554\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 4156+3387 | True: 7543 | Pred: 7754\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9458-1037 | True: 8421 | Pred: 7003\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 2578-5658 | True: -3080 | Pred: -3079\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 9443+5368 | True: 14811 | Pred: 14222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8292+8031 | True: 16323 | Pred: 16155\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 1519+7614 | True: 9133 | Pred: 8244\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 3203+5777 | True: 8980 | Pred: 8204\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 6499+3566 | True: 10065 | Pred: 9002\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 7254-4355 | True: 2899 | Pred: 2763\n",
      "Epoch 17/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 60ms/step - accuracy: 0.4403 - loss: 1.4783 - val_accuracy: 0.4308 - val_loss: 1.4946\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 1892+5885 | True: 7777 | Pred: 7707\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8058-3926 | True: 4132 | Pred: 4001\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 2379-4315 | True: -1936 | Pred: -1750\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 1238+7241 | True: 8479 | Pred: 8009\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 5731-4180 | True: 1551 | Pred: 1015\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 8222-906 | True: 7316 | Pred: 7075\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8505+6123 | True: 14628 | Pred: 14170\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 5735-8819 | True: -3084 | Pred: -2215\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 2875+6977 | True: 9852 | Pred: 9700\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 6545-5344 | True: 1201 | Pred: 1015\n",
      "Epoch 18/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 62ms/step - accuracy: 0.4436 - loss: 1.4663 - val_accuracy: 0.4409 - val_loss: 1.4696\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 3704-4194 | True: -490 | Pred: -102\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 1228-3520 | True: -2292 | Pred: -2288\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5523-3887 | True: 1636 | Pred: 1220\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 1476+5710 | True: 7186 | Pred: 7222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 1497-3583 | True: -2086 | Pred: -1753\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 4663+1622 | True: 6285 | Pred: 6952\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 4638-1451 | True: 3187 | Pred: 3321\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 1525-9003 | True: -7478 | Pred: -7102\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8540+5844 | True: 14384 | Pred: 14755\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 1441+1813 | True: 3254 | Pred: 3222\n",
      "Epoch 19/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.4466 - loss: 1.4619 - val_accuracy: 0.4356 - val_loss: 1.4765\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 2691+9922 | True: 12613 | Pred: 12729\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 6507-6664 | True: -157 | Pred: -14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 517-7209 | True: -6692 | Pred: -6223\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 8007-4602 | True: 3405 | Pred: 3955\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 1582-2584 | True: -1002 | Pred: -123\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 9113+3957 | True: 13070 | Pred: 12800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 685+6563 | True: 7248 | Pred: 7202\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 2485+2798 | True: 5283 | Pred: 5099\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 3858+8503 | True: 12361 | Pred: 12222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 6881-8623 | True: -1742 | Pred: -1252\n",
      "Epoch 20/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 63ms/step - accuracy: 0.4492 - loss: 1.4559 - val_accuracy: 0.4474 - val_loss: 1.4494\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Q: 9475-302 | True: 9173 | Pred: 8884\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Q: 2590+7899 | True: 10489 | Pred: 10211\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Q: 7054+1430 | True: 8484 | Pred: 8101\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 5865+5226 | True: 11091 | Pred: 11111\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Q: 9418+4203 | True: 13621 | Pred: 13615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 9013-3191 | True: 5822 | Pred: 5018\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 9342+1366 | True: 10708 | Pred: 10855\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 5306-8812 | True: -3506 | Pred: -3511\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Q: 2913-2249 | True: 664 | Pred: 516\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Q: 1980-2073 | True: -93 | Pred: -144\n",
      "Epoch 21/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 62ms/step - accuracy: 0.4526 - loss: 1.4463 - val_accuracy: 0.4486 - val_loss: 1.4429\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 8418-247 | True: 8171 | Pred: 7873\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 3864+7239 | True: 11103 | Pred: 11212\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Q: 8952-609 | True: 8343 | Pred: 7996\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2720-4115 | True: -1395 | Pred: -1555\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 6982+8028 | True: 15010 | Pred: 15475\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 91-1220 | True: -1129 | Pred: -1743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 6110-5317 | True: 793 | Pred: 115\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 1014+7573 | True: 8587 | Pred: 8999\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 1304-696 | True: 608 | Pred: 122\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 5232+4170 | True: 9402 | Pred: 9544\n",
      "Epoch 22/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - accuracy: 0.4557 - loss: 1.4383 - val_accuracy: 0.4532 - val_loss: 1.4328\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 2296+9611 | True: 11907 | Pred: 11585\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 3391-221 | True: 3170 | Pred: 3389\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 9036-7760 | True: 1276 | Pred: 1011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 323+8225 | True: 8548 | Pred: 8333\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3718+9043 | True: 12761 | Pred: 12885\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 1677-7877 | True: -6200 | Pred: -6808\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 3060-9799 | True: -6739 | Pred: -6808\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8072-2144 | True: 5928 | Pred: 5021\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 1922-9510 | True: -7588 | Pred: -7505\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 515+1684 | True: 2199 | Pred: 2901\n",
      "Epoch 23/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 63ms/step - accuracy: 0.4611 - loss: 1.4252 - val_accuracy: 0.4508 - val_loss: 1.4357\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Q: 177+5091 | True: 5268 | Pred: 3552\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Q: 5226+9194 | True: 14420 | Pred: 14662\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 2188+7360 | True: 9548 | Pred: 9602\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Q: 1026-1540 | True: -514 | Pred: -123\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 4365-3732 | True: 633 | Pred: 122\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 9296-5281 | True: 4015 | Pred: 4355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 3991+2819 | True: 6810 | Pred: 6552\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Q: 8552+5619 | True: 14171 | Pred: 14860\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 1459+9457 | True: 10916 | Pred: 10869\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 695+8869 | True: 9564 | Pred: 9742\n",
      "Epoch 24/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 61ms/step - accuracy: 0.4615 - loss: 1.4177 - val_accuracy: 0.4530 - val_loss: 1.4409\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 3416-7847 | True: -4431 | Pred: -3331\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 3546-1699 | True: 1847 | Pred: 2266\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 772+5153 | True: 5925 | Pred: 6113\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9222-1335 | True: 7887 | Pred: 7113\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 6841+9851 | True: 16692 | Pred: 16693\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 9189+3431 | True: 12620 | Pred: 12922\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 3498-5706 | True: -2208 | Pred: -2888\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 7621-3715 | True: 3906 | Pred: 3335\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 7537-7136 | True: 401 | Pred: 118\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8971+8831 | True: 17802 | Pred: 17674\n",
      "Epoch 25/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 59ms/step - accuracy: 0.4644 - loss: 1.4100 - val_accuracy: 0.4612 - val_loss: 1.4060\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9527+6332 | True: 15859 | Pred: 15799\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 3087+2055 | True: 5142 | Pred: 5959\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Q: 9975-8337 | True: 1638 | Pred: 1005\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 9897-6768 | True: 3129 | Pred: 3005\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 6901-9117 | True: -2216 | Pred: -2277\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 2431+85 | True: 2516 | Pred: 3228\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 8956+1486 | True: 10442 | Pred: 10251\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5275-4598 | True: 677 | Pred: 304\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 4756+4123 | True: 8879 | Pred: 8807\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 2405-669 | True: 1736 | Pred: 1000\n",
      "Epoch 26/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 63ms/step - accuracy: 0.4696 - loss: 1.3973 - val_accuracy: 0.4653 - val_loss: 1.3946\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 1725-2886 | True: -1161 | Pred: -103\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 1433-8392 | True: -6959 | Pred: -6333\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 5097+9704 | True: 14801 | Pred: 14873\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 6733-1971 | True: 4762 | Pred: 4635\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 5473+9247 | True: 14720 | Pred: 14877\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 4342-7805 | True: -3463 | Pred: -3566\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Q: 3796+7865 | True: 11661 | Pred: 11678\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 8222-906 | True: 7316 | Pred: 7083\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 2916+8061 | True: 10977 | Pred: 10973\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 226+4267 | True: 4493 | Pred: 4553\n",
      "Epoch 27/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 59ms/step - accuracy: 0.4741 - loss: 1.3852 - val_accuracy: 0.4686 - val_loss: 1.3863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4632-4467 | True: 165 | Pred: 301\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8284+36 | True: 8320 | Pred: 9658\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 1621-2322 | True: -701 | Pred: -803\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4804+3257 | True: 8061 | Pred: 8036\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 6786+5679 | True: 12465 | Pred: 12592\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 4411+8065 | True: 12476 | Pred: 12972\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8091-9332 | True: -1241 | Pred: -1200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 3494-1173 | True: 2321 | Pred: 2265\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 1044+3066 | True: 4110 | Pred: 4224\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 8183+9374 | True: 17557 | Pred: 17423\n",
      "Epoch 28/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 64ms/step - accuracy: 0.4813 - loss: 1.3689 - val_accuracy: 0.4737 - val_loss: 1.3749\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4759+5866 | True: 10625 | Pred: 10577\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 1433-8392 | True: -6959 | Pred: -6262\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9635+9766 | True: 19401 | Pred: 19448\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 970+2788 | True: 3758 | Pred: 3770\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 885+31 | True: 916 | Pred: 804\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 1543-3470 | True: -1927 | Pred: -1903\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 2154-5321 | True: -3167 | Pred: -3029\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 761-5765 | True: -5004 | Pred: -4542\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 1664-679 | True: 985 | Pred: 132\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 848-6806 | True: -5958 | Pred: -5850\n",
      "Epoch 29/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - accuracy: 0.4824 - loss: 1.3657 - val_accuracy: 0.4748 - val_loss: 1.3686\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 4183+6318 | True: 10501 | Pred: 10316\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 6201+7230 | True: 13431 | Pred: 13363\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 6510-6484 | True: 26 | Pred: -1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 4262-5650 | True: -1388 | Pred: -1350\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 2225+4189 | True: 6414 | Pred: 6613\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 9209+3883 | True: 13092 | Pred: 13199\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 591-6579 | True: -5988 | Pred: -5255\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5001-1926 | True: 3075 | Pred: 3991\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 5630-5416 | True: 214 | Pred: 316\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 3852-6266 | True: -2414 | Pred: -2226\n",
      "Epoch 30/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 64ms/step - accuracy: 0.4905 - loss: 1.3453 - val_accuracy: 0.4863 - val_loss: 1.3433\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Q: 8755-5001 | True: 3754 | Pred: 3355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 9202-609 | True: 8593 | Pred: 8006\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 9044+9172 | True: 18216 | Pred: 18464\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 1842-9487 | True: -7645 | Pred: -7105\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 4343+1068 | True: 5411 | Pred: 5656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Q: 8333-4169 | True: 4164 | Pred: 4055\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Q: 6925+1924 | True: 8849 | Pred: 8701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 3246+8326 | True: 11572 | Pred: 11565\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Q: 6931-4828 | True: 2103 | Pred: 2175\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Q: 1811+5374 | True: 7185 | Pred: 7935\n",
      "Epoch 31/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 60ms/step - accuracy: 0.4939 - loss: 1.3340 - val_accuracy: 0.4830 - val_loss: 1.3452\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 6537-3051 | True: 3486 | Pred: 3662\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 9707-922 | True: 8785 | Pred: 8289\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 2740+9568 | True: 12308 | Pred: 12349\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 9697-1297 | True: 8400 | Pred: 8289\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 5545-5892 | True: -347 | Pred: -544\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 7498+5394 | True: 12892 | Pred: 12872\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 1841+2402 | True: 4243 | Pred: 4299\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 4436+6874 | True: 11310 | Pred: 11242\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3153+5506 | True: 8659 | Pred: 8672\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 1833-3521 | True: -1688 | Pred: -1753\n",
      "Epoch 32/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 60ms/step - accuracy: 0.4975 - loss: 1.3201 - val_accuracy: 0.4935 - val_loss: 1.3188\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 6511-6456 | True: 55 | Pred: -2\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 5475-7740 | True: -2265 | Pred: -2316\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 6031+5944 | True: 11975 | Pred: 11900\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 283+7461 | True: 7744 | Pred: 7906\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 5895+3391 | True: 9286 | Pred: 9200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4142-3124 | True: 1018 | Pred: 1011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 6356-7255 | True: -899 | Pred: -906\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 7441-9126 | True: -1685 | Pred: -1515\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 1012-5482 | True: -4470 | Pred: -4041\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 5247-9992 | True: -4745 | Pred: -4651\n",
      "Epoch 33/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 66ms/step - accuracy: 0.5068 - loss: 1.2986 - val_accuracy: 0.4986 - val_loss: 1.3111\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4066+3059 | True: 7125 | Pred: 7207\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 6278-1147 | True: 5131 | Pred: 5081\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 2330-8535 | True: -6205 | Pred: -6332\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 6862+1010 | True: 7872 | Pred: 7887\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 3677-1625 | True: 2052 | Pred: 2280\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 81+4292 | True: 4373 | Pred: 3488\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 5603-9024 | True: -3421 | Pred: -3226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 1869+675 | True: 2544 | Pred: 2488\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 2909+5284 | True: 8193 | Pred: 8037\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 6974-1164 | True: 5810 | Pred: 5681\n",
      "Epoch 34/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 61ms/step - accuracy: 0.5109 - loss: 1.2866 - val_accuracy: 0.5086 - val_loss: 1.2867\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 3194-799 | True: 2395 | Pred: 2391\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 1974+5627 | True: 7601 | Pred: 7591\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 1979-6335 | True: -4356 | Pred: -4341\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 7560+7880 | True: 15440 | Pred: 15455\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 6863-9636 | True: -2773 | Pred: -2011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 7104+599 | True: 7703 | Pred: 8991\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Q: 2451-6229 | True: -3778 | Pred: -3981\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Q: 7674-8085 | True: -411 | Pred: -600\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 6355-433 | True: 5922 | Pred: 5111\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Q: 7649-1533 | True: 6116 | Pred: 5000\n",
      "Epoch 35/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 65ms/step - accuracy: 0.5180 - loss: 1.2695 - val_accuracy: 0.5148 - val_loss: 1.2646\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 2319-3080 | True: -761 | Pred: -804\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Q: 3900+3214 | True: 7114 | Pred: 7107\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 7192+6976 | True: 14168 | Pred: 14075\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 3892-5522 | True: -1630 | Pred: -1515\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Q: 7766-133 | True: 7633 | Pred: 7704\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 4482-972 | True: 3510 | Pred: 3581\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 8020-6087 | True: 1933 | Pred: 1015\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Q: 2569-2305 | True: 264 | Pred: 104\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 3355+5941 | True: 9296 | Pred: 9220\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Q: 6338-4514 | True: 1824 | Pred: 1815\n",
      "Epoch 36/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 63ms/step - accuracy: 0.5269 - loss: 1.2466 - val_accuracy: 0.5145 - val_loss: 1.2608\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2398-9450 | True: -7052 | Pred: -7155\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 3056+1845 | True: 4901 | Pred: 5995\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8724-2409 | True: 6315 | Pred: 6355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9871+8915 | True: 18786 | Pred: 18791\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 1869+8265 | True: 10134 | Pred: 10279\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 8907-1170 | True: 7737 | Pred: 7557\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 8724-2409 | True: 6315 | Pred: 6355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 9228-7343 | True: 1885 | Pred: 1815\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 6312-4866 | True: 1446 | Pred: 1557\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 9071+9211 | True: 18282 | Pred: 18344\n",
      "Epoch 37/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 63ms/step - accuracy: 0.5338 - loss: 1.2283 - val_accuracy: 0.5286 - val_loss: 1.2333\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 5333-7035 | True: -1702 | Pred: -1716\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8764+1426 | True: 10190 | Pred: 10107\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 6971-1243 | True: 5728 | Pred: 5607\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 1802-6988 | True: -5186 | Pred: -5801\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 8968-8429 | True: 539 | Pred: 659\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5796+5161 | True: 10957 | Pred: 10908\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 3846+5944 | True: 9790 | Pred: 9802\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 5326-8192 | True: -2866 | Pred: -2866\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 2588-9549 | True: -6961 | Pred: -7184\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 5575+2696 | True: 8271 | Pred: 8357\n",
      "Epoch 38/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 66ms/step - accuracy: 0.5402 - loss: 1.2125 - val_accuracy: 0.5397 - val_loss: 1.2121\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 2591-1461 | True: 1130 | Pred: 1014\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 6791+331 | True: 7122 | Pred: 7066\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 2219-5422 | True: -3203 | Pred: -3219\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 7017-3702 | True: 3315 | Pred: 3238\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 1353-4183 | True: -2830 | Pred: -2888\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3203+5777 | True: 8980 | Pred: 9002\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 7489-9420 | True: -1931 | Pred: -1911\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 456+4067 | True: 4523 | Pred: 4611\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 4740-6757 | True: -2017 | Pred: -2061\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 6158+4378 | True: 10536 | Pred: 10613\n",
      "Epoch 39/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 66ms/step - accuracy: 0.5496 - loss: 1.1904 - val_accuracy: 0.5410 - val_loss: 1.2011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 1599+9475 | True: 11074 | Pred: 11108\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 6906-6153 | True: 753 | Pred: 448\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5573-4581 | True: 992 | Pred: 101\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 8954-7411 | True: 1543 | Pred: 1581\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 3265-1017 | True: 2248 | Pred: 2388\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 470+9665 | True: 10135 | Pred: 10284\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 7371+8210 | True: 15581 | Pred: 15553\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5448+314 | True: 5762 | Pred: 5611\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 9743-8307 | True: 1436 | Pred: 1465\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 5809-1804 | True: 4005 | Pred: 3046\n",
      "Epoch 40/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 64ms/step - accuracy: 0.5557 - loss: 1.1772 - val_accuracy: 0.5480 - val_loss: 1.1858\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 8581+1632 | True: 10213 | Pred: 10190\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2893-4215 | True: -1322 | Pred: -1355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 1876-8935 | True: -7059 | Pred: -7906\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 9985-8891 | True: 1094 | Pred: 1099\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 1154+2683 | True: 3837 | Pred: 3999\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 2351-4388 | True: -2037 | Pred: -2069\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 6230+6588 | True: 12818 | Pred: 12872\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 1123+9723 | True: 10846 | Pred: 10835\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 2020+6611 | True: 8631 | Pred: 7699\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 2065-7791 | True: -5726 | Pred: -5766\n",
      "Epoch 41/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 67ms/step - accuracy: 0.5606 - loss: 1.1615 - val_accuracy: 0.5534 - val_loss: 1.1729\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 8305-8917 | True: -612 | Pred: -677\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 8406+9993 | True: 18399 | Pred: 18364\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 7164+4968 | True: 12132 | Pred: 12184\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8348-2663 | True: 5685 | Pred: 5664\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 8654-7193 | True: 1461 | Pred: 1483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 4854-9054 | True: -4200 | Pred: -4355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 9813+3002 | True: 12815 | Pred: 12846\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 8027+5242 | True: 13269 | Pred: 13344\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 1793-3164 | True: -1371 | Pred: -1213\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 6310-6572 | True: -262 | Pred: -332\n",
      "Epoch 42/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 63ms/step - accuracy: 0.5679 - loss: 1.1476 - val_accuracy: 0.5592 - val_loss: 1.1628\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 6201+7230 | True: 13431 | Pred: 13413\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 2081+8848 | True: 10929 | Pred: 10979\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 5059-591 | True: 4468 | Pred: 4339\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 6670+8935 | True: 15605 | Pred: 15655\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 4998+9623 | True: 14621 | Pred: 14653\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Q: 8482-8514 | True: -32 | Pred: -14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 930+6943 | True: 7873 | Pred: 8792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 4149-6141 | True: -1992 | Pred: -1943\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8802-9010 | True: -208 | Pred: -343\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 1269+9498 | True: 10767 | Pred: 10732\n",
      "Epoch 43/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 67ms/step - accuracy: 0.5725 - loss: 1.1372 - val_accuracy: 0.5596 - val_loss: 1.1609\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 5894+7767 | True: 13661 | Pred: 13633\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 7133-2945 | True: 4188 | Pred: 4230\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 5073-2980 | True: 2093 | Pred: 2136\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 4893-1333 | True: 3560 | Pred: 3533\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 1667+1248 | True: 2915 | Pred: 2996\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 546+6737 | True: 7283 | Pred: 7146\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 7411-8595 | True: -1184 | Pred: -1118\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 4955+8417 | True: 13372 | Pred: 13355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 8512-5890 | True: 2622 | Pred: 2666\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 7808+8261 | True: 16069 | Pred: 16003\n",
      "Epoch 44/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 67ms/step - accuracy: 0.5754 - loss: 1.1293 - val_accuracy: 0.5588 - val_loss: 1.1565\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 3371-4248 | True: -877 | Pred: -903\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 6947+6002 | True: 12949 | Pred: 12972\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 2942+3397 | True: 6339 | Pred: 6379\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 9474+7414 | True: 16888 | Pred: 16877\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 8603-5657 | True: 2946 | Pred: 2975\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 308-419 | True: -111 | Pred: -11\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 2546+7273 | True: 9819 | Pred: 9782\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 4462+5677 | True: 10139 | Pred: 10172\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 7926-9419 | True: -1493 | Pred: -1515\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 7612+8101 | True: 15713 | Pred: 15777\n",
      "Epoch 45/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 68ms/step - accuracy: 0.5784 - loss: 1.1213 - val_accuracy: 0.5704 - val_loss: 1.1340\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 6213+2417 | True: 8630 | Pred: 8655\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9387+6985 | True: 16372 | Pred: 16394\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 2137-9339 | True: -7202 | Pred: -7191\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9661+5844 | True: 15505 | Pred: 15495\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 198+8274 | True: 8472 | Pred: 8389\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 128+9674 | True: 9802 | Pred: 9888\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 9485-3854 | True: 5631 | Pred: 5628\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 440+2461 | True: 2901 | Pred: 3952\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 2590+7899 | True: 10489 | Pred: 10408\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 3208+7908 | True: 11116 | Pred: 11125\n",
      "Epoch 46/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 67ms/step - accuracy: 0.5841 - loss: 1.1090 - val_accuracy: 0.5730 - val_loss: 1.1263\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 795-5311 | True: -4516 | Pred: -4405\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 6848+2232 | True: 9080 | Pred: 9102\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 7306+188 | True: 7494 | Pred: 7583\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Q: 3120+1969 | True: 5089 | Pred: 5001\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Q: 3581-2263 | True: 1318 | Pred: 1394\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 2488-9501 | True: -7013 | Pred: -7066\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 3329+5219 | True: 8548 | Pred: 8544\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 5476-6752 | True: -1276 | Pred: -1223\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 2323+2999 | True: 5322 | Pred: 5221\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 1821-6882 | True: -5061 | Pred: -5036\n",
      "Epoch 47/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 65ms/step - accuracy: 0.5856 - loss: 1.1022 - val_accuracy: 0.5648 - val_loss: 1.1390\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 2225+4189 | True: 6414 | Pred: 6315\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 7828+5634 | True: 13462 | Pred: 13411\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 8826+7092 | True: 15918 | Pred: 15988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 4983+8634 | True: 13617 | Pred: 13615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 5271+6251 | True: 11522 | Pred: 11408\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 7138-4394 | True: 2744 | Pred: 2761\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8540+5844 | True: 14384 | Pred: 14355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 2406+2409 | True: 4815 | Pred: 4744\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8499-4615 | True: 3884 | Pred: 3865\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 8036-1477 | True: 6559 | Pred: 6624\n",
      "Epoch 48/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 61ms/step - accuracy: 0.5909 - loss: 1.0938 - val_accuracy: 0.5805 - val_loss: 1.1082\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 1433-8392 | True: -6959 | Pred: -6969\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8617-9426 | True: -809 | Pred: -870\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 4002+2468 | True: 6470 | Pred: 6511\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 3717-423 | True: 3294 | Pred: 3355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3523+8306 | True: 11829 | Pred: 11805\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 4968-1728 | True: 3240 | Pred: 3223\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 5866-1405 | True: 4461 | Pred: 4415\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 2703-3319 | True: -616 | Pred: -699\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 7727-2097 | True: 5630 | Pred: 5651\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Q: 868-7141 | True: -6273 | Pred: -6335\n",
      "Epoch 49/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 67ms/step - accuracy: 0.5964 - loss: 1.0797 - val_accuracy: 0.5752 - val_loss: 1.1142\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 795-5311 | True: -4516 | Pred: -4488\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 3760+2280 | True: 6040 | Pred: 6955\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2330-8535 | True: -6205 | Pred: -6291\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 9642+1477 | True: 11119 | Pred: 11050\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 884-639 | True: 245 | Pred: 100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 3614+5154 | True: 8768 | Pred: 8713\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 3986+167 | True: 4153 | Pred: 4183\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 2520-7078 | True: -4558 | Pred: -4521\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 4862+4474 | True: 9336 | Pred: 9375\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 8847-7131 | True: 1716 | Pred: 1795\n",
      "Epoch 50/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 67ms/step - accuracy: 0.5940 - loss: 1.0852 - val_accuracy: 0.5810 - val_loss: 1.0995\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 5581+8449 | True: 14030 | Pred: 13022\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3986+167 | True: 4153 | Pred: 4186\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 8295-5909 | True: 2386 | Pred: 2328\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Q: 1348-9177 | True: -7829 | Pred: -7866\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Q: 470+4627 | True: 5097 | Pred: 5111\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Q: 5034-5010 | True: 24 | Pred: 18\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 9634-8496 | True: 1138 | Pred: 1149\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 9330+3316 | True: 12646 | Pred: 12625\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Q: 9748-1558 | True: 8190 | Pred: 8268\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 6016+9254 | True: 15270 | Pred: 15205\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, TimeDistributed, RepeatVector, Dense\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"training_size\": 50000,\n",
    "    \"digits\": 4,\n",
    "    \"hidden_size\": 128,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 50\n",
    "}\n",
    "\n",
    "maxlen = params[\"digits\"] + 1 + params[\"digits\"]\n",
    "chars = \"0123456789+- \"\n",
    "ctable = {\n",
    "    \"char_to_index\": {c: i for i, c in enumerate(chars)},\n",
    "    \"index_to_char\": {i: c for i, c in enumerate(chars)},\n",
    "}\n",
    "\n",
    "def encode(seq, maxlen, vocab_size):\n",
    "    \"\"\"One-hot encode the sequence.\"\"\"\n",
    "    x = np.zeros((maxlen, vocab_size), dtype=np.float32)\n",
    "    for i, c in enumerate(seq):\n",
    "        x[i, ctable[\"char_to_index\"][c]] = 1\n",
    "    return x\n",
    "\n",
    "def decode(seq):\n",
    "    \"\"\"Decode one-hot encoded sequence.\"\"\"\n",
    "    indices = seq.argmax(axis=-1)\n",
    "    return ''.join(ctable[\"index_to_char\"][idx] for idx in indices)\n",
    "\n",
    "# Data Generation\n",
    "questions = []\n",
    "answers = []\n",
    "seen = set()\n",
    "while len(questions) < params[\"training_size\"]:\n",
    "    a = random.randint(0, 10 ** params[\"digits\"] - 1)\n",
    "    b = random.randint(0, 10 ** params[\"digits\"] - 1)\n",
    "    operation = random.choice([\"+\", \"-\"])\n",
    "    query = f\"{a}{operation}{b}\"\n",
    "    if query in seen:\n",
    "        continue\n",
    "    seen.add(query)\n",
    "    result = eval(query)  # Calculate the result\n",
    "    questions.append(query.ljust(maxlen))  # Padded to maxlen\n",
    "    answers.append(str(result).ljust(params[\"digits\"] + 1))  # Padded to output length\n",
    "\n",
    "vocab_size = len(chars)\n",
    "x = np.zeros((len(questions), maxlen, vocab_size), dtype=np.float32)\n",
    "y = np.zeros((len(questions), params[\"digits\"] + 1, vocab_size), dtype=np.float32)\n",
    "\n",
    "for i, (question, answer) in enumerate(zip(questions, answers)):\n",
    "    x[i] = encode(question, maxlen, vocab_size)\n",
    "    y[i] = encode(answer, params[\"digits\"] + 1, vocab_size)\n",
    "\n",
    "# Train / Test Split\n",
    "split_at = len(x) - len(x) // 10\n",
    "x_train, x_val = x[:split_at], x[split_at:]\n",
    "y_train, y_val = y[:split_at], y[split_at:]\n",
    "\n",
    "# Model Definition\n",
    "model = Sequential()\n",
    "model.add(LSTM(params[\"hidden_size\"], input_shape=(maxlen, vocab_size)))\n",
    "model.add(RepeatVector(params[\"digits\"] + 1))\n",
    "model.add(LSTM(params[\"hidden_size\"], return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(vocab_size, activation=\"softmax\")))\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Training\n",
    "for epoch in range(params[\"epochs\"]):\n",
    "    print(f\"Epoch {epoch+1}/{params['epochs']}\")\n",
    "    model.fit(x_train, y_train, batch_size=params[\"batch_size\"], epochs=1, validation_data=(x_val, y_val))\n",
    "    # Test predictions on random samples\n",
    "    indices = np.random.randint(0, len(x_val), 10)\n",
    "    for idx in indices:\n",
    "        q = decode(x_val[idx])\n",
    "        t = decode(y_val[idx])\n",
    "        p = decode(model.predict(x_val[idx:idx+1])[0])\n",
    "        print(f\"Q: {q.strip()} | True: {t.strip()} | Pred: {p.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Q-1PNGxcxQ0"
   },
   "source": [
    "# **PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7j5LI4K1pORQ",
    "outputId": "4e663c33-6158-42e5-8424-2a2883c5eaeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.0166, Val Loss: 2.0417, Accuracy: 0.2742\n",
      "Q: 8897+1019 | True: 9916 | Pred: 1119\n",
      "Q: 6672+3345 | True: 10017 | Pred: 1019\n",
      "Q: 457-2491 | True: -2034 | Pred: -236\n",
      "Q: 8594+9957 | True: 18551 | Pred: 11119\n",
      "Q: 4640+7946 | True: 12586 | Pred: 1019\n",
      "Epoch 2/50, Loss: 0.0149, Val Loss: 1.8173, Accuracy: 0.3248\n",
      "Epoch 3/50, Loss: 0.0138, Val Loss: 1.7091, Accuracy: 0.3559\n",
      "Epoch 4/50, Loss: 0.0131, Val Loss: 1.6548, Accuracy: 0.3706\n",
      "Epoch 5/50, Loss: 0.0127, Val Loss: 1.6088, Accuracy: 0.3996\n",
      "Epoch 6/50, Loss: 0.0125, Val Loss: 1.5770, Accuracy: 0.4116\n",
      "Epoch 7/50, Loss: 0.0123, Val Loss: 1.5585, Accuracy: 0.4112\n",
      "Epoch 8/50, Loss: 0.0121, Val Loss: 1.5317, Accuracy: 0.4226\n",
      "Epoch 9/50, Loss: 0.0120, Val Loss: 1.5146, Accuracy: 0.4272\n",
      "Epoch 10/50, Loss: 0.0119, Val Loss: 1.5010, Accuracy: 0.4301\n",
      "Epoch 11/50, Loss: 0.0118, Val Loss: 1.4900, Accuracy: 0.4357\n",
      "Q: 5916-3703 | True: 2213 | Pred: 2199\n",
      "Q: 8615-8058 | True: 557 | Pred: 103\n",
      "Q: 500+4359 | True: 4859 | Pred: 5199\n",
      "Q: 5218+35 | True: 5253 | Pred: 199\n",
      "Q: 4202-6013 | True: -1811 | Pred: -1529\n",
      "Epoch 12/50, Loss: 0.0117, Val Loss: 1.4798, Accuracy: 0.4337\n",
      "Epoch 13/50, Loss: 0.0116, Val Loss: 1.4728, Accuracy: 0.4362\n",
      "Epoch 14/50, Loss: 0.0115, Val Loss: 1.4646, Accuracy: 0.4390\n",
      "Epoch 15/50, Loss: 0.0114, Val Loss: 1.4536, Accuracy: 0.4443\n",
      "Epoch 16/50, Loss: 0.0114, Val Loss: 1.4474, Accuracy: 0.4483\n",
      "Epoch 17/50, Loss: 0.0113, Val Loss: 1.4398, Accuracy: 0.4504\n",
      "Epoch 18/50, Loss: 0.0113, Val Loss: 1.4469, Accuracy: 0.4466\n",
      "Epoch 19/50, Loss: 0.0112, Val Loss: 1.4281, Accuracy: 0.4539\n",
      "Epoch 20/50, Loss: 0.0111, Val Loss: 1.4309, Accuracy: 0.4511\n",
      "Epoch 21/50, Loss: 0.0110, Val Loss: 1.4126, Accuracy: 0.4583\n",
      "Q: 7523-8175 | True: -652 | Pred: -546\n",
      "Q: 7624-600 | True: 7024 | Pred: 7999\n",
      "Q: 8524+3430 | True: 11954 | Pred: 12019\n",
      "Q: 6377-6185 | True: 192 | Pred: 209\n",
      "Q: 457-619 | True: -162 | Pred: -309\n",
      "Epoch 22/50, Loss: 0.0110, Val Loss: 1.4605, Accuracy: 0.4438\n",
      "Epoch 23/50, Loss: 0.0110, Val Loss: 1.3996, Accuracy: 0.4621\n",
      "Epoch 24/50, Loss: 0.0109, Val Loss: 1.3926, Accuracy: 0.4640\n",
      "Epoch 25/50, Loss: 0.0108, Val Loss: 1.3952, Accuracy: 0.4623\n",
      "Epoch 26/50, Loss: 0.0108, Val Loss: 1.3783, Accuracy: 0.4686\n",
      "Epoch 27/50, Loss: 0.0107, Val Loss: 1.4031, Accuracy: 0.4592\n",
      "Epoch 28/50, Loss: 0.0107, Val Loss: 1.3785, Accuracy: 0.4669\n",
      "Epoch 29/50, Loss: 0.0106, Val Loss: 1.3780, Accuracy: 0.4639\n",
      "Epoch 30/50, Loss: 0.0105, Val Loss: 1.3516, Accuracy: 0.4776\n",
      "Epoch 31/50, Loss: 0.0105, Val Loss: 1.3530, Accuracy: 0.4755\n",
      "Q: 1858+1391 | True: 3249 | Pred: 3076\n",
      "Q: 5432+7796 | True: 13228 | Pred: 13202\n",
      "Q: 843-7112 | True: -6269 | Pred: -6199\n",
      "Q: 8736-3331 | True: 5405 | Pred: 5199\n",
      "Q: 7672-5444 | True: 2228 | Pred: 2399\n",
      "Epoch 32/50, Loss: 0.0104, Val Loss: 1.3448, Accuracy: 0.4768\n",
      "Epoch 33/50, Loss: 0.0103, Val Loss: 1.3237, Accuracy: 0.4924\n",
      "Epoch 34/50, Loss: 0.0103, Val Loss: 1.3494, Accuracy: 0.4747\n",
      "Epoch 35/50, Loss: 0.0103, Val Loss: 1.3428, Accuracy: 0.4754\n",
      "Epoch 36/50, Loss: 0.0102, Val Loss: 1.3301, Accuracy: 0.4817\n",
      "Epoch 37/50, Loss: 0.0102, Val Loss: 1.3187, Accuracy: 0.4883\n",
      "Epoch 38/50, Loss: 0.0101, Val Loss: 1.2961, Accuracy: 0.4991\n",
      "Epoch 39/50, Loss: 0.0100, Val Loss: 1.3212, Accuracy: 0.4824\n",
      "Epoch 40/50, Loss: 0.0100, Val Loss: 1.2756, Accuracy: 0.5104\n",
      "Epoch 41/50, Loss: 0.0100, Val Loss: 1.2956, Accuracy: 0.4924\n",
      "Q: 9254+3170 | True: 12424 | Pred: 12591\n",
      "Q: 7335-5010 | True: 2325 | Pred: 2339\n",
      "Q: 4588-476 | True: 4112 | Pred: 4019\n",
      "Q: 393+3386 | True: 3779 | Pred: 3601\n",
      "Q: 1076-5965 | True: -4889 | Pred: -4911\n",
      "Epoch 42/50, Loss: 0.0100, Val Loss: 1.3056, Accuracy: 0.4863\n",
      "Epoch 43/50, Loss: 0.0099, Val Loss: 1.2858, Accuracy: 0.4952\n",
      "Epoch 44/50, Loss: 0.0098, Val Loss: 1.2566, Accuracy: 0.5152\n",
      "Epoch 45/50, Loss: 0.0098, Val Loss: 1.2683, Accuracy: 0.5063\n",
      "Epoch 46/50, Loss: 0.0098, Val Loss: 1.2489, Accuracy: 0.5150\n",
      "Epoch 47/50, Loss: 0.0097, Val Loss: 1.2483, Accuracy: 0.5161\n",
      "Epoch 48/50, Loss: 0.0097, Val Loss: 1.2504, Accuracy: 0.5117\n",
      "Epoch 49/50, Loss: 0.0097, Val Loss: 1.2407, Accuracy: 0.5196\n",
      "Epoch 50/50, Loss: 0.0096, Val Loss: 1.2362, Accuracy: 0.5218\n",
      "Q: 6406+84 | True: 6490 | Pred: 5999\n",
      "Q: 4686-3354 | True: 1332 | Pred: 1309\n",
      "Q: 4932+6976 | True: 11908 | Pred: 11811\n",
      "Q: 7564-279 | True: 7285 | Pred: 7909\n",
      "Q: 8352-7160 | True: 1192 | Pred: 1301\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    \"training_size\": 50000,\n",
    "    \"digits\": 4,\n",
    "    \"hidden_size\": 128,\n",
    "    \"embed_dim\": 32,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 50,\n",
    "    \"lr\": 0.001,\n",
    "}\n",
    "\n",
    "maxlen = params[\"digits\"] + 1 + params[\"digits\"]  # Input sequence length\n",
    "chars = \"0123456789+- \"\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Character Table\n",
    "ctable = {\n",
    "    \"char_to_index\": {c: i for i, c in enumerate(chars)},\n",
    "    \"index_to_char\": {i: c for i, c in enumerate(chars)},\n",
    "}\n",
    "\n",
    "# Data Encoding\n",
    "def encode(seq, maxlen):\n",
    "    x = np.zeros((maxlen, vocab_size), dtype=np.float32)\n",
    "    for i, c in enumerate(seq):\n",
    "        x[i, ctable[\"char_to_index\"][c]] = 1\n",
    "    return x\n",
    "\n",
    "def decode(seq):\n",
    "    indices = seq.argmax(axis=-1)\n",
    "    return ''.join(ctable[\"index_to_char\"][idx] for idx in indices)\n",
    "\n",
    "# Generate Data\n",
    "questions, answers = [], []\n",
    "seen = set()\n",
    "while len(questions) < params[\"training_size\"]:\n",
    "    a = random.randint(0, 10 ** params[\"digits\"] - 1)\n",
    "    b = random.randint(0, 10 ** params[\"digits\"] - 1)\n",
    "    operation = random.choice([\"+\", \"-\"])\n",
    "    query = f\"{a}{operation}{b}\"\n",
    "    if query in seen:\n",
    "        continue\n",
    "    seen.add(query)\n",
    "    result = str(eval(query))\n",
    "    questions.append(query.ljust(maxlen))\n",
    "    answers.append(result.ljust(params[\"digits\"] + 1))\n",
    "\n",
    "x = np.zeros((len(questions), maxlen, vocab_size), dtype=np.float32)\n",
    "y = np.zeros((len(questions), params[\"digits\"] + 1, vocab_size), dtype=np.float32)\n",
    "for i, (question, answer) in enumerate(zip(questions, answers)):\n",
    "    x[i] = encode(question, maxlen)\n",
    "    y[i] = encode(answer, params[\"digits\"] + 1)\n",
    "\n",
    "# Train/Test Split\n",
    "split_at = len(x) - len(x) // 10\n",
    "x_train, x_val = x[:split_at], x[split_at:]\n",
    "y_train, y_val = y[:split_at], y[split_at:]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train, y_train = torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "x_val, y_val = torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Model Definition\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, output_len):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.embedding = nn.Linear(vocab_size, embed_dim)\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embed_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, cell) = self.encoder(embedded)\n",
    "\n",
    "        # Decoding\n",
    "        decoder_input = torch.zeros(embedded.size(0), 1, embedded.size(-1)).to(embedded.device)\n",
    "        outputs = []\n",
    "        for _ in range(params[\"digits\"] + 1):\n",
    "            output, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))\n",
    "            step_output = self.fc(output.squeeze(1))\n",
    "            outputs.append(step_output.unsqueeze(1))\n",
    "            decoder_input = self.embedding(step_output.softmax(-1)).unsqueeze(1)  # Feedback prediction\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2Seq(vocab_size, params[\"embed_dim\"], params[\"hidden_size\"], params[\"digits\"] + 1).to(device)\n",
    "\n",
    "# Training Setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(params[\"epochs\"]):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(x_train), params[\"batch_size\"]):\n",
    "        x_batch = x_train[i:i+params[\"batch_size\"]].to(device)\n",
    "        y_batch = y_train[i:i+params[\"batch_size\"]].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), y_batch.argmax(-1).view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(x_val.to(device))\n",
    "        val_loss = criterion(val_outputs.view(-1, vocab_size), y_val.argmax(-1).view(-1))\n",
    "\n",
    "        # Accuracy Calculation\n",
    "        val_preds = val_outputs.argmax(-1).cpu().numpy()\n",
    "        val_targets = y_val.argmax(-1).cpu().numpy()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for pred_seq, target_seq in zip(val_preds, val_targets):\n",
    "            for pred_char, target_char in zip(pred_seq, target_seq):\n",
    "                if pred_char == target_char:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        accuracy = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{params['epochs']}, Loss: {total_loss/len(x_train):.4f}, \"\n",
    "          f\"Val Loss: {val_loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Print examples\n",
    "    if epoch % 10 == 0 or epoch == params[\"epochs\"] - 1:\n",
    "        indices = np.random.randint(0, len(x_val), 5)\n",
    "        for idx in indices:\n",
    "            q = decode(x_val[idx].cpu().numpy())\n",
    "            t = decode(y_val[idx].cpu().numpy())\n",
    "            p = decode(val_outputs[idx].detach().cpu().numpy())\n",
    "            print(f\"Q: {q.strip()} | True: {t.strip()} | Pred: {p.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ih1t2goTc8gf"
   },
   "source": [
    "# **Improved Model**\n",
    "\n",
    "Larger hidden size, smaller input size, embedding before LSTM, sparse categorial crossentropy loss function and integer encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvF_Cw25dHkS",
    "outputId": "9a7d7b00-f843-4c37-94fa-e254a94af19d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 153ms/step - accuracy: 0.2279 - loss: 2.2064 - val_accuracy: 0.2846 - val_loss: 1.9358\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step\n",
      "Q: 7006+9197 | True: 16203 | Pred: 12244\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 2242-504 | True: 1738 | Pred: -155\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 4314+9353 | True: 13667 | Pred: 12244\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 2841+9821 | True: 12662 | Pred: 12244\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 3343-9269 | True: -5926 | Pred: -2455\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 6530+7665 | True: 14195 | Pred: 12244\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 4926-7275 | True: -2349 | Pred: -255\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 7528-9394 | True: -1866 | Pred: -255\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 8582+8792 | True: 17374 | Pred: 11144\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 3625+4905 | True: 8530 | Pred: 12244\n",
      "Epoch 2/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 157ms/step - accuracy: 0.3144 - loss: 1.8455 - val_accuracy: 0.3624 - val_loss: 1.6971\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 8115+5619 | True: 13734 | Pred: 13255\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Q: 2150-1417 | True: 733 | Pred: 120\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 173-8330 | True: -8157 | Pred: -7855\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 2353-4286 | True: -1933 | Pred: -155\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 4335+9044 | True: 13379 | Pred: 13055\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 310-4052 | True: -3742 | Pred: -3555\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8818-457 | True: 8361 | Pred: 6800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5343+611 | True: 5954 | Pred: 6025\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 5687+3592 | True: 9279 | Pred: 8000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 8867-1283 | True: 7584 | Pred: 6550\n",
      "Epoch 3/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 153ms/step - accuracy: 0.3709 - loss: 1.6654 - val_accuracy: 0.3870 - val_loss: 1.6145\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5778-6526 | True: -748 | Pred: -113\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 7384+2411 | True: 9795 | Pred: 9011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 8958+1233 | True: 10191 | Pred: 9061\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 5229-1984 | True: 3245 | Pred: 2846\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Q: 4373-6330 | True: -1957 | Pred: -2163\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2460-3722 | True: -1262 | Pred: -1133\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 3920-554 | True: 3366 | Pred: 4736\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 7437+2001 | True: 9438 | Pred: 9061\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 263-8303 | True: -8040 | Pred: -8666\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5817-9482 | True: -3665 | Pred: -3466\n",
      "Epoch 4/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 154ms/step - accuracy: 0.3966 - loss: 1.5845 - val_accuracy: 0.4160 - val_loss: 1.5277\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 4257+6907 | True: 11164 | Pred: 11398\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Q: 8116-1248 | True: 6868 | Pred: 6433\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Q: 8024+1145 | True: 9169 | Pred: 9037\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 5452-8753 | True: -3301 | Pred: -3333\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 7276-4953 | True: 2323 | Pred: 2836\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Q: 8344-6062 | True: 2282 | Pred: 2836\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Q: 7051-713 | True: 6338 | Pred: 6506\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Q: 7055+516 | True: 7571 | Pred: 8099\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 6179-955 | True: 5224 | Pred: 4432\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 2902-7251 | True: -4349 | Pred: -4233\n",
      "Epoch 5/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 156ms/step - accuracy: 0.4186 - loss: 1.5237 - val_accuracy: 0.4320 - val_loss: 1.4896\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 2765+1217 | True: 3982 | Pred: 4544\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 522-446 | True: 76 | Pred: 11\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 1811+7206 | True: 9017 | Pred: 8114\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 7293+3303 | True: 10596 | Pred: 10696\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 7071-6771 | True: 300 | Pred: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 5975-3521 | True: 2454 | Pred: 2444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 6577+8759 | True: 15336 | Pred: 15699\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 4907-1651 | True: 3256 | Pred: 3544\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 9786-2279 | True: 7507 | Pred: 7119\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 6503-9469 | True: -2966 | Pred: -2554\n",
      "Epoch 6/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 156ms/step - accuracy: 0.4284 - loss: 1.4975 - val_accuracy: 0.4262 - val_loss: 1.4892\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 3280+8231 | True: 11511 | Pred: 11587\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9938+4427 | True: 14365 | Pred: 14188\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 4668-4123 | True: 545 | Pred: 406\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3990+3165 | True: 7155 | Pred: 7777\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 9086-1660 | True: 7426 | Pred: 6774\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Q: 4424-7823 | True: -3399 | Pred: -3277\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 566-6377 | True: -5811 | Pred: -5177\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 6954-8833 | True: -1879 | Pred: -1775\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 7297-3155 | True: 4142 | Pred: 3853\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3288-7093 | True: -3805 | Pred: -3277\n",
      "Epoch 7/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 153ms/step - accuracy: 0.4380 - loss: 1.4689 - val_accuracy: 0.4358 - val_loss: 1.4725\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3993-8446 | True: -4453 | Pred: -4444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 787-8316 | True: -7529 | Pred: -7864\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 3506-3733 | True: -227 | Pred: -444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8876+777 | True: 9653 | Pred: 9554\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 214+2390 | True: 2604 | Pred: 2583\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3368+7540 | True: 10908 | Pred: 10821\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 6953-6778 | True: 175 | Pred: 414\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 1455-5617 | True: -4162 | Pred: -4844\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 95+9057 | True: 9152 | Pred: 8114\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 3788+9012 | True: 12800 | Pred: 12444\n",
      "Epoch 8/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 156ms/step - accuracy: 0.4423 - loss: 1.4544 - val_accuracy: 0.4424 - val_loss: 1.4461\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 9281+7685 | True: 16966 | Pred: 17233\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 3450+9523 | True: 12973 | Pred: 12466\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 7064-4505 | True: 2559 | Pred: 2241\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Q: 2229+8028 | True: 10257 | Pred: 9001\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 2422-4536 | True: -2114 | Pred: -2207\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 4217-5419 | True: -1202 | Pred: -1396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8747-3964 | True: 4783 | Pred: 4231\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8078-7215 | True: 863 | Pred: 103\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 6242+1403 | True: 7645 | Pred: 7884\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8115+5619 | True: 13734 | Pred: 13966\n",
      "Epoch 9/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 154ms/step - accuracy: 0.4502 - loss: 1.4392 - val_accuracy: 0.4462 - val_loss: 1.4407\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 8903+3976 | True: 12879 | Pred: 12333\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 1354-4985 | True: -3631 | Pred: -3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 638-166 | True: 472 | Pred: 401\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5663-571 | True: 5092 | Pred: 5444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 4937+1928 | True: 6865 | Pred: 6333\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 5847-1144 | True: 4703 | Pred: 4433\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 5059+3100 | True: 8159 | Pred: 7533\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 2089+9347 | True: 11436 | Pred: 11383\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 6947+6664 | True: 13611 | Pred: 13183\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3743-116 | True: 3627 | Pred: 3513\n",
      "Epoch 10/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - accuracy: 0.4564 - loss: 1.4184 - val_accuracy: 0.4552 - val_loss: 1.4173\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 503+7825 | True: 8328 | Pred: 8031\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 272+9455 | True: 9727 | Pred: 9731\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 382+4687 | True: 5069 | Pred: 4339\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 4093-685 | True: 3408 | Pred: 3433\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 7408-9096 | True: -1688 | Pred: -1819\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 8224-7353 | True: 871 | Pred: 111\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 1175+2678 | True: 3853 | Pred: 4339\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 4331+8193 | True: 12524 | Pred: 12333\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 6537-4186 | True: 2351 | Pred: 2341\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3162-7086 | True: -3924 | Pred: -4999\n",
      "Epoch 11/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 155ms/step - accuracy: 0.4659 - loss: 1.4004 - val_accuracy: 0.4587 - val_loss: 1.4092\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 5264-5662 | True: -398 | Pred: -454\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8977+6771 | True: 15748 | Pred: 15444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 7236+6655 | True: 13891 | Pred: 13444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 728-6486 | True: -5758 | Pred: -5244\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 2299+9921 | True: 12220 | Pred: 12204\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 9455-6798 | True: 2657 | Pred: 2942\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9992-5595 | True: 4397 | Pred: 4207\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 9231-3758 | True: 5473 | Pred: 5707\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 2309-8486 | True: -6177 | Pred: -6254\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2535+3980 | True: 6515 | Pred: 6742\n",
      "Epoch 12/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 155ms/step - accuracy: 0.4673 - loss: 1.3926 - val_accuracy: 0.4576 - val_loss: 1.4015\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 5622-4023 | True: 1599 | Pred: 1718\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 3668+991 | True: 4659 | Pred: 4400\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 8644+6321 | True: 14965 | Pred: 14221\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 7284-6401 | True: 883 | Pred: 903\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 5963+7882 | True: 13845 | Pred: 13151\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 7760+7247 | True: 15007 | Pred: 14221\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4202-8748 | True: -4546 | Pred: -4355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8272+4068 | True: 12340 | Pred: 12221\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 6447-7657 | True: -1210 | Pred: -1177\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 5066-1980 | True: 3086 | Pred: 3158\n",
      "Epoch 13/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - accuracy: 0.4700 - loss: 1.3848 - val_accuracy: 0.4771 - val_loss: 1.3635\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8464+861 | True: 9325 | Pred: 9567\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 1577-5798 | True: -4221 | Pred: -4399\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9068-4000 | True: 5068 | Pred: 4733\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Q: 9101-257 | True: 8844 | Pred: 8699\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 8956+4383 | True: 13339 | Pred: 13999\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 8981+9134 | True: 18115 | Pred: 18299\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 5816-9898 | True: -4082 | Pred: -4066\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 4879+5920 | True: 10799 | Pred: 10759\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 6910+2757 | True: 9667 | Pred: 9571\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 1288-7693 | True: -6405 | Pred: -6999\n",
      "Epoch 14/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 157ms/step - accuracy: 0.4788 - loss: 1.3667 - val_accuracy: 0.4749 - val_loss: 1.3661\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8599+6314 | True: 14913 | Pred: 14922\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8706+9091 | True: 17797 | Pred: 18288\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 8565+8599 | True: 17164 | Pred: 17277\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 4138+3838 | True: 7976 | Pred: 7032\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2150-1417 | True: 733 | Pred: 706\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 7605-7460 | True: 145 | Pred: 22\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Q: 4779+2294 | True: 7073 | Pred: 7132\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Q: 5206-4458 | True: 748 | Pred: 706\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Q: 6516+8605 | True: 15121 | Pred: 15922\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Q: 3761-123 | True: 3638 | Pred: 3502\n",
      "Epoch 15/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 154ms/step - accuracy: 0.4821 - loss: 1.3560 - val_accuracy: 0.4818 - val_loss: 1.3457\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 6791-7997 | True: -1206 | Pred: -1288\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2897-4248 | True: -1351 | Pred: -1512\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 2701-984 | True: 1717 | Pred: 2172\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 7283-1004 | True: 6279 | Pred: 6463\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5063-3637 | True: 1426 | Pred: 1412\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 2767+9637 | True: 12404 | Pred: 12311\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 634-5337 | True: -4703 | Pred: -4359\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 6889-7420 | True: -531 | Pred: -559\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 7722+3835 | True: 11557 | Pred: 11621\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 1880-2682 | True: -802 | Pred: -750\n",
      "Epoch 16/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 154ms/step - accuracy: 0.4872 - loss: 1.3396 - val_accuracy: 0.4682 - val_loss: 1.3715\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 5787+6291 | True: 12078 | Pred: 11833\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 1013-4084 | True: -3071 | Pred: -3133\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 3178-5051 | True: -1873 | Pred: -1903\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 7380+2369 | True: 9749 | Pred: 9771\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 6285+1131 | True: 7416 | Pred: 7537\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 4037+9864 | True: 13901 | Pred: 13883\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 6942-4887 | True: 2055 | Pred: 2044\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 4734+6426 | True: 11160 | Pred: 11033\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 9948+6227 | True: 16175 | Pred: 16333\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 5291+2497 | True: 7788 | Pred: 7833\n",
      "Epoch 17/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 158ms/step - accuracy: 0.4860 - loss: 1.3415 - val_accuracy: 0.4893 - val_loss: 1.3218\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 3747+2091 | True: 5838 | Pred: 5826\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2755+822 | True: 3577 | Pred: 3429\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 7704-4878 | True: 2826 | Pred: 2742\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3979+2278 | True: 6257 | Pred: 6244\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 5058-2601 | True: 2457 | Pred: 2488\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 7711+1049 | True: 8760 | Pred: 8864\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 2981-1385 | True: 1596 | Pred: 1484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 4043+4810 | True: 8853 | Pred: 8824\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 4659+4164 | True: 8823 | Pred: 8824\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 136+5254 | True: 5390 | Pred: 5333\n",
      "Epoch 18/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - accuracy: 0.4943 - loss: 1.3215 - val_accuracy: 0.4723 - val_loss: 1.3497\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 3790-7883 | True: -4093 | Pred: -4055\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 7486+5015 | True: 12501 | Pred: 12415\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 7254+9907 | True: 17161 | Pred: 17118\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3477-1287 | True: 2190 | Pred: 2291\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 8926-1846 | True: 7080 | Pred: 7481\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 6800-4553 | True: 2247 | Pred: 2211\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 2477+7448 | True: 9925 | Pred: 9881\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 8905-1630 | True: 7275 | Pred: 7481\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3792-9558 | True: -5766 | Pred: -5555\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 4991+9910 | True: 14901 | Pred: 14781\n",
      "Epoch 19/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 158ms/step - accuracy: 0.4961 - loss: 1.3151 - val_accuracy: 0.4910 - val_loss: 1.3058\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 1120-7822 | True: -6702 | Pred: -6855\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 9537+5863 | True: 15400 | Pred: 15555\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 828+3914 | True: 4742 | Pred: 4038\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 7646+3759 | True: 11405 | Pred: 11363\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 3742-4800 | True: -1058 | Pred: -1122\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 9487-5359 | True: 4128 | Pred: 4113\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 4556-2481 | True: 2075 | Pred: 2128\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 6102-2797 | True: 3305 | Pred: 3388\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 1241-6173 | True: -4932 | Pred: -5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 7496+4846 | True: 12342 | Pred: 12206\n",
      "Epoch 20/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 158ms/step - accuracy: 0.5065 - loss: 1.2882 - val_accuracy: 0.5033 - val_loss: 1.2849\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 2416-8711 | True: -6295 | Pred: -6365\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 6635+3255 | True: 9890 | Pred: 9903\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3144-7561 | True: -4417 | Pred: -4466\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 4772+6375 | True: 11147 | Pred: 11193\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 5083-2165 | True: 2918 | Pred: 2988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 9869-6061 | True: 3808 | Pred: 3683\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 7359+4430 | True: 11789 | Pred: 11796\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 602+6516 | True: 7118 | Pred: 7216\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 8864+6157 | True: 15021 | Pred: 15255\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 7242-1478 | True: 5764 | Pred: 5773\n",
      "Epoch 21/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 162ms/step - accuracy: 0.5165 - loss: 1.2675 - val_accuracy: 0.5062 - val_loss: 1.2721\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 1013-4084 | True: -3071 | Pred: -2906\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 1822-3698 | True: -1876 | Pred: -1800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Q: 6922+4554 | True: 11476 | Pred: 11406\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Q: 167-1451 | True: -1284 | Pred: -1380\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 9560+9442 | True: 19002 | Pred: 18011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Q: 9773-5597 | True: 4176 | Pred: 4211\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 8750-2825 | True: 5925 | Pred: 6000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Q: 7080-4655 | True: 2425 | Pred: 2468\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 2489+8014 | True: 10503 | Pred: 10376\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Q: 9275+3186 | True: 12461 | Pred: 12466\n",
      "Epoch 22/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 154ms/step - accuracy: 0.5198 - loss: 1.2536 - val_accuracy: 0.5030 - val_loss: 1.2722\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 3722-4113 | True: -391 | Pred: -386\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 1660+4571 | True: 6231 | Pred: 6254\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 804+6996 | True: 7800 | Pred: 7234\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8564+9875 | True: 18439 | Pred: 18586\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 728-6627 | True: -5899 | Pred: -5899\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Q: 1193-7433 | True: -6240 | Pred: -6239\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Q: 9539-6835 | True: 2704 | Pred: 2588\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Q: 5129-5629 | True: -500 | Pred: -488\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 6101-2467 | True: 3634 | Pred: 3558\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Q: 9882+9129 | True: 19011 | Pred: 18576\n",
      "Epoch 23/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - accuracy: 0.5213 - loss: 1.2440 - val_accuracy: 0.5086 - val_loss: 1.2533\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 8439-8296 | True: 143 | Pred: 11\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 7359+4430 | True: 11789 | Pred: 11810\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 5150+8751 | True: 13901 | Pred: 13801\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2540+3872 | True: 6412 | Pred: 6268\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 4098+3557 | True: 7655 | Pred: 7430\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 9973-8634 | True: 1339 | Pred: 1330\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 6831-5617 | True: 1214 | Pred: 1230\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9853-8053 | True: 1800 | Pred: 1614\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 9110-2425 | True: 6685 | Pred: 6530\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 1292-9126 | True: -7834 | Pred: -7927\n",
      "Epoch 24/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 155ms/step - accuracy: 0.5213 - loss: 1.2375 - val_accuracy: 0.5062 - val_loss: 1.2653\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 6474+2275 | True: 8749 | Pred: 8734\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 975+3435 | True: 4410 | Pred: 4288\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 6312+4642 | True: 10954 | Pred: 10988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 6340+6939 | True: 13279 | Pred: 13088\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 27+6643 | True: 6670 | Pred: 6884\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 1526-5230 | True: -3704 | Pred: -3729\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 9848-4729 | True: 5119 | Pred: 5207\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 3314+0 | True: 3314 | Pred: 3547\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 6288+3046 | True: 9334 | Pred: 9228\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 3178-5051 | True: -1873 | Pred: -1904\n",
      "Epoch 25/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 158ms/step - accuracy: 0.5278 - loss: 1.2232 - val_accuracy: 0.5038 - val_loss: 1.2612\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 9331-2738 | True: 6593 | Pred: 6649\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 2161-5883 | True: -3722 | Pred: -3622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 800+9300 | True: 10100 | Pred: 1001\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9205+9384 | True: 18589 | Pred: 18023\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 994+3420 | True: 4414 | Pred: 4446\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 2981-1385 | True: 1596 | Pred: 1664\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 7159+2207 | True: 9366 | Pred: 9368\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 9882-836 | True: 9046 | Pred: 9999\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 7828-3870 | True: 3958 | Pred: 3914\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 5348+9468 | True: 14816 | Pred: 14829\n",
      "Epoch 26/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 156ms/step - accuracy: 0.5350 - loss: 1.2090 - val_accuracy: 0.5266 - val_loss: 1.2147\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3508-2634 | True: 874 | Pred: 900\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 2589+8948 | True: 11537 | Pred: 11596\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 9612+6689 | True: 16301 | Pred: 16346\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2579+423 | True: 3002 | Pred: 3816\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 8845-8522 | True: 323 | Pred: 479\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 9700-2312 | True: 7388 | Pred: 7236\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 7737-1087 | True: 6650 | Pred: 6739\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 7621-2698 | True: 4923 | Pred: 4913\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 8867-1283 | True: 7584 | Pred: 7786\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 4215+5127 | True: 9342 | Pred: 9412\n",
      "Epoch 27/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 157ms/step - accuracy: 0.5368 - loss: 1.2018 - val_accuracy: 0.5383 - val_loss: 1.1947\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 5490+7815 | True: 13305 | Pred: 13331\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9259+6638 | True: 15897 | Pred: 15757\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 8365+3293 | True: 11658 | Pred: 11611\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2376-3889 | True: -1513 | Pred: -1512\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9053+2057 | True: 11110 | Pred: 11071\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 9299-6024 | True: 3275 | Pred: 3277\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 2130-545 | True: 1585 | Pred: 1618\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 499-7126 | True: -6627 | Pred: -6673\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 5398+1454 | True: 6852 | Pred: 6817\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 1199+9209 | True: 10408 | Pred: 10400\n",
      "Epoch 28/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 157ms/step - accuracy: 0.5477 - loss: 1.1765 - val_accuracy: 0.5294 - val_loss: 1.2064\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 6097+1933 | True: 8030 | Pred: 7929\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4365+1893 | True: 6258 | Pred: 6227\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 2350-4353 | True: -2003 | Pred: -2900\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9278+211 | True: 9489 | Pred: 9565\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Q: 3993-8446 | True: -4453 | Pred: -4456\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 57-936 | True: -879 | Pred: -819\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 480+3823 | True: 4303 | Pred: 4329\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 8679-9083 | True: -404 | Pred: -499\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 9629+9477 | True: 19106 | Pred: 19221\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 686+92 | True: 778 | Pred: 621\n",
      "Epoch 29/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 158ms/step - accuracy: 0.5461 - loss: 1.1758 - val_accuracy: 0.5395 - val_loss: 1.1795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 2379-423 | True: 1956 | Pred: 2095\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 957+7713 | True: 8670 | Pred: 8860\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 7942-4525 | True: 3417 | Pred: 3450\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2341+9681 | True: 12022 | Pred: 12022\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 3400-738 | True: 2662 | Pred: 2588\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 9000+8097 | True: 17097 | Pred: 17455\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 5036+606 | True: 5642 | Pred: 5660\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 9997+4892 | True: 14889 | Pred: 14788\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 1727+212 | True: 1939 | Pred: 1984\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 1882-2272 | True: -390 | Pred: -480\n",
      "Epoch 30/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - accuracy: 0.5532 - loss: 1.1598 - val_accuracy: 0.5348 - val_loss: 1.1942\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 167-1451 | True: -1284 | Pred: -1386\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 8636+7273 | True: 15909 | Pred: 15858\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4844-7025 | True: -2181 | Pred: -2146\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 7690-3836 | True: 3854 | Pred: 3812\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 4211+9983 | True: 14194 | Pred: 14101\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 5705-7909 | True: -2204 | Pred: -2108\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 1990+4886 | True: 6876 | Pred: 6888\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 5356+7576 | True: 12932 | Pred: 12989\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 7768-2448 | True: 5320 | Pred: 5384\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 7149+4764 | True: 11913 | Pred: 11900\n",
      "Epoch 31/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 158ms/step - accuracy: 0.5570 - loss: 1.1511 - val_accuracy: 0.5447 - val_loss: 1.1670\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 8065-1007 | True: 7058 | Pred: 7967\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 6858-5562 | True: 1296 | Pred: 1314\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 5149+8571 | True: 13720 | Pred: 13604\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 9919-9519 | True: 400 | Pred: 400\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9938+3336 | True: 13274 | Pred: 13206\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 8365-3390 | True: 4975 | Pred: 5017\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Q: 4381-3344 | True: 1037 | Pred: 103\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 5291-7330 | True: -2039 | Pred: -2004\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 98-7267 | True: -7169 | Pred: -7176\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 4019+2512 | True: 6531 | Pred: 6662\n",
      "Epoch 32/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - accuracy: 0.5620 - loss: 1.1420 - val_accuracy: 0.5260 - val_loss: 1.2120\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 2178-6787 | True: -4609 | Pred: -4544\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 7006+9197 | True: 16203 | Pred: 16310\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 161-8819 | True: -8658 | Pred: -8793\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 6888-3964 | True: 2924 | Pred: 3188\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Q: 2270-8525 | True: -6255 | Pred: -6364\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 994+3420 | True: 4414 | Pred: 4444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 687-8359 | True: -7672 | Pred: -7798\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 5572+7489 | True: 13061 | Pred: 13099\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 6556-4858 | True: 1698 | Pred: 1718\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 2965+1564 | True: 4529 | Pred: 4492\n",
      "Epoch 33/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - accuracy: 0.5674 - loss: 1.1297 - val_accuracy: 0.5486 - val_loss: 1.1568\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8146-0 | True: 8146 | Pred: 8183\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 3189+2303 | True: 5492 | Pred: 5512\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 498+8826 | True: 9324 | Pred: 9395\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 8343-4004 | True: 4339 | Pred: 4255\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 7107-4864 | True: 2243 | Pred: 2222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 579-7816 | True: -7237 | Pred: -7169\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 1431-2702 | True: -1271 | Pred: -1299\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 9275+7297 | True: 16572 | Pred: 16588\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Q: 2309-8486 | True: -6177 | Pred: -6161\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Q: 7018+8134 | True: 15152 | Pred: 15153\n",
      "Epoch 34/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 158ms/step - accuracy: 0.5701 - loss: 1.1210 - val_accuracy: 0.5515 - val_loss: 1.1565\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 1436+1335 | True: 2771 | Pred: 2742\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 1877+53 | True: 1930 | Pred: 1974\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 3253-7921 | True: -4668 | Pred: -4666\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 1960+8657 | True: 10617 | Pred: 10546\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3056+7701 | True: 10757 | Pred: 10722\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 91+4904 | True: 4995 | Pred: 4064\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 8006-2857 | True: 5149 | Pred: 5111\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 3312-3744 | True: -432 | Pred: -422\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Q: 264+4697 | True: 4961 | Pred: 5011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 9844-2814 | True: 7030 | Pred: 6949\n",
      "Epoch 35/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 160ms/step - accuracy: 0.5714 - loss: 1.1195 - val_accuracy: 0.5558 - val_loss: 1.1431\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 7126+2403 | True: 9529 | Pred: 9515\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 6494+1942 | True: 8436 | Pred: 8404\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4829+1686 | True: 6515 | Pred: 6544\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 1048+8634 | True: 9682 | Pred: 9715\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 5356+7576 | True: 12932 | Pred: 12966\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 7722+1167 | True: 8889 | Pred: 8966\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Q: 5883+9036 | True: 14919 | Pred: 14949\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Q: 8908-159 | True: 8749 | Pred: 8904\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 7236+6655 | True: 13891 | Pred: 13856\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Q: 3265+3388 | True: 6653 | Pred: 6606\n",
      "Epoch 36/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 157ms/step - accuracy: 0.5751 - loss: 1.1092 - val_accuracy: 0.5620 - val_loss: 1.1308\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Q: 4967+4968 | True: 9935 | Pred: 9882\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Q: 6858-5562 | True: 1296 | Pred: 1239\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Q: 2148-9725 | True: -7577 | Pred: -7532\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 2919-7755 | True: -4836 | Pred: -4899\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 4640+6393 | True: 11033 | Pred: 11099\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Q: 3144-7561 | True: -4417 | Pred: -4499\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Q: 428-7602 | True: -7174 | Pred: -7199\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 3895+1374 | True: 5269 | Pred: 5256\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 9369-8165 | True: 1204 | Pred: 1199\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 3249-2006 | True: 1243 | Pred: 1386\n",
      "Epoch 37/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 155ms/step - accuracy: 0.5854 - loss: 1.0913 - val_accuracy: 0.5656 - val_loss: 1.1278\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 7005-7268 | True: -263 | Pred: -281\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 2172+324 | True: 2496 | Pred: 2414\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 9179-7357 | True: 1822 | Pred: 1812\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Q: 518-5154 | True: -4636 | Pred: -4699\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Q: 2824-4093 | True: -1269 | Pred: -1133\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Q: 5664-8428 | True: -2764 | Pred: -2816\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 9907-4774 | True: 5133 | Pred: 5219\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Q: 9205+9384 | True: 18589 | Pred: 18681\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Q: 4429+8795 | True: 13224 | Pred: 13243\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 207-1300 | True: -1093 | Pred: -1162\n",
      "Epoch 38/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - accuracy: 0.5834 - loss: 1.0899 - val_accuracy: 0.5604 - val_loss: 1.1354\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 3613+3189 | True: 6802 | Pred: 6742\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Q: 1813-613 | True: 1200 | Pred: 1279\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 5415+1675 | True: 7090 | Pred: 7094\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Q: 386+5753 | True: 6139 | Pred: 6110\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Q: 900-9256 | True: -8356 | Pred: -8496\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Q: 9028+4168 | True: 13196 | Pred: 13206\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Q: 9504-1245 | True: 8259 | Pred: 8117\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Q: 7051-713 | True: 6338 | Pred: 6301\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Q: 7995-769 | True: 7226 | Pred: 7180\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Q: 9174+6003 | True: 15177 | Pred: 15250\n",
      "Epoch 39/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - accuracy: 0.5853 - loss: 1.0910 - val_accuracy: 0.5594 - val_loss: 1.1351\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 3643-5273 | True: -1630 | Pred: -1644\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9809+4812 | True: 14621 | Pred: 14537\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 6806-3563 | True: 3243 | Pred: 3248\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 8794-2824 | True: 5970 | Pred: 5944\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 3722-4113 | True: -391 | Pred: -355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 2887-5453 | True: -2566 | Pred: -2534\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 979-9388 | True: -8409 | Pred: -8412\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3389+8443 | True: 11832 | Pred: 11888\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 1527-4675 | True: -3148 | Pred: -3131\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 2935-3150 | True: -215 | Pred: -244\n",
      "Epoch 40/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - accuracy: 0.5875 - loss: 1.0828 - val_accuracy: 0.5596 - val_loss: 1.1331\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 6079+7816 | True: 13895 | Pred: 13988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 6998-5864 | True: 1134 | Pred: 1128\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 5782-4837 | True: 945 | Pred: 876\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2481-2948 | True: -467 | Pred: -548\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 7059-5222 | True: 1837 | Pred: 1868\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 9585-6480 | True: 3105 | Pred: 3078\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 8295+9685 | True: 17980 | Pred: 17936\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 8963+3493 | True: 12456 | Pred: 12428\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 585-1733 | True: -1148 | Pred: -1182\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 4851-1254 | True: 3597 | Pred: 3588\n",
      "Epoch 41/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 160ms/step - accuracy: 0.5912 - loss: 1.0735 - val_accuracy: 0.5702 - val_loss: 1.1141\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 428-7602 | True: -7174 | Pred: -7166\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8423-253 | True: 8170 | Pred: 8104\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 8146+7818 | True: 15964 | Pred: 15055\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 7793-1787 | True: 6006 | Pred: 6904\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 6873-3099 | True: 3774 | Pred: 3785\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 1446-2978 | True: -1532 | Pred: -1412\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 3311+1107 | True: 4418 | Pred: 4421\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Q: 9465-8642 | True: 823 | Pred: 876\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 6946+434 | True: 7380 | Pred: 7314\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 6294-4181 | True: 2113 | Pred: 2068\n",
      "Epoch 42/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 161ms/step - accuracy: 0.5971 - loss: 1.0610 - val_accuracy: 0.5576 - val_loss: 1.1400\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 6491+3306 | True: 9797 | Pred: 9863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4533-3667 | True: 866 | Pred: 876\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 4555-6611 | True: -2056 | Pred: -2014\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 9018+1612 | True: 10630 | Pred: 10522\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 4753+8482 | True: 13235 | Pred: 13255\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 2364-4829 | True: -2465 | Pred: -2491\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 1288-7693 | True: -6405 | Pred: -6362\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 2627-5003 | True: -2376 | Pred: -2432\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 4180-5474 | True: -1294 | Pred: -1452\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3316+4875 | True: 8191 | Pred: 8252\n",
      "Epoch 43/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - accuracy: 0.6026 - loss: 1.0531 - val_accuracy: 0.5706 - val_loss: 1.1075\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4364-5245 | True: -881 | Pred: -808\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 5957+4647 | True: 10604 | Pred: 10520\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 7848+898 | True: 8746 | Pred: 8981\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 4014+5976 | True: 9990 | Pred: 9901\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 6808+1024 | True: 7832 | Pred: 7800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 3392-8212 | True: -4820 | Pred: -4916\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 5105+3324 | True: 8429 | Pred: 8434\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3644+2353 | True: 5997 | Pred: 5977\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 2701-984 | True: 1717 | Pred: 1724\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 5318-5745 | True: -427 | Pred: -488\n",
      "Epoch 44/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - accuracy: 0.6050 - loss: 1.0485 - val_accuracy: 0.5715 - val_loss: 1.1125\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 788+4315 | True: 5103 | Pred: 5173\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 4768-7447 | True: -2679 | Pred: -2514\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Q: 2983-1428 | True: 1555 | Pred: 1613\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 1004-6709 | True: -5705 | Pred: -5840\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 8797+6610 | True: 15407 | Pred: 15457\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Q: 9397+4716 | True: 14113 | Pred: 14111\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 7462-1763 | True: 5699 | Pred: 5733\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 4668-4123 | True: 545 | Pred: 493\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 2804+3186 | True: 5990 | Pred: 6917\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 2963+1151 | True: 4114 | Pred: 4003\n",
      "Epoch 45/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - accuracy: 0.6115 - loss: 1.0333 - val_accuracy: 0.5775 - val_loss: 1.0971\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 4161+3111 | True: 7272 | Pred: 7266\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 2653-5634 | True: -2981 | Pred: -2924\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 4216+2587 | True: 6803 | Pred: 6769\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 941+9250 | True: 10191 | Pred: 10266\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 6720-7478 | True: -758 | Pred: -746\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 2921+3087 | True: 6008 | Pred: 6977\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 5076+2004 | True: 7080 | Pred: 7126\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 2244-5626 | True: -3382 | Pred: -3344\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 8773-5919 | True: 2854 | Pred: 2812\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 6928+3561 | True: 10489 | Pred: 10408\n",
      "Epoch 46/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - accuracy: 0.6125 - loss: 1.0305 - val_accuracy: 0.5806 - val_loss: 1.0955\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 8729+9977 | True: 18706 | Pred: 18613\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 8452+2382 | True: 10834 | Pred: 10822\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 1597-5842 | True: -4245 | Pred: -4299\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 5274-9972 | True: -4698 | Pred: -4724\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 4145+2920 | True: 7065 | Pred: 7039\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 7594+3479 | True: 11073 | Pred: 11034\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Q: 9874+9050 | True: 18924 | Pred: 19922\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 5696+6161 | True: 11857 | Pred: 11889\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 9809+4812 | True: 14621 | Pred: 14619\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 3558+6921 | True: 10479 | Pred: 10428\n",
      "Epoch 47/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 161ms/step - accuracy: 0.6171 - loss: 1.0212 - val_accuracy: 0.5780 - val_loss: 1.1044\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 9780+4621 | True: 14401 | Pred: 14454\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 5627-7567 | True: -1940 | Pred: -1914\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 3302+8495 | True: 11797 | Pred: 11872\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 8486+1223 | True: 9709 | Pred: 9771\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 9228-6618 | True: 2610 | Pred: 2592\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 3930-1121 | True: 2809 | Pred: 2720\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 9741+7597 | True: 17338 | Pred: 17320\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 9859+1609 | True: 11468 | Pred: 11477\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 7711-5852 | True: 1859 | Pred: 1824\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 1872+4806 | True: 6678 | Pred: 6742\n",
      "Epoch 48/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 161ms/step - accuracy: 0.6148 - loss: 1.0231 - val_accuracy: 0.5821 - val_loss: 1.0944\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 9844-2814 | True: 7030 | Pred: 7935\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 1432+8864 | True: 10296 | Pred: 10268\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 7438-1095 | True: 6343 | Pred: 6233\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 641+6102 | True: 6743 | Pred: 6743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 3566+5153 | True: 8719 | Pred: 8733\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 4554-9136 | True: -4582 | Pred: -4653\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 6207+3839 | True: 10046 | Pred: 10045\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Q: 6000-377 | True: 5623 | Pred: 5611\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Q: 8916+293 | True: 9209 | Pred: 9184\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 8047-6364 | True: 1683 | Pred: 1663\n",
      "Epoch 49/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - accuracy: 0.6244 - loss: 1.0005 - val_accuracy: 0.5798 - val_loss: 1.0930\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 4798+5341 | True: 10139 | Pred: 10117\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 8736+1151 | True: 9887 | Pred: 9831\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 8766+3325 | True: 12091 | Pred: 12017\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 5456+4784 | True: 10240 | Pred: 10226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3571+1297 | True: 4868 | Pred: 4843\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 9671-4227 | True: 5444 | Pred: 5214\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 6338+4095 | True: 10433 | Pred: 10463\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4031-6775 | True: -2744 | Pred: -2736\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3171-6352 | True: -3181 | Pred: -3251\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 5741-9928 | True: -4187 | Pred: -4295\n",
      "Epoch 50/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 160ms/step - accuracy: 0.6299 - loss: 0.9878 - val_accuracy: 0.5841 - val_loss: 1.0928\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 670-5377 | True: -4707 | Pred: -4727\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 7730+2978 | True: 10708 | Pred: 10708\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 9786-2279 | True: 7507 | Pred: 7500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Q: 6261-6023 | True: 238 | Pred: 350\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 5547+7115 | True: 12662 | Pred: 12664\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Q: 4377-424 | True: 3953 | Pred: 3956\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Q: 3526+6949 | True: 10475 | Pred: 10404\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 9404+654 | True: 10058 | Pred: 10001\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Q: 5720-9995 | True: -4275 | Pred: -4203\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Q: 4730+871 | True: 5601 | Pred: 5504\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, TimeDistributed, RepeatVector\n",
    "\n",
    "# Updated Hyperparameters\n",
    "params = {\n",
    "    \"training_size\": 50000,\n",
    "    \"digits\": 4,\n",
    "    \"hidden_size\": 256,  # Increased hidden size\n",
    "    \"embedding_size\": 50,  # Embedding dimension\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 50\n",
    "}\n",
    "\n",
    "maxlen = params[\"digits\"] + 1 + params[\"digits\"]\n",
    "chars = \"0123456789+- \"\n",
    "vocab_size = len(chars)\n",
    "ctable = {\n",
    "    \"char_to_index\": {c: i for i, c in enumerate(chars)},\n",
    "    \"index_to_char\": {i: c for i, c in enumerate(chars)},\n",
    "}\n",
    "\n",
    "def encode(seq, maxlen):\n",
    "    \"\"\"Integer encode the sequence.\"\"\"\n",
    "    x = [ctable[\"char_to_index\"][c] for c in seq]\n",
    "    return np.array(x + [0] * (maxlen - len(seq)))\n",
    "\n",
    "def decode(seq):\n",
    "    \"\"\"Decode integer-encoded sequence.\"\"\"\n",
    "    return ''.join(ctable[\"index_to_char\"][idx] for idx in seq)\n",
    "\n",
    "# Data Generation\n",
    "questions = []\n",
    "answers = []\n",
    "seen = set()\n",
    "while len(questions) < params[\"training_size\"]:\n",
    "    a = random.randint(0, 10 ** params[\"digits\"] - 1)\n",
    "    b = random.randint(0, 10 ** params[\"digits\"] - 1)\n",
    "    operation = random.choice([\"+\", \"-\"])\n",
    "    query = f\"{a}{operation}{b}\"\n",
    "    if query in seen:\n",
    "        continue\n",
    "    seen.add(query)\n",
    "    result = eval(query)  # Calculate the result\n",
    "    questions.append(query.ljust(maxlen))  # Padded to maxlen\n",
    "    answers.append(str(result).ljust(params[\"digits\"] + 1))  # Padded to output length\n",
    "\n",
    "x = np.array([encode(q, maxlen) for q in questions])\n",
    "y = np.array([encode(a, params[\"digits\"] + 1) for a in answers])\n",
    "\n",
    "# Train / Test Split\n",
    "split_at = len(x) - len(x) // 10\n",
    "x_train, x_val = x[:split_at], x[split_at:]\n",
    "y_train, y_val = y[:split_at], y[split_at:]\n",
    "\n",
    "# Model Definition with Embedding and Two LSTMs\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=params[\"embedding_size\"], input_length=maxlen))  # Embedding Layer\n",
    "model.add(LSTM(params[\"hidden_size\"]))  # Single Encoder LSTM Layer\n",
    "model.add(RepeatVector(params[\"digits\"] + 1))  # Repeat vector for Decoder\n",
    "model.add(LSTM(params[\"hidden_size\"], return_sequences=True))  # Decoder LSTM\n",
    "model.add(TimeDistributed(Dense(vocab_size, activation=\"softmax\")))  # Dense Output Layer\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Training\n",
    "for epoch in range(params[\"epochs\"]):\n",
    "    print(f\"Epoch {epoch+1}/{params['epochs']}\")\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        epochs=1,\n",
    "        validation_data=(x_val, y_val),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Test predictions on random samples\n",
    "    indices = np.random.randint(0, len(x_val), 10)\n",
    "    for idx in indices:\n",
    "        q = decode(x_val[idx])\n",
    "        t = decode(y_val[idx])\n",
    "        pred_probs = model.predict(x_val[idx:idx + 1])\n",
    "        pred = decode(np.argmax(pred_probs, axis=-1)[0])\n",
    "        print(f\"Q: {q.strip()} | True: {t.strip()} | Pred: {pred.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXJQqZbEbRup"
   },
   "source": [
    "1.2).\n",
    "\n",
    "a) Do you think this model performs well?  Why or why not?     \n",
    "b) What are its limitations?   \n",
    "c) What would you do to improve it?    \n",
    "d) Can you apply an attention mechanism to this model? Why or why not?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5fs19a93PGR"
   },
   "source": [
    "# **ANSWERS FOR 1.2**\n",
    "\n",
    "# **a)**\n",
    "\n",
    "The model performs reasonably well but has noticeable shortcomings that affect its overall reliability.\n",
    "\n",
    "Strengths:\n",
    "\n",
    "1) Ability to Learn Basic Patterns:\n",
    "\n",
    "The model can generate predictions that are often close to the correct outputs, indicating that it has learned some patterns of addition and subtraction.\n",
    "\n",
    "Training loss consistently decreases, showing the model effectively minimizes errors on the training set.\n",
    "\n",
    "2) Handling a Range of Inputs:\n",
    "\n",
    "The model performs adequately on randomly generated equations, showing it has generalized to a reasonable extent, especially for simpler cases.\n",
    "\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "1) Validation Loss Fluctuations:\n",
    "\n",
    "The validation loss fluctuates significantly and does not decrease as consistently as the training loss. This indicates the model struggles to generalize well to unseen data, potentially due to overfitting.\n",
    "\n",
    "2) Prediction Errors:\n",
    "\n",
    "The model makes some errors that deviate significantly from the correct answers.\n",
    "\n",
    "These errors may result from error propagation during decoding, where incorrect predictions in earlier time steps negatively affect later steps.\n",
    "\n",
    "3) Mismatch Between Training and Inference (Teacher Forcing):\n",
    "\n",
    "The model uses ground truth during training (teacher forcing) but relies on its own predictions during inference. This discrepancy can lead to cascading errors during testing.\n",
    "\n",
    "4) Simplistic Design:\n",
    "\n",
    "The model lacks attention mechanisms or other strategies that might help it focus on relevant parts of the input sequence, limiting its ability to handle more complex or edge case inputs.\n",
    "\n",
    "Overall Performance:\n",
    "\n",
    "The model performs adequately for simple examples but struggles with generalization and robustness, particularly for more complex or edge case inputs. its limitations, such as fluctuating validation loss and occasional large prediction errors, indicate that improvements are needed for reliable performance across all cases.\n",
    "\n",
    "In conclusion, the model performs moderately well but does not consistently achieve high accuracy, particularly on unseen data.\n",
    "\n",
    "# **b)**\n",
    "1) Error Propagation in Decoding Without Teacher Forcing:\n",
    "\n",
    "During inference, the model relies on its own predictions to generate the next input (instead of ground truth), leading to error accumulation. If an early prediction is incorrect, it can mislead subsequent predictions, especially for longer sequences.\n",
    "This can result in cascading errors, particularly for more complex problems or larger numbers.\n",
    "\n",
    "2) Lack of Rich Semantic Embedding:\n",
    "\n",
    "The embedding in the Keras implementation is implemented as a dense layer, which is less robust compared to learned embeddings like PyTorch's nn.Embedding. It treats input tokens as isolated entities and doesn't leverage relationships or patterns between them effectively.\n",
    "This limits the model’s ability to capture deeper structural relationships.\n",
    "\n",
    "3) Overfitting to Training Data:\n",
    "\n",
    "The model could memorize patterns in the training data, particularly when the training set is small or lacks diversity.\n",
    "This would reduce its ability to generalize to unseen or edge case equations.\n",
    "\n",
    "4) Lack of Attention Mechanism:\n",
    "\n",
    "The model lacks an attention mechanism, which could help it focus on specific parts of the input sequence during decoding.\n",
    "Without attention, the model processes all input tokens uniformly, which may hinder its ability to learn complex, position specific dependencies effectively.\n",
    "\n",
    "# **c)**\n",
    "\n",
    "1) Use a Learned Embedding Layer:\n",
    "The current embedding is a dense layer, which is less effective at capturing semantic relationships between input tokens.\n",
    "Replace the dense layer embedding with a proper learned embedding layer (for example PyTorch’s nn.Embedding). This approach will\n",
    "Learn a distributed representation of tokens during training and Better capture patterns and relationships between input tokens.\n",
    "\n",
    "2) Incorporate Attention Mechanisms:\n",
    "The model processes the input uniformly and struggles with position specific dependencies.\n",
    "The solution is to Add an attention mechanism to the decoder. This would allow the model to focus on relevant parts of the input sequence during each decoding step.\n",
    "This would enhance the model’s ability to handle long term dependencies and improve accuracy on complex arithmetic tasks.\n",
    "\n",
    "3) Use Pretraining or Transfer Learning:\n",
    "\n",
    "The model is trained from scratch, which can be slow and suboptimal for learning general patterns.\n",
    "Solution: Pretrain the encoder or decoder on related tasks (for example language modeling or sequence prediction) and fine tune for arithmetic tasks.\n",
    "Pretraining provides the model with a strong initialization, leading to faster convergence and improved performance.\n",
    "\n",
    "# **d)**\n",
    "Yes, attention can be integrated into this model because:\n",
    "\n",
    "1) Encoder-Decoder Architecture: Attention mechanisms are designed to work with encoder-decoder setups like the one in this model.\n",
    "\n",
    "2) Compatibility: Attention does not fundamentally alter the structure but enhances it by modifying how the decoder accesses the encoder’s output.\n",
    "\n",
    "3) Task Relevance: For sequence to sequence tasks like arithmetic equation solving, attention can help the model identify relevant parts of the input sequence for generating each output token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wvRhhOcgmrQ"
   },
   "source": [
    "1.3).  \n",
    "\n",
    "Add attention to the model. Evaluate the performance against the `seq2seq` you trained above. Which one is performing better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6NxnictUEYr"
   },
   "source": [
    "# **Answers for 1.3)**\n",
    "\n",
    "Performance:\n",
    "\n",
    "Accuracy: The model with attention achieves 56.5% training accuracy and 54.9% validation accuracy by the final epoch. While slightly lower than the basic model’s best accuracy, the attention model performs better in handling long sequences and edge cases.\n",
    "\n",
    "mproved Predictions:\n",
    "Predictions are closer to the true values compared to the vanilla model, especially for longer or more complex sequences.\n",
    "For examples:\n",
    "\n",
    "Q: 7851+2214 | True: 10065 | Pred: 10002 → Small error compared to larger deviations in the vanilla model.\n",
    "\n",
    "Q: 3910+4285 | True: 8195 | Pred: 8177 → More accurate than without attention.\n",
    "\n",
    "Benefits of Attention:\n",
    "\n",
    "Dynamic Context: The attention mechanism overcomes the bottleneck of a fixed length context vector by dynamically attending to relevant parts of the input sequence.\n",
    "\n",
    "Improved Long Sequence Handling: Predictions for longer sequences are more accurate and consistent.\n",
    "\n",
    "Error Reduction: The attention mechanism reduces large errors, particularly for sequences requiring precise arithmetic operations.\n",
    "\n",
    "The results show that while both models achieve similar overall accuracy, the attention mechanism provides a significant qualitative improvement in handling complex and longer sequences.\n",
    "\n",
    "\n",
    "Qualitative Performance:\n",
    "\n",
    "Error Handling:\n",
    "\n",
    "Without Attention:\n",
    "The predictions often deviate significantly for longer or complex sequences. Inaccurate for subtraction.\n",
    "\n",
    "With Attention:\n",
    "\n",
    "Errors are smaller and more localized.\n",
    "\n",
    "Handling of Long Sequences:\n",
    "\n",
    "The model with attention handles long sequences and complex operations better by focusing dynamically on relevant parts of the input.\n",
    "\n",
    "The fixed length context vector of the model without attention causes performance degradation for such cases.\n",
    "\n",
    "The attention mechanism provides a better ability to generalize to unseen or complex inputs. While its overall accuracy is slightly lower, it shows better alignment between input and output patterns, especially for tasks requiring more nuanced understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rf1zSskSZKBI"
   },
   "source": [
    "# **Code with Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkonHyZZNc6c",
    "outputId": "099b7841-6c3b-45ba-e271-a64d72822501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 62ms/step - accuracy: 0.2492 - loss: 2.1565 - val_accuracy: 0.2936 - val_loss: 1.9130\n",
      "Epoch 2/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 60ms/step - accuracy: 0.2948 - loss: 1.8914 - val_accuracy: 0.3076 - val_loss: 1.8328\n",
      "Epoch 3/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 63ms/step - accuracy: 0.3136 - loss: 1.8134 - val_accuracy: 0.3415 - val_loss: 1.7428\n",
      "Epoch 4/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 61ms/step - accuracy: 0.3448 - loss: 1.7202 - val_accuracy: 0.3568 - val_loss: 1.6798\n",
      "Epoch 5/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.3671 - loss: 1.6578 - val_accuracy: 0.3808 - val_loss: 1.6195\n",
      "Epoch 6/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 59ms/step - accuracy: 0.3886 - loss: 1.6119 - val_accuracy: 0.3900 - val_loss: 1.5922\n",
      "Epoch 7/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 58ms/step - accuracy: 0.3987 - loss: 1.5817 - val_accuracy: 0.4042 - val_loss: 1.5616\n",
      "Epoch 8/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 59ms/step - accuracy: 0.4107 - loss: 1.5523 - val_accuracy: 0.4120 - val_loss: 1.5421\n",
      "Epoch 9/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 62ms/step - accuracy: 0.4187 - loss: 1.5317 - val_accuracy: 0.4296 - val_loss: 1.5084\n",
      "Epoch 10/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - accuracy: 0.4289 - loss: 1.5093 - val_accuracy: 0.4391 - val_loss: 1.4880\n",
      "Epoch 11/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 62ms/step - accuracy: 0.4371 - loss: 1.4869 - val_accuracy: 0.4266 - val_loss: 1.5022\n",
      "Epoch 12/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - accuracy: 0.4443 - loss: 1.4679 - val_accuracy: 0.4500 - val_loss: 1.4465\n",
      "Epoch 13/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.4521 - loss: 1.4421 - val_accuracy: 0.4611 - val_loss: 1.4194\n",
      "Epoch 14/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - accuracy: 0.4601 - loss: 1.4152 - val_accuracy: 0.4636 - val_loss: 1.4000\n",
      "Epoch 15/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 62ms/step - accuracy: 0.4693 - loss: 1.3903 - val_accuracy: 0.4582 - val_loss: 1.3946\n",
      "Epoch 16/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 60ms/step - accuracy: 0.4735 - loss: 1.3722 - val_accuracy: 0.4734 - val_loss: 1.3600\n",
      "Epoch 17/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - accuracy: 0.4792 - loss: 1.3579 - val_accuracy: 0.4853 - val_loss: 1.3308\n",
      "Epoch 18/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 61ms/step - accuracy: 0.4857 - loss: 1.3379 - val_accuracy: 0.4912 - val_loss: 1.3213\n",
      "Epoch 19/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.4930 - loss: 1.3206 - val_accuracy: 0.4960 - val_loss: 1.3018\n",
      "Epoch 20/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 60ms/step - accuracy: 0.4986 - loss: 1.3041 - val_accuracy: 0.4871 - val_loss: 1.3104\n",
      "Epoch 21/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.4961 - loss: 1.3056 - val_accuracy: 0.5086 - val_loss: 1.2795\n",
      "Epoch 22/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 60ms/step - accuracy: 0.5058 - loss: 1.2818 - val_accuracy: 0.5040 - val_loss: 1.2836\n",
      "Epoch 23/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - accuracy: 0.5059 - loss: 1.2775 - val_accuracy: 0.5061 - val_loss: 1.2695\n",
      "Epoch 24/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 60ms/step - accuracy: 0.5103 - loss: 1.2681 - val_accuracy: 0.5229 - val_loss: 1.2496\n",
      "Epoch 25/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.5162 - loss: 1.2549 - val_accuracy: 0.5089 - val_loss: 1.2573\n",
      "Epoch 26/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 59ms/step - accuracy: 0.5205 - loss: 1.2445 - val_accuracy: 0.5168 - val_loss: 1.2447\n",
      "Epoch 27/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.5203 - loss: 1.2398 - val_accuracy: 0.5250 - val_loss: 1.2329\n",
      "Epoch 28/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.5198 - loss: 1.2406 - val_accuracy: 0.5213 - val_loss: 1.2294\n",
      "Epoch 29/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 58ms/step - accuracy: 0.5286 - loss: 1.2275 - val_accuracy: 0.5204 - val_loss: 1.2383\n",
      "Epoch 30/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5240 - loss: 1.2339 - val_accuracy: 0.5411 - val_loss: 1.2060\n",
      "Epoch 31/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - accuracy: 0.5356 - loss: 1.2113 - val_accuracy: 0.5418 - val_loss: 1.2054\n",
      "Epoch 32/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - accuracy: 0.5345 - loss: 1.2111 - val_accuracy: 0.5196 - val_loss: 1.2260\n",
      "Epoch 33/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 59ms/step - accuracy: 0.5352 - loss: 1.2082 - val_accuracy: 0.5120 - val_loss: 1.2420\n",
      "Epoch 34/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 56ms/step - accuracy: 0.5351 - loss: 1.2040 - val_accuracy: 0.5221 - val_loss: 1.2172\n",
      "Epoch 35/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 61ms/step - accuracy: 0.5354 - loss: 1.2047 - val_accuracy: 0.5292 - val_loss: 1.2014\n",
      "Epoch 36/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - accuracy: 0.5452 - loss: 1.1863 - val_accuracy: 0.5559 - val_loss: 1.1775\n",
      "Epoch 37/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.5468 - loss: 1.1829 - val_accuracy: 0.5398 - val_loss: 1.1925\n",
      "Epoch 38/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.5471 - loss: 1.1800 - val_accuracy: 0.5466 - val_loss: 1.1784\n",
      "Epoch 39/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 56ms/step - accuracy: 0.5456 - loss: 1.1811 - val_accuracy: 0.5452 - val_loss: 1.1765\n",
      "Epoch 40/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 61ms/step - accuracy: 0.5522 - loss: 1.1696 - val_accuracy: 0.5247 - val_loss: 1.2004\n",
      "Epoch 41/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 63ms/step - accuracy: 0.5480 - loss: 1.1784 - val_accuracy: 0.5358 - val_loss: 1.1841\n",
      "Epoch 42/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 62ms/step - accuracy: 0.5486 - loss: 1.1743 - val_accuracy: 0.5238 - val_loss: 1.2042\n",
      "Epoch 43/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 62ms/step - accuracy: 0.5471 - loss: 1.1752 - val_accuracy: 0.5463 - val_loss: 1.1764\n",
      "Epoch 44/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5536 - loss: 1.1623 - val_accuracy: 0.5391 - val_loss: 1.1715\n",
      "Epoch 45/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 63ms/step - accuracy: 0.5588 - loss: 1.1552 - val_accuracy: 0.5692 - val_loss: 1.1490\n",
      "Epoch 46/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - accuracy: 0.5620 - loss: 1.1494 - val_accuracy: 0.5152 - val_loss: 1.2548\n",
      "Epoch 47/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 59ms/step - accuracy: 0.5493 - loss: 1.1697 - val_accuracy: 0.5370 - val_loss: 1.1850\n",
      "Epoch 48/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.5629 - loss: 1.1445 - val_accuracy: 0.5510 - val_loss: 1.1571\n",
      "Epoch 49/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5573 - loss: 1.1530 - val_accuracy: 0.5694 - val_loss: 1.1399\n",
      "Epoch 50/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 61ms/step - accuracy: 0.5650 - loss: 1.1394 - val_accuracy: 0.5492 - val_loss: 1.1545\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470ms/step\n",
      "Q: 7851+2214 | True: 10065 | Pred: 10002\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 7391+5529 | True: 12920 | Pred: 12802\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Q: 3198-3256 | True: -58 | Pred: -1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Q: 4675-2037 | True: 2638 | Pred: 2605\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 5059+557 | True: 5616 | Pred: 5603\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 4944-6100 | True: -1156 | Pred: -1299\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 3910+4285 | True: 8195 | Pred: 8177\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Q: 8376-3661 | True: 4715 | Pred: 4750\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Q: 7936-6410 | True: 1526 | Pred: 1407\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Q: 446-9843 | True: -9397 | Pred: -9356\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, TimeDistributed, RepeatVector, Dot, Activation, Concatenate\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    \"training_size\": 50000,\n",
    "    \"digits\": 4,\n",
    "    \"hidden_size\": 128,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 50\n",
    "}\n",
    "\n",
    "maxlen = params[\"digits\"] + 1 + params[\"digits\"]\n",
    "chars = \"0123456789+- \"\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Character Table\n",
    "ctable = {\n",
    "    \"char_to_index\": {c: i for i, c in enumerate(chars)},\n",
    "    \"index_to_char\": {i: c for i, c in enumerate(chars)},\n",
    "}\n",
    "\n",
    "# Encoding/Decoding Functions\n",
    "def encode(seq, maxlen, vocab_size):\n",
    "    x = np.zeros((maxlen, vocab_size), dtype=np.float32)\n",
    "    for i, c in enumerate(seq):\n",
    "        x[i, ctable[\"char_to_index\"][c]] = 1\n",
    "    return x\n",
    "\n",
    "def decode(seq):\n",
    "    indices = seq.argmax(axis=-1)\n",
    "    return ''.join(ctable[\"index_to_char\"][idx] for idx in indices)\n",
    "\n",
    "# Generate Data\n",
    "questions = []\n",
    "answers = []\n",
    "seen = set()\n",
    "while len(questions) < params[\"training_size\"]:\n",
    "    a = random.randint(0, 10 ** params[\"digits\"] - 1)\n",
    "    b = random.randint(0, 10 ** params[\"digits\"] - 1)\n",
    "    operation = random.choice([\"+\", \"-\"])\n",
    "    query = f\"{a}{operation}{b}\"\n",
    "    if query in seen:\n",
    "        continue\n",
    "    seen.add(query)\n",
    "    result = eval(query)\n",
    "    questions.append(query.ljust(maxlen))\n",
    "    answers.append(str(result).ljust(params[\"digits\"] + 1))\n",
    "\n",
    "x = np.zeros((len(questions), maxlen, vocab_size), dtype=np.float32)\n",
    "y = np.zeros((len(questions), params[\"digits\"] + 1, vocab_size), dtype=np.float32)\n",
    "for i, (question, answer) in enumerate(zip(questions, answers)):\n",
    "    x[i] = encode(question, maxlen, vocab_size)\n",
    "    y[i] = encode(answer, params[\"digits\"] + 1, vocab_size)\n",
    "\n",
    "# Train / Test Split\n",
    "split_at = len(x) - len(x) // 10\n",
    "x_train, x_val = x[:split_at], x[split_at:]\n",
    "y_train, y_val = y[:split_at], y[split_at:]\n",
    "\n",
    "# Prepare Decoder Inputs\n",
    "decoder_inputs_train = np.zeros_like(y_train)\n",
    "decoder_inputs_train[:, 1:, :] = y_train[:, :-1, :]\n",
    "decoder_inputs_train[:, 0, ctable[\"char_to_index\"][\" \"]] = 1\n",
    "\n",
    "decoder_inputs_val = np.zeros_like(y_val)\n",
    "decoder_inputs_val[:, 1:, :] = y_val[:, :-1, :]\n",
    "decoder_inputs_val[:, 0, ctable[\"char_to_index\"][\" \"]] = 1\n",
    "\n",
    "# Model Definition with Attention\n",
    "encoder_inputs = Input(shape=(maxlen, vocab_size))\n",
    "encoder_lstm = LSTM(params[\"hidden_size\"], return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, vocab_size))\n",
    "decoder_lstm = LSTM(params[\"hidden_size\"], return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "attention_dot = Dot(axes=[2, 2])([decoder_outputs, encoder_outputs])\n",
    "attention_softmax = Activation('softmax')(attention_dot)\n",
    "context_vector = Dot(axes=[2, 1])([attention_softmax, encoder_outputs])\n",
    "decoder_combined_context = Concatenate(axis=-1)([context_vector, decoder_outputs])\n",
    "\n",
    "output_dense = TimeDistributed(Dense(vocab_size, activation=\"softmax\"))\n",
    "decoder_predictions = output_dense(decoder_combined_context)\n",
    "\n",
    "attention_model = Model([encoder_inputs, decoder_inputs], decoder_predictions)\n",
    "attention_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Training\n",
    "attention_model.fit(\n",
    "    [x_train, decoder_inputs_train],\n",
    "    y_train,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    epochs=params[\"epochs\"],\n",
    "    validation_data=([x_val, decoder_inputs_val], y_val)\n",
    ")\n",
    "\n",
    "# Evaluate Model\n",
    "for _ in range(10):\n",
    "    idx = np.random.randint(0, len(x_val))\n",
    "    q = decode(x_val[idx])\n",
    "    t = decode(y_val[idx])\n",
    "    p = decode(attention_model.predict([x_val[idx:idx+1], decoder_inputs_val[idx:idx+1]])[0])\n",
    "    print(f\"Q: {q.strip()} | True: {t.strip()} | Pred: {p.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtEJK5IZkk8j"
   },
   "source": [
    "1.4)\n",
    "\n",
    "Using any neural network architecture of your liking, build  a model with the aim to beat the best performing model in 1.1 or 1.3. Compare your results in a meaningful way, and add a short explanation to why you think/thought your suggested network is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvI6yfzs6ekL"
   },
   "source": [
    "# **Answer 1.4)**\n",
    "\n",
    "Enhanced Gradient Flow and Input Preservation:\n",
    "\n",
    "Residual connections help retain input features and improve gradient flow, especially in deeper architectures. This leads to more stable training and better convergence.\n",
    "\n",
    "Stable Learning Dynamics:\n",
    "\n",
    "Layer normalization ensures consistent and stable updates during training, reducing variance in gradient updates.\n",
    "\n",
    "Improved Attention Mechanism:\n",
    "\n",
    "Scaled dot product attention improves numerical stability and allows the model to better focus on relevant parts of the sequence, particularly beneficial for long term dependencies.\n",
    "\n",
    "Better Generalization:\n",
    "\n",
    "The learning rate scheduler dynamically adjusts the learning rate, leading to better generalization on validation data as training progresses.\n",
    "\n",
    "Efficient Encoding:\n",
    "\n",
    "The integer encoding combined with embedding and projection layers reduces memory usage and computation overhead compared to one hot encoding.\n",
    "\n",
    "\n",
    "The code in 1.4 integrates advanced techniques (residual connections, normalization, scaled attention, and learning rate scheduling) that improve the model’s ability to learn complex patterns, focus on important parts of the input sequence, and generalize better. These additions come at the cost of longer training times but lead to better overall accuracy and robustness in predictions.\n",
    "\n",
    "It is better in accuracy and generalization, especially for larger or more complex datasets. While it may not be as fast as simpler models, it’s better suited for tasks requiring high precision and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bwZKyzoBKl4G"
   },
   "outputs": [],
   "source": [
    "config = {}\n",
    "config[\"training_size\"] = 40000\n",
    "config[\"digits\"] = 4\n",
    "config[\"hidden_size\"] = 128\n",
    "config[\"batch_size\"] = 128\n",
    "config[\"iterations\"] = 50\n",
    "chars = '0123456789-+ '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6YxgNvo0W_o"
   },
   "source": [
    "SOLUTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GlF7abtLjz06",
    "outputId": "3a97dd1b-9869-4dec-dfa3-3f1d6bef8a73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 150ms/step - accuracy: 0.2622 - loss: 2.0756 - val_accuracy: 0.3479 - val_loss: 1.7206 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 146ms/step - accuracy: 0.3646 - loss: 1.6795 - val_accuracy: 0.4099 - val_loss: 1.5623 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 146ms/step - accuracy: 0.4091 - loss: 1.5541 - val_accuracy: 0.4293 - val_loss: 1.5001 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 145ms/step - accuracy: 0.4285 - loss: 1.4960 - val_accuracy: 0.4389 - val_loss: 1.4618 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 147ms/step - accuracy: 0.4435 - loss: 1.4532 - val_accuracy: 0.4515 - val_loss: 1.4208 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 147ms/step - accuracy: 0.4574 - loss: 1.4135 - val_accuracy: 0.4711 - val_loss: 1.3740 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 146ms/step - accuracy: 0.4727 - loss: 1.3686 - val_accuracy: 0.4809 - val_loss: 1.3366 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 146ms/step - accuracy: 0.4841 - loss: 1.3312 - val_accuracy: 0.4938 - val_loss: 1.3034 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 145ms/step - accuracy: 0.5005 - loss: 1.2933 - val_accuracy: 0.5053 - val_loss: 1.2644 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 147ms/step - accuracy: 0.5162 - loss: 1.2536 - val_accuracy: 0.5207 - val_loss: 1.2256 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 146ms/step - accuracy: 0.5277 - loss: 1.2199 - val_accuracy: 0.5337 - val_loss: 1.1965 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 146ms/step - accuracy: 0.5423 - loss: 1.1829 - val_accuracy: 0.5384 - val_loss: 1.1812 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 146ms/step - accuracy: 0.5555 - loss: 1.1475 - val_accuracy: 0.5559 - val_loss: 1.1375 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 148ms/step - accuracy: 0.5690 - loss: 1.1160 - val_accuracy: 0.5641 - val_loss: 1.1107 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 146ms/step - accuracy: 0.5814 - loss: 1.0854 - val_accuracy: 0.5773 - val_loss: 1.0857 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 146ms/step - accuracy: 0.5906 - loss: 1.0643 - val_accuracy: 0.5907 - val_loss: 1.0600 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 148ms/step - accuracy: 0.6013 - loss: 1.0341 - val_accuracy: 0.5948 - val_loss: 1.0467 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 147ms/step - accuracy: 0.6077 - loss: 1.0176 - val_accuracy: 0.5973 - val_loss: 1.0351 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 147ms/step - accuracy: 0.6162 - loss: 0.9988 - val_accuracy: 0.6077 - val_loss: 1.0120 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 146ms/step - accuracy: 0.6237 - loss: 0.9806 - val_accuracy: 0.6086 - val_loss: 1.0031 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 147ms/step - accuracy: 0.6320 - loss: 0.9605 - val_accuracy: 0.6179 - val_loss: 0.9896 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 146ms/step - accuracy: 0.6491 - loss: 0.9198 - val_accuracy: 0.6305 - val_loss: 0.9526 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 146ms/step - accuracy: 0.6655 - loss: 0.8860 - val_accuracy: 0.6407 - val_loss: 0.9288 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 145ms/step - accuracy: 0.6735 - loss: 0.8675 - val_accuracy: 0.6435 - val_loss: 0.9218 - learning_rate: 1.2500e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 146ms/step - accuracy: 0.6794 - loss: 0.8582 - val_accuracy: 0.6477 - val_loss: 0.9169 - learning_rate: 6.2500e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 147ms/step - accuracy: 0.6822 - loss: 0.8539 - val_accuracy: 0.6479 - val_loss: 0.9149 - learning_rate: 3.1250e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 148ms/step - accuracy: 0.6817 - loss: 0.8516 - val_accuracy: 0.6489 - val_loss: 0.9141 - learning_rate: 1.5625e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 147ms/step - accuracy: 0.6824 - loss: 0.8502 - val_accuracy: 0.6498 - val_loss: 0.9137 - learning_rate: 7.8125e-06\n",
      "Epoch 29/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 147ms/step - accuracy: 0.6833 - loss: 0.8493 - val_accuracy: 0.6492 - val_loss: 0.9135 - learning_rate: 3.9063e-06\n",
      "Epoch 30/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 146ms/step - accuracy: 0.6827 - loss: 0.8499 - val_accuracy: 0.6493 - val_loss: 0.9135 - learning_rate: 1.9531e-06\n",
      "Epoch 31/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 147ms/step - accuracy: 0.6816 - loss: 0.8500 - val_accuracy: 0.6489 - val_loss: 0.9134 - learning_rate: 9.7656e-07\n",
      "Epoch 32/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 147ms/step - accuracy: 0.6824 - loss: 0.8504 - val_accuracy: 0.6493 - val_loss: 0.9134 - learning_rate: 4.8828e-07\n",
      "Epoch 33/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 147ms/step - accuracy: 0.6831 - loss: 0.8489 - val_accuracy: 0.6493 - val_loss: 0.9134 - learning_rate: 2.4414e-07\n",
      "Epoch 34/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 148ms/step - accuracy: 0.6823 - loss: 0.8502 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 1.2207e-07\n",
      "Epoch 35/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 147ms/step - accuracy: 0.6834 - loss: 0.8483 - val_accuracy: 0.6491 - val_loss: 0.9133 - learning_rate: 6.1035e-08\n",
      "Epoch 36/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 147ms/step - accuracy: 0.6831 - loss: 0.8504 - val_accuracy: 0.6491 - val_loss: 0.9133 - learning_rate: 3.0518e-08\n",
      "Epoch 37/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 147ms/step - accuracy: 0.6818 - loss: 0.8501 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 1.5259e-08\n",
      "Epoch 38/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 147ms/step - accuracy: 0.6837 - loss: 0.8487 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 7.6294e-09\n",
      "Epoch 39/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 149ms/step - accuracy: 0.6810 - loss: 0.8501 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 3.8147e-09\n",
      "Epoch 40/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 149ms/step - accuracy: 0.6826 - loss: 0.8498 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 1.9073e-09\n",
      "Epoch 41/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 155ms/step - accuracy: 0.6824 - loss: 0.8495 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 9.5367e-10\n",
      "Epoch 42/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 148ms/step - accuracy: 0.6827 - loss: 0.8484 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 4.7684e-10\n",
      "Epoch 43/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 149ms/step - accuracy: 0.6833 - loss: 0.8491 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 2.3842e-10\n",
      "Epoch 44/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 148ms/step - accuracy: 0.6824 - loss: 0.8504 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 1.1921e-10\n",
      "Epoch 45/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 157ms/step - accuracy: 0.6824 - loss: 0.8497 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 5.9605e-11\n",
      "Epoch 46/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 148ms/step - accuracy: 0.6825 - loss: 0.8510 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 2.9802e-11\n",
      "Epoch 47/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 148ms/step - accuracy: 0.6808 - loss: 0.8492 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 1.4901e-11\n",
      "Epoch 48/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 148ms/step - accuracy: 0.6838 - loss: 0.8498 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 7.4506e-12\n",
      "Epoch 49/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 147ms/step - accuracy: 0.6827 - loss: 0.8495 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 3.7253e-12\n",
      "Epoch 50/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 148ms/step - accuracy: 0.6811 - loss: 0.8501 - val_accuracy: 0.6492 - val_loss: 0.9133 - learning_rate: 1.8626e-12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Q: 625+6573 | True: 7198 | Pred: 7180\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 1913+6609 | True: 8522 | Pred: 8518\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 625+5616 | True: 6241 | Pred: 6267\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 1949+1921 | True: 3870 | Pred: 3896\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Q: 8073-5143 | True: 2930 | Pred: 2900\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Q: 8811+6100 | True: 14911 | Pred: 14918\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Q: 9097+8056 | True: 17153 | Pred: 17141\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 2868+1374 | True: 4242 | Pred: 4249\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Q: 6211-9560 | True: -3349 | Pred: -3379\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Q: 4030+7788 | True: 11818 | Pred: 11849\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, GRU, Dense, TimeDistributed, Concatenate, Embedding, Dropout, LayerNormalization, Add, Dot, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"training_size\": 40000,\n",
    "    \"digits\": 4,\n",
    "    \"hidden_size\": 256,\n",
    "    \"embedding_size\": 50,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 50,\n",
    "    \"learning_rate\": 0.001\n",
    "}\n",
    "\n",
    "maxlen = config[\"digits\"] + 1 + config[\"digits\"]\n",
    "chars = \"0123456789+- \"\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Character Table\n",
    "ctable = {\n",
    "    \"char_to_index\": {c: i for i, c in enumerate(chars)},\n",
    "    \"index_to_char\": {i: c for i, c in enumerate(chars)},\n",
    "}\n",
    "\n",
    "# Encoding/Decoding Functions\n",
    "def encode(seq, maxlen):\n",
    "    \"\"\"Integer encode the sequence.\"\"\"\n",
    "    x = [ctable[\"char_to_index\"][c] for c in seq]\n",
    "    return np.array(x + [0] * (maxlen - len(seq)))\n",
    "\n",
    "def decode(seq):\n",
    "    \"\"\"Decode integer-encoded sequence.\"\"\"\n",
    "    return ''.join(ctable[\"index_to_char\"][idx] for idx in seq)\n",
    "\n",
    "# Generate Data\n",
    "questions = []\n",
    "answers = []\n",
    "seen = set()\n",
    "while len(questions) < config[\"training_size\"]:\n",
    "    a = random.randint(0, 10 ** config[\"digits\"] - 1)\n",
    "    b = random.randint(0, 10 ** config[\"digits\"] - 1)\n",
    "    operation = random.choice([\"+\", \"-\"])\n",
    "    query = f\"{a}{operation}{b}\"\n",
    "    if query in seen:\n",
    "        continue\n",
    "    seen.add(query)\n",
    "    result = eval(query)\n",
    "    questions.append(query.ljust(maxlen))\n",
    "    answers.append(str(result).ljust(config[\"digits\"] + 1))\n",
    "\n",
    "x = np.array([encode(q, maxlen) for q in questions])\n",
    "y = np.array([encode(a, config[\"digits\"] + 1) for a in answers])\n",
    "\n",
    "# Train / Test Split\n",
    "split_at = len(x) - len(x) // 10\n",
    "x_train, x_val = x[:split_at], x[split_at:]\n",
    "y_train, y_val = y[:split_at], y[split_at:]\n",
    "\n",
    "# Prepare Decoder Inputs\n",
    "decoder_inputs_train = np.zeros_like(y_train)\n",
    "decoder_inputs_train[:, 1:] = y_train[:, :-1]\n",
    "decoder_inputs_train[:, 0] = ctable[\"char_to_index\"][\" \"]\n",
    "\n",
    "decoder_inputs_val = np.zeros_like(y_val)\n",
    "decoder_inputs_val[:, 1:] = y_val[:, :-1]\n",
    "decoder_inputs_val[:, 0] = ctable[\"char_to_index\"][\" \"]\n",
    "\n",
    "# Model Definition\n",
    "encoder_inputs = Input(shape=(maxlen,))\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=config[\"embedding_size\"])(encoder_inputs)\n",
    "embedded_norm = LayerNormalization()(embedding)\n",
    "\n",
    "# Project embedding to match hidden size\n",
    "projected_embedding = Dense(config[\"hidden_size\"])(embedded_norm)\n",
    "\n",
    "# Encoder\n",
    "encoder_gru = GRU(config[\"hidden_size\"], return_sequences=True, return_state=True, dropout=0.3)\n",
    "encoder_outputs, state_h = encoder_gru(projected_embedding)\n",
    "\n",
    "# Residual Connection\n",
    "encoder_outputs = Add()([projected_embedding, encoder_outputs])\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(input_dim=vocab_size, output_dim=config[\"embedding_size\"])(decoder_inputs)\n",
    "decoder_gru = GRU(config[\"hidden_size\"], return_sequences=True, return_state=True, dropout=0.3)\n",
    "decoder_outputs, _ = decoder_gru(decoder_embedding, initial_state=state_h)\n",
    "\n",
    "# Scaled Dot Product Attention\n",
    "attention_scores = Dot(axes=[2, 2])([decoder_outputs, encoder_outputs])\n",
    "scale = tf.Variable(1.0, trainable=True, dtype=tf.float32, name=\"attention_scale\")\n",
    "attention_scores = attention_scores * scale\n",
    "attention_softmax = Activation('softmax')(attention_scores)\n",
    "context_vector = Dot(axes=[2, 1])([attention_softmax, encoder_outputs])\n",
    "decoder_combined_context = Concatenate(axis=-1)([context_vector, decoder_outputs])\n",
    "\n",
    "# Output Layer\n",
    "output_dense = TimeDistributed(Dense(vocab_size, activation=\"softmax\"))\n",
    "decoder_predictions = output_dense(decoder_combined_context)\n",
    "\n",
    "# Compile Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_predictions)\n",
    "optimizer = Adam(learning_rate=config[\"learning_rate\"])\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch > 20:\n",
    "        return lr * 0.5\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Training\n",
    "model.fit(\n",
    "    [x_train, decoder_inputs_train],\n",
    "    np.expand_dims(y_train, -1),\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    epochs=config[\"epochs\"],\n",
    "    validation_data=([x_val, decoder_inputs_val], np.expand_dims(y_val, -1)),\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate Model\n",
    "for _ in range(10):\n",
    "    idx = np.random.randint(0, len(x_val))\n",
    "    q = decode(x_val[idx])\n",
    "    t = decode(y_val[idx])\n",
    "    p = decode(model.predict([x_val[idx:idx+1], decoder_inputs_val[idx:idx+1]])[0].argmax(axis=-1))\n",
    "    print(f\"Q: {q.strip()} | True: {t.strip()} | Pred: {p.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voVYROYNlO49"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-d0eIM6FeaM"
   },
   "source": [
    "## Part 2: A language translation model with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80jhFbWPMW_a"
   },
   "source": [
    "In this part of the problem set we are going to implement a translation with a Sequence to Sequence Network and Attention model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgL38lJGTYaF"
   },
   "source": [
    "0) Please go over the NLP From Scratch: Translation with a Sequence to Sequence Network and Attention [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html). This attention model is very similar to what was learned in class (Luong), but a bit different. What are the main differences between  Badahnau and Luong attention mechanisms?    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2R7gH-35d_Ml"
   },
   "source": [
    "# **Answer 0)**\n",
    "\n",
    "Bahdanau Attention:\n",
    "\n",
    "Additive Attention: Combines the decoders hidden state and encoders outputs using a feed forward neural network with non-linearity (for example tanh).\n",
    "\n",
    "Each alignment score is calculated individually for each source word.\n",
    "\n",
    "It combines information from both the encoder and decoder hidden states via a concatenation operation and a learned transformation.\n",
    "The non linear transformation allows for flexible learning of complex dependencies.\n",
    "\n",
    "This makes Bahdanau attention more expressive and capable of learning nuanced alignments.\n",
    "\n",
    "Luong Attention:\n",
    "\n",
    "Multiplicative Attention: Uses dot products between the decoder hidden state and encoder outputs.\n",
    "\n",
    "Provides global and local attention options:\n",
    "\n",
    "Global focuses on the full input sequence.\n",
    "Local limits the focus to a subset of the input sequence.\n",
    "Computationally more efficient than Bahdanau.\n",
    "\n",
    "The dot product measures the similarity between the encoder and decoder hidden states.\n",
    "\n",
    "No additional parameters are learned for alignment scoring, making this mechanism simpler and computationally efficient.\n",
    "\n",
    "The attention mechanisms ability to learn is limited compared to Bahdanau attention because there is no transformation or added flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBX873GJlDl9"
   },
   "source": [
    "1.a) Using `!wget`, `!unzip` , download and extract the [hebrew-english](https://www.manythings.org/anki/) sentence pairs text file to the Colab `content/`  folder (or local folder if not using Colab).\n",
    "1.b) The `heb.txt` must be parsed and cleaned (see tutorial for requirements or change the code as you see fit).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSkZfjZWriMV"
   },
   "source": [
    "# **Answer 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZAZymk81C81",
    "outputId": "a999f58f-73b7-4adf-bb51-348fabe3e3d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hebrew: לך | English: go\n",
      "Hebrew: היי | English: hi\n",
      "Hebrew: אהלן | English: hi\n",
      "Hebrew: רוץ | English: run\n",
      "Hebrew: רוצי | English: run\n",
      "Total sentence pairs: 128133\n"
     ]
    }
   ],
   "source": [
    "# Step 1.a: Download and extract the file\n",
    "def download_and_extract_data():\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(\"heb-eng.zip\"):\n",
    "        !wget https://www.manythings.org/anki/heb-eng.zip\n",
    "\n",
    "    if not os.path.exists(\"heb.txt\"):\n",
    "        !unzip heb-eng.zip\n",
    "\n",
    "# Call the function to ensure data is downloaded and extracted\n",
    "download_and_extract_data()\n",
    "\n",
    "# Step 1.b: Parse and clean the data\n",
    "def parse_and_clean_data(file_path):\n",
    "    \"\"\"\n",
    "    Parse and clean the heb.txt file.\n",
    "    - Remove punctuation and convert text to lowercase.\n",
    "    - Split into sentence pairs.\n",
    "    - Return processed data.\n",
    "    \"\"\"\n",
    "    import string\n",
    "\n",
    "    # Read the file\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        lines = f.read().strip().split(\"\\n\")\n",
    "\n",
    "    # Split lines into sentence pairs (Hebrew-English → English-Hebrew)\n",
    "    sentence_pairs = [line.split(\"\\t\")[:2] for line in lines]\n",
    "\n",
    "    # Swap the order of Hebrew and English\n",
    "    sentence_pairs = [(pair[1], pair[0]) for pair in sentence_pairs]\n",
    "\n",
    "    # Define a function to clean text\n",
    "    def clean_text(text):\n",
    "        text = text.lower().strip()\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        return text\n",
    "\n",
    "    # Clean both English and Hebrew sentences\n",
    "    cleaned_pairs = [(clean_text(pair[0]), clean_text(pair[1])) for pair in sentence_pairs]\n",
    "\n",
    "    return cleaned_pairs\n",
    "\n",
    "# Parse and clean the data\n",
    "file_path = \"heb.txt\"\n",
    "cleaned_data = parse_and_clean_data(file_path)\n",
    "\n",
    "# Display a sample of the cleaned data\n",
    "for i in range(5):\n",
    "    print(f\"Hebrew: {cleaned_data[i][0]} | English: {cleaned_data[i][1]}\")\n",
    "\n",
    "print(f\"Total sentence pairs: {len(cleaned_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvIIlNvPlGWB"
   },
   "source": [
    "2.a) Use the tutorial example to build  and train a Hebrew to English translation model with attention (using the parameters in the code cell below). Apply the same `eng_prefixes` filter to limit the train/test data.   \n",
    "2.b) Evaluate your trained model randomly on 20 sentences.  \n",
    "2.c) Show the attention plot for 5 random sentences.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EElGfE0ixvA_"
   },
   "source": [
    "# **Answer 2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1YjsIQSHht2"
   },
   "source": [
    "# **2a + 2b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpL6otMYGdkm",
    "outputId": "88f64cec-9b05-4586-846c-7935519b5d7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 4 sentence pairs\n",
      "Trimmed to 4 sentence pairs\n",
      "Counted words:\n",
      "heb 9\n",
      "eng 11\n",
      "Epoch 1, Loss: 1.2114\n",
      "Epoch 2, Loss: 1.1712\n",
      "Epoch 3, Loss: 2.3316\n",
      "Epoch 4, Loss: 2.2807\n",
      "Epoch 5, Loss: 1.7591\n",
      "Epoch 6, Loss: 2.2340\n",
      "Epoch 7, Loss: 2.1709\n",
      "Epoch 8, Loss: 2.1460\n",
      "Epoch 9, Loss: 1.7260\n",
      "Epoch 10, Loss: 2.0495\n",
      "Epoch 11, Loss: 2.0102\n",
      "Epoch 12, Loss: 1.6696\n",
      "Epoch 13, Loss: 1.6190\n",
      "Epoch 14, Loss: 1.5779\n",
      "Epoch 15, Loss: 1.8632\n",
      "Epoch 16, Loss: 1.5627\n",
      "Epoch 17, Loss: 1.5194\n",
      "Epoch 18, Loss: 1.4684\n",
      "Epoch 19, Loss: 1.6803\n",
      "Epoch 20, Loss: 1.6206\n",
      "Epoch 21, Loss: 1.5526\n",
      "Epoch 22, Loss: 1.3337\n",
      "Epoch 23, Loss: 1.4434\n",
      "Epoch 24, Loss: 1.3909\n",
      "Epoch 25, Loss: 1.3343\n",
      "Epoch 26, Loss: 1.2393\n",
      "Epoch 27, Loss: 1.2152\n",
      "Epoch 28, Loss: 1.1730\n",
      "Epoch 29, Loss: 1.1070\n",
      "Epoch 30, Loss: 0.9580\n",
      "Epoch 31, Loss: 0.8914\n",
      "Epoch 32, Loss: 0.8458\n",
      "Epoch 33, Loss: 0.9216\n",
      "Epoch 34, Loss: 0.8605\n",
      "Epoch 35, Loss: 0.8575\n",
      "Epoch 36, Loss: 0.7833\n",
      "Epoch 37, Loss: 0.7269\n",
      "Epoch 38, Loss: 0.6883\n",
      "Epoch 39, Loss: 0.6661\n",
      "Epoch 40, Loss: 0.6270\n",
      "Epoch 41, Loss: 0.5988\n",
      "Epoch 42, Loss: 0.5478\n",
      "Epoch 43, Loss: 0.5000\n",
      "Epoch 44, Loss: 0.4756\n",
      "Epoch 45, Loss: 0.4746\n",
      "Epoch 46, Loss: 0.4351\n",
      "Epoch 47, Loss: 0.4019\n",
      "Epoch 48, Loss: 0.3908\n",
      "Epoch 49, Loss: 0.3729\n",
      "Epoch 50, Loss: 0.3802\n",
      "Input: אני שמח\n",
      "Target: i am happy\n",
      "Predicted: i am tired <EOS>\n",
      "Input: אני עייף\n",
      "Target: i am tired\n",
      "Predicted: i am tired <EOS>\n",
      "Input: הוא גבוה\n",
      "Target: he is tall\n",
      "Predicted: he is tall <EOS>\n",
      "Input: היא נמוכה\n",
      "Target: she is short\n",
      "Predicted: she is short <EOS>\n",
      "Word-Level Accuracy: 0.9167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Parameters\n",
    "MAX_LENGTH = 10\n",
    "hidden_size = 128\n",
    "epochs = 50\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "# Helper Functions\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return f'{m}m {s:.0f}s'\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    elapsed = now - since\n",
    "    estimated_total = elapsed / percent\n",
    "    remaining = estimated_total - elapsed\n",
    "    return f'{asMinutes(elapsed)} (- {asMinutes(remaining)})'\n",
    "\n",
    "# Language Class\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "# Preprocess Data\n",
    "def prepareData(lang1, lang2, pairs):\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    pairs = [pair for pair in pairs if len(pair[0].split()) < MAX_LENGTH and len(pair[1].split()) < MAX_LENGTH]\n",
    "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# Encoder Model\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "# Decoder with Attention\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attn = nn.Linear(hidden_size * 2, max_length)\n",
    "        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = torch.nn.functional.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1\n",
    "        )\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = torch.nn.functional.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "# Training\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]])\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]])\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        for di in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n",
    "\n",
    "# Helper Functions\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "# Word-Level Accuracy\n",
    "def compute_word_accuracy(encoder, decoder, pairs, input_lang, output_lang):\n",
    "    total_words = 0\n",
    "    correct_words = 0\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_sentence, target_sentence = pair\n",
    "        predicted_words = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "        predicted_words = [word for word in predicted_words if word != '<EOS>']\n",
    "        target_words = target_sentence.split()\n",
    "\n",
    "        for pred_word, ref_word in zip(predicted_words, target_words):\n",
    "            if pred_word == ref_word:\n",
    "                correct_words += 1\n",
    "            total_words += 1\n",
    "\n",
    "    accuracy = correct_words / total_words if total_words > 0 else 0\n",
    "    print(f\"Word-Level Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "# Prepare Data\n",
    "pairs = [\n",
    "    [\"אני שמח\", \"i am happy\"],\n",
    "    [\"אני עייף\", \"i am tired\"],\n",
    "    [\"הוא גבוה\", \"he is tall\"],\n",
    "    [\"היא נמוכה\", \"she is short\"]\n",
    "]\n",
    "input_lang, output_lang, pairs = prepareData('heb', 'eng', pairs)\n",
    "\n",
    "# Initialize Models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words)\n",
    "\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.01)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for pair in pairs:\n",
    "        input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "        target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "for pair in pairs:\n",
    "    print(f\"Input: {pair[0]}\")\n",
    "    print(f\"Target: {pair[1]}\")\n",
    "    output_words = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "    print(f\"Predicted: {' '.join(output_words)}\")\n",
    "\n",
    "# Compute Accuracy\n",
    "compute_word_accuracy(encoder, decoder, pairs, input_lang, output_lang)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt4kwsu4Hnys"
   },
   "source": [
    "# **2c**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NYPXzME3EEtr",
    "outputId": "e490582d-7ef7-4045-e048-fb4a0d83ed17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Input Sentence: הוא גבוה\n",
      "Output Sentence: he is tall <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2:\n",
      "Input Sentence: אני עייף\n",
      "Output Sentence: i am tired <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3:\n",
      "Input Sentence: הוא גבוה\n",
      "Output Sentence: he is tall <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 4:\n",
      "Input Sentence: הוא גבוה\n",
      "Output Sentence: he is tall <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 5:\n",
      "Input Sentence: אני עייף\n",
      "Output Sentence: i am tired <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwgAAANYCAYAAABgi8rgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAARDlJREFUeJzt3X2YVXW9N/7PzOjMoDgjhjKAKCI+UTwYKFGRnCTBUx4x7QLzFuSknVtvT3nI2/JkoJf9Dj5laEelNEWtlDp3eexBrMbQY6IUHm+fSPHpBh+Gp4KRUUFnr98fyLcmwcDNsNdiXq+51nXYa6+99md2eJ394fNe31WVZVkWAAAAEVFd6QIAAID80CAAAACJBgEAAEg0CAAAQKJBAAAAEg0CAACQaBAAAIBEgwAAACQaBAAAINEgAAAAiQYBAABINAgAAECiQQAAABINAgBQcW1tbfHYY49t9rknnngi1q1bt4Mrgq5LgwAAVNybb74ZI0eOjIULF3bY/+STT8bhhx+uQYAdSIMAAFTcnnvuGZ/61Kfilltu6bD/1ltvjaOPPjqampoqVBl0PRoEACAXpkyZEnPnzo233norIiKyLIvvf//7MXXq1ApXBl2LBgEAyIXx48fHLrvsEj//+c8jImL+/Pmxbt26mDBhQmULgy5GgwAA5EJNTU2ccsopKWZ06623xsSJE6O2trbClUHXUpVlWVbpIgAAIiIee+yxOPLII+OZZ56JQYMGxd133x0f+tCHKl0WdCkaBAAgV4YPHx577LFHtLS0xB/+8IdKlwNdjogRAJArkydPjvvuuy8mT55c6VKgS9ql0gUAAPylU089NdasWRP/+I//WOlSoEsSMQIAABIRIwAg17IsixUrVlS6DOgyNAgAQEXttttusXLlyvT4k5/8ZLzyyivp8YoVK6J3796VKA26JA0CAFBRb7zxRvxl4vm+++6L119/vcMxEtGw42gQAIDcq6qqqnQJ0GVoEAAAgESDAABUVFVVVYcJwV8/BnYsy5wCABVVXV0djY2NqSlYs2ZNNDQ0RHX1xn/HzLIsWltbo729vZJlQpfhRmkAQEXddNNNlS4B+AsmCAAAQGKCAADkwuuvvx6/+tWv4umnn46IiEMOOSTGjh0b3bp1q3Bl0LVoEACAirvzzjvj9NNPj1WrVnXY37Nnz/jud78bxx13XIUqg67HKkYAQEU98MADcdJJJ8XHPvax+O1vfxt//OMf449//GPcf//9MXr06DjppJPiwQcfrHSZ0GW4BgEAqKi///u/j379+sW3v/3tzT7/T//0T7Fs2bL4xS9+sYMrg65JgwAAVNRee+0V9957bwwePHizzz/66KNx1FFHxZ/+9KcdXBl0TSJGAEBFvf7669HQ0LDF5xsbG+ONN97YgRVB16ZBAAAq6qCDDop77rlni883NzfHQQcdtAMrgq5NgwAAVNTUqVPj3HPP3ew1Bj//+c/jvPPOi9NOO23HFwZdlGsQAHLinnvuiaqqqqirq4t99tknBg4cWOmSYIcolUoxceLE+D//5//EIYccEocddlhkWRaLFy+OJUuWxIQJE+JHP/pRVFf7d03YETQIADnx119+9tprrzjrrLPiwgsv9MWILmHu3Llx2223pRulHXzwwTFp0qSYNGlShSuDrkWDAJAzb775ZqxevToWLlwYl112WfTp0yd++MMfVrosALoIDQJAjr355pvxoQ99KL70pS/FZz/72UqXA53irbfeivb29qirq0v7li9fHrNnz462trY47rjjYvTo0RWsELoWDQJATrW1tUVzc3N861vfipUrV8YjjzxS6ZKgU0ydOjVqa2vTjdJeffXVeP/73x9vvPFG9O7dO5588sn4z//8z/j7v//7ClcKXYNQK7kxffr0uPjii+P666+PJUuWdHju0UcfrVBVsGM9+uijcdlll8XHP/7xeN/73hennnpqdO/ePVpaWmLhwoWVLg86xW9/+9s48cQT0+Nbbrkl2tvbY8mSJfF//+//jWnTpsXll19ewQqha9EgkBv/9V//Fffcc09ce+21MXjw4LjqqqvirbfeihkzZsSRRx5Z6fKg0/Xt2zcOP/zwuOWWW2L48OFx1113xapVq+InP/lJnHLKKXHLLbdUukToFC+99FKH+xw0NzfHiSeeGI2NjRERMWXKlHjiiScqVR50ObtUugDY5De/+U3685IlS+Lv/u7v4rvf/W6sWrUq5s6dW8HKYMeYMWNGHHvssdGvX793PHfmmWfGXXfdVYGqoPPV19fH66+/nh4/+OCDHSYG9fX1sW7dukqUBl2SCQK5s2HDhrjppptixYoV8cEPfjCefPLJOP744ytdFnS6008/Pfr27RulUilKpVKH5wYOHBj//M//XKHKoHMNGzYsbr311ojYOE1evnx5fPzjH0/PP/vss9GnT59KlQddjgaBXHnggQdiyJAh8Z3vfCd++tOfxpw5c2LPPfesdFmwQ+yyyy6x6667pu3rX/96pUuCHWL69Olx1VVXxYEHHhjjxo2L0047LXr37p2e/8lPfhIf+chHKlghdC0iRuTGF77whZg9e3acfvrpceedd8aLL74Ya9asiYaGhoh4502kYGez6U7Km/To0aOC1cCOc9RRR8WiRYvil7/8ZTQ1NcVnPvOZDs8PGzbMtWiwA1nmlNwYOHBg3HDDDTFmzJi4//7749RTT42lS5em59vb2ytYHXS+v44VaYrpqkqlUjz++OMxaNCg2GUX/5YJO5r/70NuPPbYYzFmzJiIiPjoRz8azz33XCxevDjuu+++uOeeeypbHOwAIkaw0Z133hmHH364BSqgQkwQAHJi/vz5HSJGe+21VwwePLiCFUFlnHDCCbFgwYIYPHhw/OpXv6p0OdDlaBDIlT/+8Y+xZMmSaGtre8dzf7miBeysfve738Wdd94Zy5cvj7feemuLx9144407sCrYcVatWhX77rtv3HHHHfEP//AP8dxzz8W+++5b6bKgSxHsIzduuummOPPMM2PDhg3veK66uvpdvyzBzmDOnDlx+umnx8iRI2O//faTvaZLuu222+IDH/hAjB8/PkaPHh233nprnH/++ZUuC7oUEwRy48ADD4wLL7wwJk2aFLvuumuH53bdddd48803K1QZ7BiDBg2K6dOnx6RJkypdClTM8OHDY8qUKfGFL3whbrrpprjsssti8eLFlS4LuhQNArlRV1cX69ev3+xzGgS6gm7dusXatWujtrY22tvb47//+79jxIgRlS4LdpjHH388hg8fHi+99FL07Nkz1q1bF7169Yp77rknRo4cWenyoMuwihG58etf/zqee+65tL3++uvpuauvvrqClcGO0d7eHrW1tRGxcZnHo48+usIVwY518803xzHHHBM9e/aMiIju3bvHhAkTYs6cOZUtDLoYEwRyo7q6OqqqqiLLsqiqqopzzz03Lr300kqXBTtMTU1NXHDBBZFlWTz++OOxZMmSeOyxxypdFuwQ7e3tse+++8bVV1/d4UZpd911V5xyyinR0tKSGmigc7kCjtx4/vnnOzyur6+vUCVQGaNHj4777rsvampqYt9997UGPF3KihUr4swzz4zjjz++w/5x48bFtGnToqWlJfbbb78KVQddiwkCAACQuAYBAABINAgAAECiQSC31q9fHxdeeOEWlz6FnZm//3R1/huAynENArnV2toajY2NsXbt2mhoaKh0ObBD+ftPV+e/AagcEwQAACDRIAAAAIn7IGxBqVSKl19+OfbYY4+oqqqqdDldUmtra4f/C12Jv/90df4bqKwsy+LVV1+NPn36RHV1/v49+Y033ogNGzZUuox3qK2t3Snu4+QahC148cUXo1+/fpUuAwCgYpYtWxb77rtvpcvo4I033ogDDjggWlpaKl3KOzQ1NcXzzz9f+CbBBGEL9thjj4iIqK3dzQSBLmn9+rZKlwAVNae5udIlQMW83tYWZ/7DP6TvQ3myYcOGaGlpiWXLluXqAvbW1tbo169fbNiwQYOws9rUFFRVVWkQALqg3XbfvdIlQMXl+TtQQ0NDrhqEnYkGAQCAwsmyLPKUlM9TLeXK31UnAABAxWgQAACARMQIAIDCKWVZlHIU68lTLeUyQQAAABINAgAAVMg111wT/fv3j/r6+hg5cmQsXLhwi8f++Mc/jhEjRsSee+4Zu+++ewwbNixuvfXWDsecdtppaRXOTdv48eO3qSYRIwAACmdnWMVo7ty5MW3atJg9e3aMHDkyZs2aFePGjYunnnoq9tlnn3ccv9dee8VXv/rVOPTQQ6O2tjZ+9rOfxdSpU2OfffaJcePGpePGjx8fN910U3pcV1e3TXWZIAAAQAVceeWVccYZZ8TUqVNj0KBBMXv27Nhtt93ixhtv3OzxY8aMiRNOOCEOO+ywOPDAA+OLX/xiDBkyJO6///4Ox9XV1UVTU1PaevTosU11aRAAAGAH27BhQyxatCjGjh2b9lVXV8fYsWNjwYIFf/P1WZZFc3NzPPXUU/Gxj32sw3Pz58+PffbZJw455JA488wzY/Xq1dtUm4gRAACFk739kxebamltbe2wv66ubrMRn1WrVkV7e3v06tWrw/5evXrFH/7why2+z9q1a6Nv376xfv36qKmpiWuvvTY+8YlPpOfHjx8fn/70p+OAAw6IZ599Nv71X/81jj322FiwYEHU1NRs1e+iQQAAgO2kX79+HR7PmDEjLrzwwu12/j322CMeeeSRWLduXTQ3N8e0adNiwIABMWbMmIiImDRpUjp28ODBMWTIkDjwwANj/vz5cfTRR2/Ve2gQAABgO1m2bFk0NDSkx1u6QLhnz55RU1MTy5cv77B/+fLl0dTUtMXzV1dXx8CBAyMiYtiwYbF48eKYOXNmahD+2oABA6Jnz57xzDPPbHWD4BoEAAAKp5Tlb4uIaGho6LBtqUGora2N4cOHR3Nz859/p1IpmpubY9SoUVv/OZRKsX79+i0+/+KLL8bq1aujd+/eW31OEwQAAKiAadOmxZQpU2LEiBFx5JFHxqxZs6KtrS2mTp0aERGTJ0+Ovn37xsyZMyMiYubMmTFixIg48MADY/369fGLX/wibr311rjuuusiImLdunVx0UUXxYknnhhNTU3x7LPPxnnnnRcDBw7ssAzq36JBAACACpg4cWKsXLkypk+fHi0tLTFs2LCYN29eunB56dKlUV3958BPW1tbnHXWWfHiiy9Gt27d4tBDD43vfe97MXHixIiIqKmpiUcffTRuvvnmWLNmTfTp0yeOOeaYuPjii7fpXghVWZ7uMJEjra2t0djYGHV1u0dVVVWly4Ed7o031lW6BKioHz74YKVLgIp5ra0tTjv66Fi7dm2HPH0ebPqOtnzVqlzV1traGr169szlZ7atXIMAAAAkGgQAACBxDQIAAIVTyrIo5Sgpn6daymWCAAAAJBoEAAAgETECAKBwsiyLPC3GmadaymWCAAAAJBoEAAAgETECAKBwRIw6jwkCAACQaBAAAIBExAgAgMJxo7TOY4IAAAAkGgQAACARMQIAoHCsYtR5TBAAAIBEgwAAACQiRgAAFE729k9e5KmWcpkgAAAAiQYBAABIRIwAACicUrZxy4s81VIuEwQAACDRIAAAAImIEQAAxZOzG6VFnmopkwkCAACQaBAAAIBExAgAgMIpZVmUchTryVMt5TJBAAAAEg0CAACQiBgBAFA4Wc5WMcpTLeUyQQAAABINAgAAkIgYAQBQOCJGnccEAQAASDQIAABAImIEAEDhuFFa5zFBAAAAEg0CAACQiBgBAFA4VjHqPCYIAABAokEAAAASESMAAAone/snL/JUS7lMEAAAgESDAAAAJCJGAAAUTinbuOVFnmoplwkCAACQaBAAAIBExAgAgMLJIl83J8tPJeUzQQAAABINAgAAkIgYAQBQOFmW5StilKNaymWCAAAAJBoEAAAgETECAKBwSlkWpRzFevJUS7lMEAAAgESDAAAAJCJGAAAUjlWMOo8JAgAAkGgQAACARMQIAIDCsYpR5zFBAAAAEg0CAACQiBgBAFA8OVvFKPJUS5lMEAAAgESDAAAAJCJGAAAUTvb2T17kqZZymSAAAACJBgEAAEhEjAAAKJxStnHLizzVUi4TBAAAINEgAAAAiYgRAACFk+XsRml5qqVcXWqCMGbMmDjnnHMqXQYAAORWl5og/PjHP45dd9210mUAAEBudakGYa+99qp0CQAAbAciRp1HxAgAAEi61ATh3axfvz7Wr1+fHre2tlawGgAAqIwuNUF4NzNnzozGxsa09evXr9IlAQCwBaUsy922s9AgvO3888+PtWvXpm3ZsmWVLgkAAHY4EaO31dXVRV1dXaXLAACAitIgAABQOFYx6jwiRgAAQKJBAAAAki4VMZo/f36lSwAAYDsQMeo8JggAAECiQQAAAJIuFTECAGDnkLebk+WplnKZIAAAAIkGAQAASESMAAAonOztn7zIUy3lMkEAAAASDQIAAJCIGAEAUDilbOOWF3mqpVwmCAAAQKJBAAAAEhEjAAAKJ8uyyHJ0c7I81VIuEwQAACDRIAAAAImIEQAAhSNi1HlMEAAAgESDAAAAJCJGAAAUTpZlUcpRrEfECAAA2ClpEAAAgETECACAwrGKUecxQQAAABINAgAAkIgYAQBQOFnkK9aTn0rKZ4IAAAAkGgQAACARMQIAoHBKObtRWp5qKZcJAgAAkGgQAACARMQIAIDCyd7+yYs81VIuEwQAAKiQa665Jvr37x/19fUxcuTIWLhw4RaP/fGPfxwjRoyIPffcM3bfffcYNmxY3HrrrR2OybIspk+fHr17945u3brF2LFjY8mSJdtUkwYBAAAqYO7cuTFt2rSYMWNGPPzwwzF06NAYN25crFixYrPH77XXXvHVr341FixYEI8++mhMnTo1pk6dGnfffXc65rLLLourr746Zs+eHQ899FDsvvvuMW7cuHjjjTe2ui4NAgAAhVPK8rdtqyuvvDLOOOOMmDp1agwaNChmz54du+22W9x4442bPX7MmDFxwgknxGGHHRYHHnhgfPGLX4whQ4bE/fffHxEbpwezZs2KCy64II4//vgYMmRI3HLLLfHyyy/HHXfcsdV1aRAAAGA7aW1t7bCtX79+s8dt2LAhFi1aFGPHjk37qqurY+zYsbFgwYK/+T5ZlkVzc3M89dRT8bGPfSwiIp5//vloaWnpcM7GxsYYOXLkVp0z1bHVRwIAAO+qX79+0djYmLaZM2du9rhVq1ZFe3t79OrVq8P+Xr16RUtLyxbPv3bt2ujevXvU1tbGJz/5yfjWt74Vn/jEJyIi0uu29Zx/zSpGAAAUTpZlkeXo5mSbalm2bFk0NDSk/XV1ddv1ffbYY4945JFHYt26ddHc3BzTpk2LAQMGxJgxY7bbe2gQAABgO2loaOjQIGxJz549o6amJpYvX95h//Lly6OpqWmLr6uuro6BAwdGRMSwYcNi8eLFMXPmzBgzZkx63fLly6N3794dzjls2LCt/h1EjAAAYAerra2N4cOHR3Nzc9pXKpWiubk5Ro0atdXnKZVK6TqHAw44IJqamjqcs7W1NR566KFtOqcJAgAAVMC0adNiypQpMWLEiDjyyCNj1qxZ0dbWFlOnTo2IiMmTJ0ffvn3TdQwzZ86MESNGxIEHHhjr16+PX/ziF3HrrbfGddddFxERVVVVcc4558TXv/71OOigg+KAAw6Ir33ta9GnT5+YMGHCVtelQQAAoHDyeg3Ctpg4cWKsXLkypk+fHi0tLTFs2LCYN29eush46dKlUV3958BPW1tbnHXWWfHiiy9Gt27d4tBDD43vfe97MXHixHTMeeedF21tbfH5z38+1qxZEx/96Edj3rx5UV9fv9V1VWV5+mRzpLW1NRobG6OubveoqqqqdDmww73xxrpKlwAV9cMHH6x0CVAxr7W1xWlHHx1r167dqjz9jrTpO9rPFi6M3bt3r3Q5Sdu6dfGpI4/M5We2rVyDAAAAJCJGAAAUTinLopSjIEyeaimXCQIAAJBoEAAAgETECACAwtkZVjHKKxMEAAAg0SAAAACJiBEAAIUjYtR5TBAAAIBEgwAAACQiRgAAFI4bpXUeEwQAACDRIAAAAImIEQAAhZO9/ZMXeaqlXCYIAABAokEAAAASESMAAAonyzZueZGnWsplggAAACQaBAAAIBExAgCgcLKc3Sgty1Et5TJBAAAAEg0CAACQiBgBAFA4WZblKtaTp1rKZYIAAAAkGgQAACARMQIAoHBKOVvFKE+1lMsEAQAASDQIAABAImIEAEDhWMWo85ggAAAAiQYBAABIRIwAACgcEaPOY4IAAAAkGgQAACARMQIAoHDcKK3zmCAAAACJBgEAAEhEjAAAKJzs7Z+8yFMt5TJBAAAAEg0CAACQiBgBAFA4WbZxy4s81VIuEwQAACDRIAAAAImIEQAAheNGaZ3HBAEAAEg0CAAAQCJiBABA4WQRkeUo1pOfSspnggAAACQmCH9Dr177R3V1TaXLgB1u330PqXQJUFE/v+GuSpcAFbNhw/pKl0AFaRAAACgcqxh1HhEjAAAg0SAAAACJiBEAAIWTZVm+VjHKUS3lMkEAAAASDQIAAJCIGAEAUDgiRp3HBAEAAEg0CAAAQCJiBABA8WTZxi0v8lRLmUwQAACARIMAAAAkIkYAABROVsoiK+Un1pOnWsplggAAACQaBAAAIBExAgCgeHK2iFHkqZYymSAAAACJBgEAAEhEjAAAKJwsyyLLUcYoT7WUywQBAABINAgAAEAiYgQAQOGIGHUeEwQAACDRIAAAAImIEQAAhSNi1HlMEAAAgESDAAAAJCJGAAAUTlbKIivlJ9aTp1rKZYIAAAAkGgQAACARMQIAoHCsYtR5TBAAAIBEgwAAACQiRgAAFI6IUecxQQAAABINAgAAkIgYAQBQPFm2ccuLPNVSJhMEAAAg0SAAAACJiBEAAIUjYdR5TBAAAIBEgwAAACQiRgAAFE6WZZGV8pPrcaM0AABgp6RBAAAAEhEjAAAKJ8uyXMV68lRLuUwQAACARIMAAAAkIkYAABSOiFHnMUEAAAASDQIAAJCIGAEAUDgiRp3HBAEAAEg0CAAAQCJiBABA4YgYdR4TBAAAINEgAAAAiYgRAADFU4qIUo5iPaVKF7D9mCAAAACJBgEAAEhEjAAAKByrGHUeEwQAACDRIAAAAImIEQAAhZNlG7e8yFMt5TJBAAAAEg0CAABUyDXXXBP9+/eP+vr6GDlyZCxcuHCLx15//fUxevTo6NGjR/To0SPGjh37juNPO+20qKqq6rCNHz9+m2rSIAAAUDibVjHK07at5s6dG9OmTYsZM2bEww8/HEOHDo1x48bFihUrNnv8/Pnz4+STT47f/OY3sWDBgujXr18cc8wx8dJLL3U4bvz48fHKK6+k7bbbbtumujQIAABQAVdeeWWcccYZMXXq1Bg0aFDMnj07dtttt7jxxhs3e/z3v//9OOuss2LYsGFx6KGHxg033BClUimam5s7HFdXVxdNTU1p69GjxzbVpUEAAIDtpLW1tcO2fv36zR63YcOGWLRoUYwdOzbtq66ujrFjx8aCBQu26r1ee+21ePPNN2OvvfbqsH/+/Pmxzz77xCGHHBJnnnlmrF69ept+Bw0CAACFU+k40ZYiRv369YvGxsa0zZw5c7P1r1q1Ktrb26NXr14d9vfq1StaWlq26jP48pe/HH369OnQZIwfPz5uueWWaG5ujksvvTTuvffeOPbYY6O9vX2rP1vLnAIAwHaybNmyaGhoSI/r6uo65X0uueSSuP3222P+/PlRX1+f9k+aNCn9efDgwTFkyJA48MADY/78+XH00Udv1blNEAAAYDtpaGjosG2pQejZs2fU1NTE8uXLO+xfvnx5NDU1vet7XHHFFXHJJZfEL3/5yxgyZMi7HjtgwIDo2bNnPPPMM1v9O2gQAAAonKyU5W7bFrW1tTF8+PAOFxhvuuB41KhRW3zdZZddFhdffHHMmzcvRowY8Tff58UXX4zVq1dH7969t7o2DQIAAFTAtGnT4vrrr4+bb745Fi9eHGeeeWa0tbXF1KlTIyJi8uTJcf7556fjL7300vja174WN954Y/Tv3z9aWlqipaUl1q1bFxER69ati//9v/93PPjgg/HCCy9Ec3NzHH/88TFw4MAYN27cVtflGgQAAKiAiRMnxsqVK2P69OnR0tISw4YNi3nz5qULl5cuXRrV1X/+9/zrrrsuNmzYECeddFKH88yYMSMuvPDCqKmpiUcffTRuvvnmWLNmTfTp0yeOOeaYuPjii7fpWggNAgAAxfMeb07Wad5jLWeffXacffbZm31u/vz5HR6/8MIL73qubt26xd133/2e6vhLIkYAAECiQQAAABIRIwAACifLWcQoT7WUywQBAABINAgAAEAiYgQAQOGIGHUeEwQAACDRIAAAAImIEQAAxZNl7/nmZJ0iT7WUyQQBAABINAgAAEAiYgQAQOFkpY1bXuSplnKZIAAAAIkGAQAASESMAAAonCxydqO0yE8t5TJBAAAAEg0CAACQiBgBAFA4WZaziFGOailXISYI8+bNi49+9KOx5557xvve97741Kc+Fc8++2xERLzwwgtRVVUVP/zhD2P06NHRrVu3OOKII+Lpp5+O3/3udzFixIjo3r17HHvssbFy5coK/yYAAJBvhWgQ2traYtq0afH73/8+mpubo7q6Ok444YQolf684OyMGTPiggsuiIcffjh22WWX+OxnPxvnnXdeXHXVVfFf//Vf8cwzz8T06dO3+B7r16+P1tbWDhsAAHQ1hYgYnXjiiR0e33jjjbH33nvHk08+Gd27d4+IiHPPPTfGjRsXERFf/OIX4+STT47m5ub4yEc+EhERn/vc52LOnDlbfI+ZM2fGRRdd1Dm/AAAA25WIUecpxARhyZIlcfLJJ8eAAQOioaEh+vfvHxERS5cuTccMGTIk/blXr14RETF48OAO+1asWLHF9zj//PNj7dq1aVu2bNl2/i0AACD/CjFBOO6442L//feP66+/Pvr06ROlUik+8IEPxIYNG9Ixu+66a/pzVVXVZvf9ZSTpr9XV1UVdXV0nVA8AAMWR+wZh9erV8dRTT8X1118fo0ePjoiI+++/v8JVAQBQSSJGnSf3DUKPHj3ife97X3znO9+J3r17x9KlS+MrX/lKpcsCAICdUu6vQaiuro7bb789Fi1aFB/4wAfiX/7lX+Lyyy+vdFkAALBTyv0EISJi7Nix8eSTT3bY95djnL8e6YwZM+Yd+0477bQ47bTTOq1GAAB2nKyURVbKT6wnT7WUK/cTBAAAYMfRIAAAAEkhIkYAANBBlm3c8iJPtZTJBAEAAEg0CAAAQCJiBABA4bhRWucxQQAAABINAgAAkIgYAQBQOBYx6jwmCAAAQKJBAAAAEhEjAAAKxypGnccEAQAASDQIAABAImIEAEDhZKUsslJ+Yj15qqVcJggAAECiQQAAABIRIwAACscqRp3HBAEAAEg0CAAAQCJiBABA4WRZvmI9OSqlbCYIAABAokEAAAASESMAAArHKkadxwQBAABINAgAAEAiYgQAQOGIGHUeEwQAACDRIAAAAImIEQAAxVPKNm55kadaymSCAAAAJBoEAAAgETECAKBwsojI08JBOSqlbCYIAABAokEAAAASESMAAIonZzdKy1XeqUwmCAAAQKJBAAAAEhEjAAAKJ8tZxChPtZTLBAEAAEg0CAAAQCJiBABA4WSlLLJSfmI9eaqlXCYIAABAokEAAAASESMAAArHKkadxwQBAABINAgAAEAiYgQAQOGIGHUeEwQAACDRIAAAAImIEQAAxZNlG7e8yFMtZTJBAAAAEg0CAACQiBgBAFA4VjHqPCYIAABAokEAAAASESMAAAonK23c8iJPtZTLBAEAAEg0CAAAQCJiBABA4VjFqPOYIAAAAIkGAQAASESMAAAoHBGjzmOCAAAAJBoEAAAgETECAKBwRIw6jwkCAACQaBAAAIBExAgAgMIRMeo8JggAAECiQQAAABIRIwAACicrZZGV8hPryVMt5TJBAAAAEg0CAACQiBgBAFA4VjHqPCYIAABAokEAAAASESMAAAooi8hVrCdPtZTHBAEAAEg0CAAAQCJiBABA4WQ5SxjlqZZymSAAAACJBgEAAEhEjAAAKJyNEaP85HpyVErZTBAAAIBEgwAAACQaBAAACicrZbnb3otrrrkm+vfvH/X19TFy5MhYuHDhFo+9/vrrY/To0dGjR4/o0aNHjB079h3HZ1kW06dPj969e0e3bt1i7NixsWTJkm2qSYMAAAAVMHfu3Jg2bVrMmDEjHn744Rg6dGiMGzcuVqxYsdnj58+fHyeffHL85je/iQULFkS/fv3imGOOiZdeeikdc9lll8XVV18ds2fPjoceeih23333GDduXLzxxhtbXZcGAQAAKuDKK6+MM844I6ZOnRqDBg2K2bNnx2677RY33njjZo///ve/H2eddVYMGzYsDj300LjhhhuiVCpFc3NzRGycHsyaNSsuuOCCOP7442PIkCFxyy23xMsvvxx33HHHVtdlFaO/4c0334jq6ppKlwE7XI89e1W6BKio+39zZ6VLgIopldorXcLflGVZzlYx2lhLa2trh/11dXVRV1f3juM3bNgQixYtivPPPz/tq66ujrFjx8aCBQu26j1fe+21ePPNN2OvvfaKiIjnn38+WlpaYuzYsemYxsbGGDlyZCxYsCAmTZq0Vec1QQAAgO2kX79+0djYmLaZM2du9rhVq1ZFe3t79OrV8R/kevXqFS0tLVv1Xl/+8pejT58+qSHY9LpyzhlhggAAANvNsmXLoqGhIT3e3PRge7jkkkvi9ttvj/nz50d9ff12PbcGAQCAwslrxKihoaFDg7AlPXv2jJqamli+fHmH/cuXL4+mpqZ3fe0VV1wRl1xySfz617+OIUOGpP2bXrd8+fLo3bt3h3MOGzZsa38VESMAANjRamtrY/jw4ekC44hIFxyPGjVqi6+77LLL4uKLL4558+bFiBEjOjx3wAEHRFNTU4dztra2xkMPPfSu5/xrJggAAFAB06ZNiylTpsSIESPiyCOPjFmzZkVbW1tMnTo1IiImT54cffv2TdcxXHrppTF9+vT4wQ9+EP3790/XFXTv3j26d+8eVVVVcc4558TXv/71OOigg+KAAw6Ir33ta9GnT5+YMGHCVtelQQAAoHhyFjGK91DLxIkTY+XKlTF9+vRoaWmJYcOGxbx589JFxkuXLo3q6j8Hfq677rrYsGFDnHTSSR3OM2PGjLjwwgsjIuK8886Ltra2+PznPx9r1qyJj370ozFv3rxtuk6hKsvVJ5sfra2t0djYGL17D7DMKV3SBw8/ptIlQEU9ufiBSpcAFVMqtcfzzz8aa9eu3ao8/Y606TvaP/3L/xe1ddv34txybFj/Rnz7m1/N5We2rVyDAAAAJCJGAAAUT5a9p1hPp8lTLWUyQQAAABINAgAAkIgYAQBQOFkpi6yUn1hPnmoplwkCAACQaBAAAIBExAgAgMKxiFHnMUEAAAASDQIAAJCIGAEAUDhZlkWWo1xPnmoplwkCAACQaBAAAIBExAgAgMIRMeo8JggAAECiQQAAABIRIwAACkfEqPOYIAAAAIkGAQAASESMAAAonKyURVbKT6wnT7WUywQBAABINAgAAEAiYgQAQOFYxajzmCAAAACJBgEAAEhEjAAAKKAsIlexnjzVUh4TBAAAINEgAAAAiYgRAACFYxWjzmOCAAAAJBoEAAAgETECAKBwspwtYpSnWsplggAAACQaBAAAIBExAgCgcLJSFlkpP7mePNVSLhMEAAAg0SAAAACJiBEAAIXjRmmdxwQBAABINAgAAEAiYgQAQOGIGHUeEwQAACDRIAAAAImIEQAAhSNi1HlMEAAAgESDAAAAJCJGAAAUTpblK9aTo1LKZoIAAAAkGgQAACARMQIAoHCyUhZZKT+5njzVUi4TBAAAINEgAAAAiYgRAADFs3EZo0pX8Wd5qqVMJggAAECiQQAAABIRIwAACkfCqPOYIAAAAIkGAQAASESMAAAonCzLIstRridPtZTLBAEAAEg0CAAAQCJiBABA8eQsYrQzLWNkggAAACQaBAAAIBExAgCgcLJSFlkpP7GePNVSLhMEAAAg0SAAAACJiBEAAIXjRmmdxwQBAABINAgAAEAiYgQAQOFkkbOIUeSnlnKZIAAAAIkGAQAASESMAAAoHKsYdR4TBAAAINEgAAAAiYgRAADFk2Ubt7zIUy1lMkEAAAASDQIAAJCIGAEAUDhZaeOWF3mqpVwmCAAAQKJBAAAAEhEjAAAKx43SOk+uJgjz58+PqqqqWLNmzXY/95w5c2LPPffc7ucFAICdSUUbhDFjxsQ555yTHn/4wx+OV155JRobGytXFAAAdGG5ihjV1tZGU1PTFp9vb2+PqqqqqK7O1eADAIAdTMSo81Tsm/Zpp50W9957b1x11VVRVVUVVVVVMWfOnA4Ro02xoDvvvDMGDRoUdXV1sXTp0li/fn2ce+650bdv39h9991j5MiRMX/+/A7nnzNnTuy3336x2267xQknnBCrV6/e8b8kAAAUTMUahKuuuipGjRoVZ5xxRrzyyivxyiuvRL9+/d5x3GuvvRaXXnpp3HDDDfHEE0/EPvvsE2effXYsWLAgbr/99nj00UfjM5/5TIwfPz6WLFkSEREPPfRQfO5zn4uzzz47Hnnkkfi7v/u7+PrXv/6u9axfvz5aW1s7bAAA0NVULGLU2NgYtbW1sdtuu6VY0R/+8Id3HPfmm2/GtddeG0OHDo2IiKVLl8ZNN90US5cujT59+kRExLnnnhvz5s2Lm266Kf7t3/4trrrqqhg/fnycd955ERFx8MEHxwMPPBDz5s3bYj0zZ86Miy66aHv/mgAAdAIRo86T+zB/bW1tDBkyJD1+7LHHor29PQ4++ODo3r172u6999549tlnIyJi8eLFMXLkyA7nGTVq1Lu+z/nnnx9r165N27Jly7b/LwMAADmXq4uUN6dbt25RVVWVHq9bty5qampi0aJFUVNT0+HY7t27v+f3qauri7q6uvf8egAA2BlUtEGora2N9vb2bXrN4YcfHu3t7bFixYoYPXr0Zo857LDD4qGHHuqw78EHH3zPdQIAkC8iRp2nog1C//7946GHHooXXnghunfvHqVS6W++5uCDD45TTjklJk+eHN/4xjfi8MMPj5UrV0Zzc3MMGTIkPvnJT8YXvvCF+MhHPhJXXHFFHH/88XH33Xe/6/UHAADARhW9BuHcc8+NmpqaGDRoUOy9996xdOnSrXrdTTfdFJMnT44vfelLccghh8SECRPid7/7Xey3334REfGhD30orr/++rjqqqti6NCh8ctf/jIuuOCCzvxVAABgp1CV7UzzkO2otbU1Ghsbo3fvAVFdXfO3XwA7mQ8efkylS4CKenLxA5UuASqmVGqP559/NNauXRsNDQ2VLqeDTd/RPvWps2LXXfNz/eibb66Pn/3s2lx+Ztsq96sYAQAAO44GAQAASHK/zCkAALxDlm3c8iJPtZTJBAEAAEg0CAAAQCJiBABA4WRv/+RFnmoplwkCAACQaBAAAIBExAgAgMLJsizydL/fPNVSLhMEAAAg0SAAAACJiBEAAIWzMWJUqnQZiYgRAACwU9IgAAAAiYgRAACFYxWjzmOCAAAAJBoEAAAgETECAKBwRIw6jwkCAACQaBAAAKBCrrnmmujfv3/U19fHyJEjY+HChVs89oknnogTTzwx+vfvH1VVVTFr1qx3HHPhhRdGVVVVh+3QQw/dppo0CAAAFM6miFGetm01d+7cmDZtWsyYMSMefvjhGDp0aIwbNy5WrFix2eNfe+21GDBgQFxyySXR1NS0xfO+//3vj1deeSVt999//zbVpUEAAIAKuPLKK+OMM86IqVOnxqBBg2L27Nmx2267xY033rjZ44844oi4/PLLY9KkSVFXV7fF8+6yyy7R1NSUtp49e25TXRoEAADYTlpbWzts69ev3+xxGzZsiEWLFsXYsWPTvurq6hg7dmwsWLCgrBqWLFkSffr0iQEDBsQpp5wSS5cu3abXaxAAACicLCvlbouI6NevXzQ2NqZt5syZm61/1apV0d7eHr169eqwv1evXtHS0vKeP5eRI0fGnDlzYt68eXHdddfF888/H6NHj45XX311q89hmVMAANhOli1bFg0NDenxu0WBOsOxxx6b/jxkyJAYOXJk7L///vHDH/4wPve5z23VOTQIAACwnTQ0NHRoELakZ8+eUVNTE8uXL++wf/ny5e96AfK22nPPPePggw+OZ555ZqtfI2IEAEDxZFn+tm1QW1sbw4cPj+bm5rSvVCpFc3NzjBo1art9TOvWrYtnn302evfuvdWvMUEAAIAKmDZtWkyZMiVGjBgRRx55ZMyaNSva2tpi6tSpERExefLk6Nu3b7qOYcOGDfHkk0+mP7/00kvxyCOPRPfu3WPgwIEREXHuuefGcccdF/vvv3+8/PLLMWPGjKipqYmTTz55q+vSIAAAQAVMnDgxVq5cGdOnT4+WlpYYNmxYzJs3L124vHTp0qiu/nPg5+WXX47DDz88Pb7iiiviiiuuiKOOOirmz58fEREvvvhinHzyybF69erYe++946Mf/Wg8+OCDsffee291XRoEAAAKJ3v7Jy/eay1nn312nH322Zt9btOX/k369+//N2/Idvvtt7+nOv6SaxAAAIBEgwAAACQiRgAAFFD2N+M2O1aeaimPCQIAAJBoEAAAgETECACAwsmyfEWM8lRLuUwQAACARIMAAAAkGgQAACBxDQIAAIWTZaXIslKly0jyVEu5TBAAAIBEgwAAACQiRgAAFI5lTjuPCQIAAJBoEAAAgETECACAwhEx6jwmCAAAQKJBAAAAEhEjAAAKR8So85ggAAAAiQYBAABIRIwAACieLNu45UWeaimTCQIAAJBoEAAAgETECACAwskiiyxKlS4jyULECAAA2AlpEAAAgETECACAwnGjtM5jggAAACQaBAAAIBExAgCgcESMOo8JAgAAkGgQAACARMQIAIDCETHqPCYIAABAokEAAAASESMAAAony0qRZaVKl5HkqZZymSAAAACJBgEAAEhEjAAAKByrGHUeEwQAACDRIAAAAImIEQAAhSNi1HlMEAAAgESDAAAAJCJGAAAUT5Zt3PIiT7WUyQQBAABINAgAAEAiYgQAQOFkb//kRZ5qKZcJAgAAkGgQAACARMQIAIDCybJSZFmp0mUkeaqlXCYIAABAokEAAAASESMAAAony7LIcnRzsjzVUi4TBAAAINEgAAAAiYgRAACFI2LUeUwQAACAxAThb3jrrbeiunrnWdcWttaTix+odAlQUc8883ClS4CKaW1tjcbGxkqXQYVoEAAAKBwRo84jYgQAACQaBAAAIBExAgCggEqRZXm6TjRPtZTHBAEAAEg0CAAAQCJiBABA4VjFqPOYIAAAAIkGAQAASESMAAAonizbuOVFnmopkwkCAACQaBAAAIBExAgAgMLJIiKL/MR68lNJ+UwQAACARIMAAAAkIkYAABSOG6V1HhMEAAAg0SAAAACJiBEAAIWTZaXIslKly0jyVEu5TBAAAIBEgwAAACQiRgAAFI5VjDqPCQIAAJBoEAAAgETECACAwhEx6jwmCAAAQKJBAAAAEhEjAAAKR8So85ggAAAAiQYBAABIRIwAACgcEaPOY4IAAAAkGgQAACARMQIAoHiy0sYtL/JUS5lMEAAAgESDAAAAJCJGAAAUTvb2T17kqZZymSAAAACJBgEAAEhEjAAAKBw3Sus8JggAAECiQQAAABIRIwAACkfEqPOYIAAAAIkGAQAASESMAAAonCwrRZaVKl1GkqdaymWCAAAAJBoEAAAgETECAKBwrGLUeUwQAACARIMAAAAkIkYAABSOiFHnMUEAAAASDQIAAFTINddcE/3794/6+voYOXJkLFy4cIvHPvHEE3HiiSdG//79o6qqKmbNmlX2OTdHgwAAQOFsihjladtWc+fOjWnTpsWMGTPi4YcfjqFDh8a4ceNixYoVmz3+tddeiwEDBsQll1wSTU1N2+Wcm6NBAACACrjyyivjjDPOiKlTp8agQYNi9uzZsdtuu8WNN9642eOPOOKIuPzyy2PSpElRV1e3Xc65ORoEAADYTlpbWzts69ev3+xxGzZsiEWLFsXYsWPTvurq6hg7dmwsWLDgPb339jqnBgEAgOLJIiLLcrRtLKtfv37R2NiYtpkzZ262/FWrVkV7e3v06tWrw/5evXpFS0vLe/pIttc5LXMKAADbybJly6KhoSE93lIUKM80CAAAsJ00NDR0aBC2pGfPnlFTUxPLly/vsH/58uVbvAB5R51TxAgAgMLJopS7bVvU1tbG8OHDo7m5Oe0rlUrR3Nwco0aNek+fyfY6pwkCAABUwLRp02LKlCkxYsSIOPLII2PWrFnR1tYWU6dOjYiIyZMnR9++fdN1DBs2bIgnn3wy/fmll16KRx55JLp37x4DBw7cqnNuDQ0CAABUwMSJE2PlypUxffr0aGlpiWHDhsW8efPSRcZLly6N6uo/B35efvnlOPzww9PjK664Iq644oo46qijYv78+Vt1zq1Rlb2Xuzp0Aa2trdHY2Bh7771fh/9hoKvo3r1HpUuAinrmmYcrXQJUzKbvQWvXrt2qPP2OtKm2AQOGRU1NTaXLSdrb2+O55x7J5We2rXzzBQAAEg0CAACQuAYBAIACyiJfSfk81VIeEwQAACDRIAAAAImIEQAAhZNl+YoY5amWcpkgAAAAiQYBAABIRIwAACicLCtFllVVuowky0qVLmG7MUEAAAASDQIAAJCIGAEAUDhWMeo8JggAAECiQQAAABIRIwAACkfEqPOYIAAAAIkGAQAASESMAAAonizbuOVFnmopkwkCAACQaBAAAIBExAgAgMLJ3v7JizzVUi4TBAAAINEgAAAAiYgRAACFk2WlyLKqSpeRZFmp0iVsNyYIAABAokEAAAASESMAAAony7LIcnRzsjzVUi4TBAAAIOnUBqGqqmqz2+23356OaW9vj29+85sxePDgqK+vjx49esSxxx4bv/3tbzucq729PS655JI49NBDo1u3brHXXnvFyJEj44YbbujMXwEAALqU7R4x+tOf/hS77rprdO/ePSIibrrpphg/fnyHY/bcc8+I2DiKmTRpUvz617+Oyy+/PI4++uhobW2Na665JsaMGRM/+tGPYsKECRERcdFFF8W3v/3t+Pd///cYMWJEtLa2xu9///v405/+lM778ssvxz777BO77CI5BQCwMxMx6jzb5Zv0W2+9FXfffXfMmTMnfvrTn8ZDDz0UQ4cOjYiNzUBTU9NmX/fDH/4w/uM//iPuvPPOOO6449L+73znO7F69eo4/fTT4xOf+ETsvvvuceedd8ZZZ50Vn/nMZ9Jxm95jk+uvvz6uu+66+B//43/ElClTYvDgwdvj1wMAgC6jrIjRY489Fl/60pdi3333jcmTJ8fee+8dv/nNb97xxX1LfvCDH8TBBx/coTnY5Etf+lKsXr06fvWrX0VERFNTU9xzzz2xcuXKLZ7vy1/+clx11VWxePHi+OAHPxgf/OAH4+qrr37X12yyfv36aG1t7bABAEBXs80NwurVq+Oqq66KD37wgzFixIh47rnn4tprr41XXnklrr322hg1alSH408++eTo3r17h23p0qUREfH000/HYYcdttn32bT/6aefjoiIK6+8MlauXBlNTU0xZMiQ+J//83/GXXfd1eE19fX1MXHixPj5z38eL730UkyePDnmzJkTffv2jQkTJsRPfvKTeOuttzb7fjNnzozGxsa09evXb1s/GgAAdpBNEaM8bTuLbW4QvvWtb8U555wT3bt3j2eeeSZ+8pOfxKc//emora3d7PHf/OY345FHHumw9enTJz2/tR/moEGD4vHHH48HH3ww/vEf/zFWrFgRxx13XJx++umbPX6fffaJc845Jx5++OH4z//8z1iwYEF8+tOfjscff3yzx59//vmxdu3atC1btmyr6gIAgJ3JNl+D8PnPfz522WWXuOWWW+L9739/nHjiiXHqqafGmDFjorr6nf1GU1NTDBw4cLPnOvjgg2Px4sWbfW7T/oMPPjjtq66ujiOOOCKOOOKIOOecc+J73/tenHrqqfHVr341DjjggA6vf/XVV+M//uM/4tZbb4377rsvjjrqqJgyZUoMGjRos+9XV1cXdXV1W/UZAADAzmqbJwh9+vSJCy64IJ5++umYN29e1NbWxqc//enYf//94ytf+Uo88cQTW32uSZMmxZIlS+KnP/3pO577xje+Ee973/viE5/4xBZfv+nLfltbW0RsXAr1rrvuis9+9rPRq1evuOSSS+Loo4+O5557Lpqbm2Py5MlbnHQAAFAclY4TiRhtwYc//OH49re/HS0tLXH55ZfHI488EkOHDo3HHnssHbNmzZpoaWnpsG36Qj9p0qQ44YQTYsqUKfHd7343XnjhhXj00Ufjn/7pn+LOO++MG264IXbfffeIiDjppJPim9/8Zjz00EPx//7f/4v58+fH//pf/ysOPvjgOPTQQyMi4t/+7d/i5JNPjj322CN+/etfx1NPPRVf/epXY7/99ivn1wQAgC6jKtvO7c7LL78c3bt3j4aGhqiqqtrsMTNnzoyvfOUrEbFxidRZs2bFnDlzYsmSJVFfXx+jRo2Kr33ta/GRj3wkveb666+P2267LR5//PFYu3ZtNDU1xcc//vG48MILY//994+IiBdeeCGampqivr6+7N+jtbU1GhsbY++999tsdAp2dt2796h0CVBRzzzzcKVLgIrZ9D1o7dq10dDQUOlyOthUW1PTgKiurql0OUmp1B4tLc/l8jPbVtu9QdhZaBDo6jQIdHUaBLqyIjQIvXodkKvvaKVSKZYvfz6Xn9m2ys+nCgAAVJwGAQAASLZ5mVMAAKi4LNu45UWeaimTCQIAAJBoEAAAgETECACAwsne/smLPNVSLhMEAAAg0SAAAACJiBEAAIWTZVnk6X6/eaqlXCYIAABAokEAAAASESMAAAony0q5ujdZlpUqXcJ2Y4IAAAAkGgQAACARMQIAoHCsYtR5TBAAAIBEgwAAACQiRgAAFI6IUecxQQAAABINAgAAkIgYAQBQOCJGnccEAQAASDQIAABAImIEAEAB5StiFJGnWspjggAAACQaBAAAIBExAgCgeLJSpSvoKG/1lMEEAQAASDQIAABAImIEAEDhZJFFnlYOynJUS7lMEAAAgESDAAAAJCJGAAAUzsabpOUn1pOvm7aVxwQBAABINAgAAEAiYgQAQOGIGHUeEwQAACDRIAAAAImIEQAAhZNlpUqX0EHe6imHCQIAAJBoEAAAgETECACAwtm4aFB+Vg7aiRYxMkEAAAD+TIMAAAAkIkYAABRO3m5Mlrd6ymGCAAAAJBoEAAAgETECAKBw8hbpyVs95TBBAAAAEg0CAACQiBgBAFA8eYv05K2eMpggAAAAiQYBAABIRIwAACicLEoRUVXpMpIsRIwAAICdkAYBAABIRIwAACicvN2YLG/1lMMEAQAASDQIAABAImIEAEDh5C3Sk7d6ymGCAAAAJBoEAAAgETECAKBw8hbpyVs95TBBAAAAEg0CAACQiBgBAFA4eYv05K2ecpggAAAAiQYBAABIRIwAACicLCtFRFWly0hEjAAAgJ2SBgEAAEhEjAAAKJy8RXryVk85TBAAAIBEgwAAACQiRgAAFE/eIj15q6cMJggAAFAh11xzTfTv3z/q6+tj5MiRsXDhwnc9/kc/+lEceuihUV9fH4MHD45f/OIXHZ4/7bTToqqqqsM2fvz4bapJgwAAABUwd+7cmDZtWsyYMSMefvjhGDp0aIwbNy5WrFix2eMfeOCBOPnkk+Nzn/tc/Pd//3dMmDAhJkyYEI8//niH48aPHx+vvPJK2m677bZtqqsq25kuud6OWltbo7GxMfbee7+ortZH0fV0796j0iVART3zzMOVLgEqZtP3oLVr10ZDQ0Oly+lgU23V1TVRVZWvG6WVSu3b9JmNHDkyjjjiiPj3f//3iIgolUrRr1+/+Od//uf4yle+8o7jJ06cGG1tbfGzn/0s7fvQhz4Uw4YNi9mzZ0fExgnCmjVr4o477njPv4tvvgAAsJ20trZ22NavX7/Z4zZs2BCLFi2KsWPHpn3V1dUxduzYWLBgwWZfs2DBgg7HR0SMGzfuHcfPnz8/9tlnnzjkkEPizDPPjNWrV2/T76BBAACA7aRfv37R2NiYtpkzZ272uFWrVkV7e3v06tWrw/5evXpFS0vLZl/T0tLyN48fP3583HLLLdHc3ByXXnpp3HvvvXHsscdGe3v7Vv8OVjECAKBwsqwUEfmKGEVELFu2rEPEqK6ubofWMWnSpPTnwYMHx5AhQ+LAAw+M+fPnx9FHH71V5zBBAACA7aShoaHDtqUGoWfPnlFTUxPLly/vsH/58uXR1NS02dc0NTVt0/EREQMGDIiePXvGM888s9W/gwYBAAB2sNra2hg+fHg0NzenfaVSKZqbm2PUqFGbfc2oUaM6HB8R8atf/WqLx0dEvPjii7F69ero3bv3VtcmYgQAQOHkbSHO91LPtGnTYsqUKTFixIg48sgjY9asWdHW1hZTp06NiIjJkydH375903UMX/ziF+Ooo46Kb3zjG/HJT34ybr/99vj9738f3/nOdyIiYt26dXHRRRfFiSeeGE1NTfHss8/GeeedFwMHDoxx48ZtdV0aBAAAqICJEyfGypUrY/r06dHS0hLDhg2LefPmpQuRly5d2mG5/Q9/+MPxgx/8IC644IL413/91zjooIPijjvuiA984AMREVFTUxOPPvpo3HzzzbFmzZro06dPHHPMMXHxxRdv07UQ7oOwBe6DQFfnPgh0de6DQFdWhPsgRETu7oMQEbn8zLaVCQIAAIXk37k7h38aBwAAEhOELdjUkZZKpQpXApVRKm39DVVgZ9Ta2lrpEqBiNv399y/0XZMGYQteffXViIhYvfrFClcClbFy5dJKlwAVtSnjDF3Zq6++mrv/Fmpra6OpqWmLdxuupKampqitra10GWVzkfIWlEqlePnll2OPPfbI1QUwXUlra2v069fvHXckhK7A33+6Ov8NVFaWZfHqq69Gnz59crlYyxtvvBEbNmyodBnvUFtbG/X19ZUuo2wmCFtQXV0d++67b6XLIP58R0Loivz9p6vz30Dl5G1y8Jfq6+t3ii/ieZW/lhAAAKgYDQIAAJBoEMiturq6mDFjxjbd+Q92Fv7+09X5bwAqx0XKAABAYoIAAAAkGgQAACDRIAAAAIkGAQAASDQIAABAokEAAAASDQIAAJBoEAAAgOT/B7zXwUbIO/8EAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to visualize attention\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Ensure attentions is 2D\n",
    "    if attentions.dim() == 1:\n",
    "        attentions = attentions.unsqueeze(0)  # Add a dimension if necessary\n",
    "    attentions = attentions.cpu().detach().numpy()  # Convert to numpy array\n",
    "\n",
    "    # Crop the attention matrix to match the input and output lengths\n",
    "    cropped_attentions = attentions[:len(output_words), :len(input_sentence.split(' ') + ['<EOS>'])]\n",
    "\n",
    "    # Reverse each word in the input sentence for proper Hebrew display\n",
    "    input_labels = input_sentence.split(' ') + ['<EOS>']\n",
    "    reversed_input_labels = [''.join(reversed(word)) for word in input_labels]  # Reverse the characters of each word\n",
    "\n",
    "    # Plot the attention matrix\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cropped_attentions, cmap='bone')\n",
    "\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticks(range(len(reversed_input_labels)))\n",
    "    ax.set_yticks(range(len(output_words)))\n",
    "    ax.set_xticklabels(reversed_input_labels, rotation=90)  # Display reversed words\n",
    "    ax.set_yticklabels(output_words)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Function to evaluate and show attention for a single sentence\n",
    "def evaluate_and_show_attention(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
    "\n",
    "        for di in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            attentions[di] = attn_weights.data\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        attentions = attentions[:len(decoded_words), :input_length]\n",
    "        print(f'Input Sentence: {sentence}')\n",
    "        print(f'Output Sentence: {\" \".join(decoded_words)}')\n",
    "        show_attention(sentence, decoded_words, attentions)\n",
    "\n",
    "# Generate attention plots for 5 random sentences\n",
    "def show_attention_for_random_sentences(encoder, decoder, pairs, input_lang, output_lang, n=5):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print(f'Example {i + 1}:')\n",
    "        evaluate_and_show_attention(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "\n",
    "# Display attention plots for 5 random sentences\n",
    "show_attention_for_random_sentences(encoder, decoder, pairs, input_lang, output_lang, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcqtVxkclIWG"
   },
   "source": [
    "3) Do you think this model performs well? Why or why not? What are its limitations/disadvantages? What would you do to improve it?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcZZMZv7Ijk8"
   },
   "source": [
    "# **Answer 3)**\n",
    "\n",
    "Performance:\n",
    "\n",
    "The model has limitations that affect its performance. While it may perform reasonably well on sentences within the filtered training data (eng_prefixes), it struggles to generalize to unseen sentence structures or words, as the vocabulary and diversity are restricted by the filter. Additionally, its limited size and training data may hinder its ability to capture complex linguistic relationships, leading to suboptimal translations for longer or more diverse sentences. Thus, the model’s performance is acceptable within the constraints of its training data but is inadequate for broader, real-world use cases.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "The model has several limitations. Its vocabulary and linguistic diversity are restricted by the eng_prefixes filter, which limits its ability to generalize to sentences outside the training data. It also struggles with longer or complex sentences due to the fixed MAX_LENGTH parameter and the relatively small hidden size, which constrain its capacity to handle complex linguistic patterns. Furthermore, the dataset may not provide sufficient examples for effective training, and the lack of advanced techniques like pretraining or larger architectures reduces its overall performance. These constraints make the model suitable for basic translations but inadequate for more diverse or nuanced tasks.\n",
    "\n",
    "Improvements:\n",
    "\n",
    "1) Expand the Dataset:\n",
    "\n",
    "Use a larger and more diverse dataset without filtering too strictly by prefixes to increase vocabulary and linguistic diversity.\n",
    "\n",
    "2) Increase Model Capacity:\n",
    "\n",
    "Increase the hidden size or use more advanced architectures like Transformer based models for better handling of complex linguistic patterns.\n",
    "\n",
    "3) Regularization and Optimization:\n",
    "\n",
    "Apply techniques like gradient clipping, or better optimizers to stabilize training and prevent overfitting.\n",
    "\n",
    "4) Data Augmentation:\n",
    "\n",
    "Using techniques like back translation to create additional training data, further improving the model’s generalization.\n",
    "\n",
    "5) Incorporate Pretrained Embeddings:\n",
    "\n",
    "Use pretrained word embeddings for Hebrew and English to provide the model with richer word representations.\n",
    "\n",
    "6) Increase Computational Resources:\n",
    "\n",
    "Train the model for more epochs to ensure better convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2VSrRNtlJub"
   },
   "source": [
    "4) Using any neural network architecture of your liking, build  a model with the aim to beat the model in 2.a. Compare your results in a meaningful way, and add a short explanation to why you think/thought your suggested network is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GY4C7dilqR8"
   },
   "source": [
    "# **Answer 4)**\n",
    "\n",
    "Explanation of the Improvements:\n",
    "\n",
    "The improved model performed better because of its larger capacity, diverse dataset, and the use of the Adam optimizer. Additionally, I increased the maximum sentence length to 15 words, allowing for more complex and realistic sentence pairs in training. The larger hidden size allowed the model to capture more nuanced relationships between input and output sequences, while the Adam optimizer improved convergence, reducing the chances of getting stuck in poor local minima.\n",
    "\n",
    "The expanded dataset introduced variability and complexity, which pushed the improved model to learn generalizable patterns instead of memorizing the data. While both models achieved perfect accuracy on their respective datasets, the improved model is better equipped to generalize to unseen examples due to its enhancements.\n",
    "\n",
    "Word-Level Accuracy of the model in Q2: 0.9167\n",
    "Word-Level Accuracy of the current model: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvUx_ewGNiGg",
    "outputId": "4cbd641e-92b9-4e2f-8a9b-6f0841d3ba6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 9 sentence pairs\n",
      "Trimmed to 9 sentence pairs\n",
      "Counted words:\n",
      "heb 22\n",
      "eng 28\n",
      "Epoch 1, Loss: 3.4166\n",
      "Epoch 2, Loss: 2.7168\n",
      "Epoch 3, Loss: 1.1721\n",
      "Epoch 4, Loss: 1.6244\n",
      "Epoch 5, Loss: 0.9653\n",
      "Epoch 6, Loss: 0.5052\n",
      "Epoch 7, Loss: 0.1915\n",
      "Epoch 8, Loss: 0.1022\n",
      "Epoch 9, Loss: 0.0573\n",
      "Epoch 10, Loss: 0.0395\n",
      "Epoch 11, Loss: 0.0300\n",
      "Epoch 12, Loss: 0.0245\n",
      "Epoch 13, Loss: 0.0205\n",
      "Epoch 14, Loss: 0.0175\n",
      "Epoch 15, Loss: 0.0164\n",
      "Epoch 16, Loss: 0.0144\n",
      "Epoch 17, Loss: 0.0130\n",
      "Epoch 18, Loss: 0.0122\n",
      "Epoch 19, Loss: 0.0106\n",
      "Epoch 20, Loss: 0.0103\n",
      "Epoch 21, Loss: 0.0094\n",
      "Epoch 22, Loss: 0.0087\n",
      "Epoch 23, Loss: 0.0080\n",
      "Epoch 24, Loss: 0.0075\n",
      "Epoch 25, Loss: 0.0073\n",
      "Epoch 26, Loss: 0.0064\n",
      "Epoch 27, Loss: 0.0062\n",
      "Epoch 28, Loss: 0.0058\n",
      "Epoch 29, Loss: 0.0055\n",
      "Epoch 30, Loss: 0.0052\n",
      "Epoch 31, Loss: 0.0049\n",
      "Epoch 32, Loss: 0.0047\n",
      "Epoch 33, Loss: 0.0046\n",
      "Epoch 34, Loss: 0.0045\n",
      "Epoch 35, Loss: 0.0042\n",
      "Epoch 36, Loss: 0.0038\n",
      "Epoch 37, Loss: 0.0037\n",
      "Epoch 38, Loss: 0.0035\n",
      "Epoch 39, Loss: 0.0036\n",
      "Epoch 40, Loss: 0.0033\n",
      "Epoch 41, Loss: 0.0033\n",
      "Epoch 42, Loss: 0.0032\n",
      "Epoch 43, Loss: 0.0029\n",
      "Epoch 44, Loss: 0.0028\n",
      "Epoch 45, Loss: 0.0028\n",
      "Epoch 46, Loss: 0.0028\n",
      "Epoch 47, Loss: 0.0025\n",
      "Epoch 48, Loss: 0.0024\n",
      "Epoch 49, Loss: 0.0024\n",
      "Epoch 50, Loss: 0.0024\n",
      "Input: אני שמח\n",
      "Target: i am happy\n",
      "Predicted: i am happy <EOS>\n",
      "Input: אני עייף\n",
      "Target: i am tired\n",
      "Predicted: i am tired <EOS>\n",
      "Input: הוא גבוה\n",
      "Target: he is tall\n",
      "Predicted: he is tall <EOS>\n",
      "Input: היא נמוכה\n",
      "Target: she is short\n",
      "Predicted: she is short <EOS>\n",
      "Input: אני אוהב ללמוד\n",
      "Target: i love learning\n",
      "Predicted: i love learning <EOS>\n",
      "Input: אתה חכם מאוד\n",
      "Target: you are very smart\n",
      "Predicted: you are very smart <EOS>\n",
      "Input: היא גרה בעיר הגדולה\n",
      "Target: she lives in the big city\n",
      "Predicted: she lives in the big city <EOS>\n",
      "Input: אני רוצה לאכול\n",
      "Target: i want to eat\n",
      "Predicted: i want to eat <EOS>\n",
      "Input: הם מטיילים ביער\n",
      "Target: they are walking in the forest\n",
      "Predicted: they are walking in the forest <EOS>\n",
      "Word-Level Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "MAX_LENGTH = 15  # Increased to allow longer sentences\n",
    "hidden_size = 256  # Increased hidden size for better capacity\n",
    "epochs = 50\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "# Helper Functions\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return f'{m}m {s:.0f}s'\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    elapsed = now - since\n",
    "    estimated_total = elapsed / percent\n",
    "    remaining = estimated_total - elapsed\n",
    "    return f'{asMinutes(elapsed)} (- {asMinutes(remaining)})'\n",
    "\n",
    "# Language Class\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "# Preprocess Data\n",
    "def prepareData(lang1, lang2, pairs):\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    pairs = [pair for pair in pairs if len(pair[0].split()) < MAX_LENGTH and len(pair[1].split()) < MAX_LENGTH]\n",
    "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# Encoder Model\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "# Decoder with Attention\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attn = nn.Linear(hidden_size * 2, max_length)\n",
    "        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = torch.nn.functional.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1\n",
    "        )\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = torch.nn.functional.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "# Training\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]])\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]])\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        for di in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n",
    "\n",
    "# Helper Functions\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "# Word-Level Accuracy\n",
    "def compute_word_accuracy(encoder, decoder, pairs, input_lang, output_lang):\n",
    "    total_words = 0\n",
    "    correct_words = 0\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_sentence, target_sentence = pair\n",
    "        predicted_words = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "        predicted_words = [word for word in predicted_words if word != '<EOS>']\n",
    "        target_words = target_sentence.split()\n",
    "\n",
    "        for pred_word, ref_word in zip(predicted_words, target_words):\n",
    "            if pred_word == ref_word:\n",
    "                correct_words += 1\n",
    "            total_words += 1\n",
    "\n",
    "    accuracy = correct_words / total_words if total_words > 0 else 0\n",
    "    print(f\"Word-Level Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "# Expanded Dataset\n",
    "pairs = [\n",
    "    [\"אני שמח\", \"i am happy\"],\n",
    "    [\"אני עייף\", \"i am tired\"],\n",
    "    [\"הוא גבוה\", \"he is tall\"],\n",
    "    [\"היא נמוכה\", \"she is short\"],\n",
    "    [\"אני אוהב ללמוד\", \"i love learning\"],\n",
    "    [\"אתה חכם מאוד\", \"you are very smart\"],\n",
    "    [\"היא גרה בעיר הגדולה\", \"she lives in the big city\"],\n",
    "    [\"אני רוצה לאכול\", \"i want to eat\"],\n",
    "    [\"הם מטיילים ביער\", \"they are walking in the forest\"]\n",
    "]\n",
    "\n",
    "# Prepare Data\n",
    "input_lang, output_lang, pairs = prepareData('heb', 'eng', pairs)\n",
    "\n",
    "# Initialize Models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)  # Use Adam optimizer\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for pair in pairs:\n",
    "        input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "        target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "for pair in pairs:\n",
    "    print(f\"Input: {pair[0]}\")\n",
    "    print(f\"Target: {pair[1]}\")\n",
    "    output_words = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "    print(f\"Predicted: {' '.join(output_words)}\")\n",
    "\n",
    "# Compute Accuracy\n",
    "compute_word_accuracy(encoder, decoder, pairs, input_lang, output_lang)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-tVmomvXcKk"
   },
   "outputs": [],
   "source": [
    "# use the following parameters:\n",
    "MAX_LENGTH = 10\n",
    "hidden_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9-C4pLEXzCF"
   },
   "source": [
    "SOLUTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WrHkLD6p813"
   },
   "outputs": [],
   "source": [
    "### ANSWERS ABOVE: CODE + EXPLANATIONS"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
